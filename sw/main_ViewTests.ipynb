{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load headers\n",
    "\n",
    "import warnings\n",
    "import argparse\n",
    "from time import localtime, strftime\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Tests Using all 9 containers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### A. Benchmark, uses all 4 views with 4 GRU units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    init_time: 200903214601\n",
      "    task: flvl\n",
      "    output_dim: 3\n",
      "    model_type: GRU\n",
      "    bi_dir: False\n",
      "    device: cuda:0\n",
      "    data_root: ./r21d_rgb_features\n",
      "    drop_p: 0.0\n",
      "    batch_size: 64\n",
      "    input_dim: 512\n",
      "    hidden_dim: 512\n",
      "    n_layers: 3\n",
      "    num_epochs: 30\n",
      "    seed: 1337\n",
      "{'train': [1, 2, 4, 5, 7, 8], 'valid': [3, 6, 9], 'public_test': [10, 11, 12], 'private_test': [13, 14, 15]}\n",
      "Using pre-trained model from .\n",
      "Saving predictions @ ./predictions/200903214601/flvl_train_1_2_4_5_7_8_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/200903214601/flvl_valid_3_6_9_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/200903214601/flvl_public_test_trained_on_1_2_4_5_7_8_r21d_rgb.csv\n",
      "{'train': [1, 3, 4, 6, 7, 9], 'valid': [2, 5, 8], 'public_test': [10, 11, 12], 'private_test': [13, 14, 15]}\n",
      "Using pre-trained model from .\n",
      "Saving predictions @ ./predictions/200903214601/flvl_train_1_3_4_6_7_9_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/200903214601/flvl_valid_2_5_8_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/200903214601/flvl_public_test_trained_on_1_3_4_6_7_9_r21d_rgb.csv\n",
      "{'train': [2, 3, 5, 6, 8, 9], 'valid': [1, 4, 7], 'public_test': [10, 11, 12], 'private_test': [13, 14, 15]}\n",
      "Using pre-trained model from .\n",
      "Saving predictions @ ./predictions/200903214601/flvl_train_2_3_5_6_8_9_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/200903214601/flvl_valid_1_4_7_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/200903214601/flvl_public_test_trained_on_2_3_5_6_8_9_r21d_rgb.csv\n",
      "Saved test results @ ./predictions/200903214601/flvl_public_test_agg_r21d_rgb.csv\n"
     ]
    }
   ],
   "source": [
    "from main import Config, run_kfold\n",
    "# Reproduce the best experiment\n",
    "# if True, will use the pre-trained model and make predictions, if False, will train the model\n",
    "use_pretrained = True\n",
    "exp_name = 200903214601\n",
    "cfg = Config()\n",
    "cfg.load_from(path=f'./predictions/{exp_name}/cfg.txt')\n",
    "# replacing the time with the old_time + current_time such that there is no collision\n",
    "if use_pretrained:\n",
    "    cfg.init_time = exp_name\n",
    "else:\n",
    "    cfg.init_time = f'{cfg.init_time}_{strftime(\"%y%m%d%H%M%S\", localtime())}'\n",
    "run_kfold(cfg, use_pretrained)  # Expected average of Best Metrics on Each Valid Set: 0.747354 @ 200903214601"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### B. Experiment with 3 Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': [1, 2, 4, 5, 7, 8], 'valid': [3, 6, 9], 'public_test': [10, 11, 12], 'private_test': [13, 14, 15]}\n",
      "(train @ 1): L: 1.399172; A: 0.394737; R: 0.394737; P: 0.394046; F1: 0.386594\n",
      "(valid @ 1): L: 1.157390; A: 0.495614; R: 0.495614; P: 0.597845; F1: 0.450747\n",
      "(train @ 2): L: 1.001254; A: 0.473684; R: 0.473684; P: 0.554834; F1: 0.438632\n",
      "(valid @ 2): L: 1.049755; A: 0.469298; R: 0.469298; P: 0.426429; F1: 0.404527\n",
      "(train @ 3): L: 0.942169; A: 0.497807; R: 0.497807; P: 0.539756; F1: 0.455182\n",
      "(valid @ 3): L: 1.098041; A: 0.416667; R: 0.416667; P: 0.486260; F1: 0.326225\n",
      "(train @ 4): L: 0.914559; A: 0.530702; R: 0.530702; P: 0.583913; F1: 0.529705\n",
      "(valid @ 4): L: 1.066251; A: 0.478070; R: 0.478070; P: 0.435605; F1: 0.415544\n",
      "(train @ 5): L: 0.882465; A: 0.554825; R: 0.554825; P: 0.599953; F1: 0.549315\n",
      "(valid @ 5): L: 1.005003; A: 0.570175; R: 0.570175; P: 0.624105; F1: 0.549443\n",
      "(train @ 6): L: 0.857013; A: 0.548246; R: 0.548246; P: 0.670982; F1: 0.502982\n",
      "(valid @ 6): L: 0.971067; A: 0.570175; R: 0.570175; P: 0.642084; F1: 0.531207\n",
      "(train @ 7): L: 0.795648; A: 0.572368; R: 0.572368; P: 0.604247; F1: 0.570893\n",
      "(valid @ 7): L: 0.949167; A: 0.570175; R: 0.570175; P: 0.580995; F1: 0.548669\n",
      "(train @ 8): L: 0.766544; A: 0.594298; R: 0.594298; P: 0.596231; F1: 0.594928\n",
      "(valid @ 8): L: 0.970852; A: 0.552632; R: 0.552632; P: 0.602632; F1: 0.530545\n",
      "(train @ 9): L: 0.756167; A: 0.594298; R: 0.594298; P: 0.611200; F1: 0.591010\n",
      "(valid @ 9): L: 0.900317; A: 0.627193; R: 0.627193; P: 0.627174; F1: 0.621935\n",
      "(train @ 10): L: 0.753983; A: 0.627193; R: 0.627193; P: 0.650198; F1: 0.623649\n",
      "(valid @ 10): L: 0.875441; A: 0.653509; R: 0.653509; P: 0.652885; F1: 0.649392\n",
      "(train @ 11): L: 0.710084; A: 0.664474; R: 0.664474; P: 0.681815; F1: 0.660160\n",
      "(valid @ 11): L: 0.868252; A: 0.592105; R: 0.592105; P: 0.595947; F1: 0.568748\n",
      "(train @ 12): L: 0.700527; A: 0.614035; R: 0.614035; P: 0.620895; F1: 0.606356\n",
      "(valid @ 12): L: 0.803500; A: 0.583333; R: 0.583333; P: 0.675297; F1: 0.518499\n",
      "(train @ 13): L: 0.697955; A: 0.618421; R: 0.618421; P: 0.629049; F1: 0.613578\n",
      "(valid @ 13): L: 0.796106; A: 0.583333; R: 0.583333; P: 0.702112; F1: 0.512719\n",
      "(train @ 14): L: 0.645735; A: 0.682018; R: 0.682018; P: 0.680863; F1: 0.681147\n",
      "(valid @ 14): L: 0.773461; A: 0.635965; R: 0.635965; P: 0.642565; F1: 0.626071\n",
      "(train @ 15): L: 0.640547; A: 0.701754; R: 0.701754; P: 0.706409; F1: 0.702175\n",
      "(valid @ 15): L: 0.824642; A: 0.618421; R: 0.618421; P: 0.626738; F1: 0.605727\n",
      "(train @ 16): L: 0.650550; A: 0.655702; R: 0.655702; P: 0.671140; F1: 0.652289\n",
      "(valid @ 16): L: 0.917311; A: 0.552632; R: 0.552632; P: 0.684954; F1: 0.449609\n",
      "(train @ 17): L: 0.666383; A: 0.671053; R: 0.671053; P: 0.677951; F1: 0.669582\n",
      "(valid @ 17): L: 0.739857; A: 0.679825; R: 0.679825; P: 0.688842; F1: 0.675914\n",
      "(train @ 18): L: 0.622986; A: 0.741228; R: 0.741228; P: 0.748157; F1: 0.741623\n",
      "(valid @ 18): L: 0.753473; A: 0.657895; R: 0.657895; P: 0.685459; F1: 0.657485\n",
      "(train @ 19): L: 0.634262; A: 0.712719; R: 0.712719; P: 0.725395; F1: 0.712694\n",
      "(valid @ 19): L: 0.748814; A: 0.697368; R: 0.697368; P: 0.734519; F1: 0.688708\n",
      "(train @ 20): L: 0.550388; A: 0.745614; R: 0.745614; P: 0.761713; F1: 0.744632\n",
      "(valid @ 20): L: 0.712030; A: 0.719298; R: 0.719298; P: 0.730274; F1: 0.716486\n",
      "(train @ 21): L: 0.543733; A: 0.730263; R: 0.730263; P: 0.737691; F1: 0.728413\n",
      "(valid @ 21): L: 0.710579; A: 0.701754; R: 0.701754; P: 0.737979; F1: 0.691378\n",
      "(train @ 22): L: 0.526264; A: 0.739035; R: 0.739035; P: 0.747515; F1: 0.738107\n",
      "(valid @ 22): L: 0.737247; A: 0.666667; R: 0.666667; P: 0.667677; F1: 0.662697\n",
      "(train @ 23): L: 0.611668; A: 0.701754; R: 0.701754; P: 0.708210; F1: 0.700622\n",
      "(valid @ 23): L: 0.740385; A: 0.662281; R: 0.662281; P: 0.744017; F1: 0.639999\n",
      "(train @ 24): L: 0.554014; A: 0.743421; R: 0.743421; P: 0.744512; F1: 0.741699\n",
      "(valid @ 24): L: 0.680619; A: 0.675439; R: 0.675439; P: 0.686502; F1: 0.673372\n",
      "(train @ 25): L: 0.546419; A: 0.743421; R: 0.743421; P: 0.758690; F1: 0.742160\n",
      "(valid @ 25): L: 0.675622; A: 0.675439; R: 0.675439; P: 0.742321; F1: 0.656514\n",
      "(train @ 26): L: 0.462749; A: 0.804825; R: 0.804825; P: 0.824682; F1: 0.805054\n",
      "(valid @ 26): L: 0.659043; A: 0.684211; R: 0.684211; P: 0.690129; F1: 0.682637\n",
      "(train @ 27): L: 0.462697; A: 0.787281; R: 0.787281; P: 0.795049; F1: 0.786945\n",
      "(valid @ 27): L: 0.662644; A: 0.684211; R: 0.684211; P: 0.688438; F1: 0.680714\n",
      "(train @ 28): L: 0.456575; A: 0.793860; R: 0.793860; P: 0.804387; F1: 0.794405\n",
      "(valid @ 28): L: 0.734706; A: 0.706140; R: 0.706140; P: 0.722257; F1: 0.706551\n",
      "(train @ 29): L: 0.422340; A: 0.800439; R: 0.800439; P: 0.802339; F1: 0.800587\n",
      "(valid @ 29): L: 0.644709; A: 0.710526; R: 0.710526; P: 0.712709; F1: 0.710816\n",
      "(train @ 30): L: 0.397275; A: 0.826754; R: 0.826754; P: 0.828048; F1: 0.826704\n",
      "(valid @ 30): L: 0.666514; A: 0.679825; R: 0.679825; P: 0.704626; F1: 0.672095\n",
      "Best val Metric 0.716486 @ 20\n",
      "\n",
      "models are saved @ ./predictions/211209022331/flvl_1_2_4_5_7_8_pretrained_model_c1.pt\n",
      "Saving predictions @ ./predictions/211209022331/flvl_train_1_2_4_5_7_8_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209022331/flvl_valid_3_6_9_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209022331/flvl_public_test_trained_on_1_2_4_5_7_8_r21d_rgb.csv\n",
      "{'train': [1, 3, 4, 6, 7, 9], 'valid': [2, 5, 8], 'public_test': [10, 11, 12], 'private_test': [13, 14, 15]}\n",
      "(train @ 1): L: 1.441022; A: 0.381579; R: 0.381579; P: 0.373686; F1: 0.369343\n",
      "(valid @ 1): L: 0.992816; A: 0.526316; R: 0.526316; P: 0.476573; F1: 0.473684\n",
      "(train @ 2): L: 0.973552; A: 0.475877; R: 0.475877; P: 0.397519; F1: 0.432101\n",
      "(valid @ 2): L: 1.004010; A: 0.500000; R: 0.500000; P: 0.456548; F1: 0.442138\n",
      "(train @ 3): L: 0.954206; A: 0.473684; R: 0.473684; P: 0.515388; F1: 0.441302\n",
      "(valid @ 3): L: 0.997669; A: 0.552632; R: 0.552632; P: 0.571780; F1: 0.550657\n",
      "(train @ 4): L: 1.011167; A: 0.438596; R: 0.438596; P: 0.442015; F1: 0.417688\n",
      "(valid @ 4): L: 1.090563; A: 0.583333; R: 0.583333; P: 0.519582; F1: 0.523082\n",
      "(train @ 5): L: 0.949989; A: 0.502193; R: 0.502193; P: 0.524172; F1: 0.477821\n",
      "(valid @ 5): L: 0.951090; A: 0.600877; R: 0.600877; P: 0.651787; F1: 0.593278\n",
      "(train @ 6): L: 0.871967; A: 0.530702; R: 0.530702; P: 0.645910; F1: 0.483741\n",
      "(valid @ 6): L: 0.948491; A: 0.535088; R: 0.535088; P: 0.609337; F1: 0.487771\n",
      "(train @ 7): L: 0.818790; A: 0.583333; R: 0.583333; P: 0.594827; F1: 0.578555\n",
      "(valid @ 7): L: 0.909606; A: 0.570175; R: 0.570175; P: 0.643256; F1: 0.555048\n",
      "(train @ 8): L: 0.823170; A: 0.543860; R: 0.543860; P: 0.544437; F1: 0.544080\n",
      "(valid @ 8): L: 0.931107; A: 0.504386; R: 0.504386; P: 0.606572; F1: 0.443477\n",
      "(train @ 9): L: 0.845100; A: 0.543860; R: 0.543860; P: 0.552442; F1: 0.544759\n",
      "(valid @ 9): L: 0.774064; A: 0.706140; R: 0.706140; P: 0.712395; F1: 0.706229\n",
      "(train @ 10): L: 0.854655; A: 0.541667; R: 0.541667; P: 0.550131; F1: 0.540826\n",
      "(valid @ 10): L: 0.762692; A: 0.657895; R: 0.657895; P: 0.693134; F1: 0.648969\n",
      "(train @ 11): L: 0.775862; A: 0.638158; R: 0.638158; P: 0.661839; F1: 0.629440\n",
      "(valid @ 11): L: 0.799800; A: 0.649123; R: 0.649123; P: 0.689458; F1: 0.647364\n",
      "(train @ 12): L: 0.744854; A: 0.603070; R: 0.603070; P: 0.647031; F1: 0.576594\n",
      "(valid @ 12): L: 0.812820; A: 0.504386; R: 0.504386; P: 0.350791; F1: 0.381708\n",
      "(train @ 13): L: 0.764399; A: 0.574561; R: 0.574561; P: 0.584378; F1: 0.572312\n",
      "(valid @ 13): L: 0.775063; A: 0.552632; R: 0.552632; P: 0.353454; F1: 0.416791\n",
      "(train @ 14): L: 0.749555; A: 0.563596; R: 0.563596; P: 0.562546; F1: 0.556549\n",
      "(valid @ 14): L: 0.703637; A: 0.679825; R: 0.679825; P: 0.721914; F1: 0.675240\n",
      "(train @ 15): L: 0.710133; A: 0.640351; R: 0.640351; P: 0.639169; F1: 0.637985\n",
      "(valid @ 15): L: 0.720189; A: 0.657895; R: 0.657895; P: 0.697145; F1: 0.656053\n",
      "(train @ 16): L: 0.703372; A: 0.625000; R: 0.625000; P: 0.646306; F1: 0.618575\n",
      "(valid @ 16): L: 0.815867; A: 0.535088; R: 0.535088; P: 0.320782; F1: 0.396992\n",
      "(train @ 17): L: 0.762133; A: 0.585526; R: 0.585526; P: 0.584965; F1: 0.584144\n",
      "(valid @ 17): L: 0.745710; A: 0.692982; R: 0.692982; P: 0.728626; F1: 0.688764\n",
      "(train @ 18): L: 0.684900; A: 0.692982; R: 0.692982; P: 0.698794; F1: 0.692549\n",
      "(valid @ 18): L: 0.667343; A: 0.714912; R: 0.714912; P: 0.750610; F1: 0.714615\n",
      "(train @ 19): L: 0.650434; A: 0.677632; R: 0.677632; P: 0.695936; F1: 0.677594\n",
      "(valid @ 19): L: 0.618688; A: 0.697368; R: 0.697368; P: 0.730998; F1: 0.687683\n",
      "(train @ 20): L: 0.642025; A: 0.679825; R: 0.679825; P: 0.690446; F1: 0.678576\n",
      "(valid @ 20): L: 0.626334; A: 0.701754; R: 0.701754; P: 0.728762; F1: 0.697980\n",
      "(train @ 21): L: 0.655477; A: 0.640351; R: 0.640351; P: 0.647548; F1: 0.636744\n",
      "(valid @ 21): L: 0.724687; A: 0.662281; R: 0.662281; P: 0.702742; F1: 0.652447\n",
      "(train @ 22): L: 0.630403; A: 0.662281; R: 0.662281; P: 0.676194; F1: 0.657934\n",
      "(valid @ 22): L: 0.602361; A: 0.719298; R: 0.719298; P: 0.745778; F1: 0.717356\n",
      "(train @ 23): L: 0.622092; A: 0.692982; R: 0.692982; P: 0.705485; F1: 0.687195\n",
      "(valid @ 23): L: 0.726885; A: 0.622807; R: 0.622807; P: 0.689034; F1: 0.603355\n",
      "(train @ 24): L: 0.617188; A: 0.719298; R: 0.719298; P: 0.721468; F1: 0.718508\n",
      "(valid @ 24): L: 0.626045; A: 0.728070; R: 0.728070; P: 0.765622; F1: 0.728144\n",
      "(train @ 25): L: 0.577768; A: 0.754386; R: 0.754386; P: 0.760319; F1: 0.754734\n",
      "(valid @ 25): L: 0.570007; A: 0.750000; R: 0.750000; P: 0.784785; F1: 0.749176\n",
      "(train @ 26): L: 0.554214; A: 0.745614; R: 0.745614; P: 0.762553; F1: 0.743671\n",
      "(valid @ 26): L: 0.548979; A: 0.780702; R: 0.780702; P: 0.832916; F1: 0.778902\n",
      "(train @ 27): L: 0.555963; A: 0.736842; R: 0.736842; P: 0.737852; F1: 0.736143\n",
      "(valid @ 27): L: 0.564501; A: 0.728070; R: 0.728070; P: 0.790058; F1: 0.724696\n",
      "(train @ 28): L: 0.538118; A: 0.754386; R: 0.754386; P: 0.760649; F1: 0.754622\n",
      "(valid @ 28): L: 0.501793; A: 0.776316; R: 0.776316; P: 0.782783; F1: 0.776638\n",
      "(train @ 29): L: 0.496151; A: 0.785088; R: 0.785088; P: 0.785684; F1: 0.785066\n",
      "(valid @ 29): L: 0.604719; A: 0.736842; R: 0.736842; P: 0.772765; F1: 0.734440\n",
      "(train @ 30): L: 0.521540; A: 0.763158; R: 0.763158; P: 0.764727; F1: 0.763549\n",
      "(valid @ 30): L: 0.606769; A: 0.732456; R: 0.732456; P: 0.753981; F1: 0.730477\n",
      "Best val Metric 0.778902 @ 26\n",
      "\n",
      "models are saved @ ./predictions/211209022331/flvl_1_3_4_6_7_9_pretrained_model_c1.pt\n",
      "Saving predictions @ ./predictions/211209022331/flvl_train_1_3_4_6_7_9_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209022331/flvl_valid_2_5_8_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209022331/flvl_public_test_trained_on_1_3_4_6_7_9_r21d_rgb.csv\n",
      "{'train': [2, 3, 5, 6, 8, 9], 'valid': [1, 4, 7], 'public_test': [10, 11, 12], 'private_test': [13, 14, 15]}\n",
      "(train @ 1): L: 1.417588; A: 0.385965; R: 0.385965; P: 0.379385; F1: 0.375487\n",
      "(valid @ 1): L: 1.019435; A: 0.526316; R: 0.526316; P: 0.470543; F1: 0.476935\n",
      "(train @ 2): L: 1.001278; A: 0.517544; R: 0.517544; P: 0.434741; F1: 0.471016\n",
      "(valid @ 2): L: 1.022052; A: 0.500000; R: 0.500000; P: 0.456548; F1: 0.442138\n",
      "(train @ 3): L: 0.976690; A: 0.482456; R: 0.482456; P: 0.559862; F1: 0.451033\n",
      "(valid @ 3): L: 1.003656; A: 0.521930; R: 0.521930; P: 0.551672; F1: 0.520475\n",
      "(train @ 4): L: 0.992119; A: 0.453947; R: 0.453947; P: 0.460401; F1: 0.430838\n",
      "(valid @ 4): L: 1.122702; A: 0.421053; R: 0.421053; P: 0.177285; F1: 0.249513\n",
      "(train @ 5): L: 0.946352; A: 0.456140; R: 0.456140; P: 0.457063; F1: 0.446431\n",
      "(valid @ 5): L: 0.969295; A: 0.618421; R: 0.618421; P: 0.672904; F1: 0.611919\n",
      "(train @ 6): L: 0.853937; A: 0.548246; R: 0.548246; P: 0.656506; F1: 0.506352\n",
      "(valid @ 6): L: 0.993094; A: 0.570175; R: 0.570175; P: 0.673688; F1: 0.516112\n",
      "(train @ 7): L: 0.810562; A: 0.603070; R: 0.603070; P: 0.607432; F1: 0.601354\n",
      "(valid @ 7): L: 0.931536; A: 0.539474; R: 0.539474; P: 0.616725; F1: 0.521349\n",
      "(train @ 8): L: 0.821195; A: 0.526316; R: 0.526316; P: 0.527086; F1: 0.526650\n",
      "(valid @ 8): L: 0.961932; A: 0.500000; R: 0.500000; P: 0.613231; F1: 0.436886\n",
      "(train @ 9): L: 0.856202; A: 0.532895; R: 0.532895; P: 0.545126; F1: 0.531448\n",
      "(valid @ 9): L: 0.807190; A: 0.631579; R: 0.631579; P: 0.631120; F1: 0.631224\n",
      "(train @ 10): L: 0.868513; A: 0.528509; R: 0.528509; P: 0.535694; F1: 0.525923\n",
      "(valid @ 10): L: 0.852506; A: 0.614035; R: 0.614035; P: 0.750877; F1: 0.574283\n",
      "(train @ 11): L: 0.779778; A: 0.607456; R: 0.607456; P: 0.618939; F1: 0.598736\n",
      "(valid @ 11): L: 0.881091; A: 0.614035; R: 0.614035; P: 0.669173; F1: 0.601139\n",
      "(train @ 12): L: 0.759991; A: 0.581140; R: 0.581140; P: 0.647106; F1: 0.548448\n",
      "(valid @ 12): L: 0.848353; A: 0.473684; R: 0.473684; P: 0.458787; F1: 0.394104\n",
      "(train @ 13): L: 0.760159; A: 0.559211; R: 0.559211; P: 0.574027; F1: 0.559882\n",
      "(valid @ 13): L: 0.813045; A: 0.530702; R: 0.530702; P: 0.357013; F1: 0.399797\n",
      "(train @ 14): L: 0.744556; A: 0.567982; R: 0.567982; P: 0.562630; F1: 0.556641\n",
      "(valid @ 14): L: 0.801382; A: 0.614035; R: 0.614035; P: 0.672769; F1: 0.606436\n",
      "(train @ 15): L: 0.697960; A: 0.622807; R: 0.622807; P: 0.623067; F1: 0.622383\n",
      "(valid @ 15): L: 0.780077; A: 0.622807; R: 0.622807; P: 0.674007; F1: 0.617447\n",
      "(train @ 16): L: 0.720339; A: 0.609649; R: 0.609649; P: 0.635684; F1: 0.604548\n",
      "(valid @ 16): L: 0.833269; A: 0.539474; R: 0.539474; P: 0.358995; F1: 0.407534\n",
      "(train @ 17): L: 0.751261; A: 0.589912; R: 0.589912; P: 0.591018; F1: 0.586672\n",
      "(valid @ 17): L: 0.956494; A: 0.653509; R: 0.653509; P: 0.701377; F1: 0.629711\n",
      "(train @ 18): L: 0.693054; A: 0.690789; R: 0.690789; P: 0.693869; F1: 0.691257\n",
      "(valid @ 18): L: 0.753763; A: 0.706140; R: 0.706140; P: 0.731320; F1: 0.702723\n",
      "(train @ 19): L: 0.625053; A: 0.684211; R: 0.684211; P: 0.704347; F1: 0.685663\n",
      "(valid @ 19): L: 0.680575; A: 0.706140; R: 0.706140; P: 0.723563; F1: 0.709001\n",
      "(train @ 20): L: 0.659560; A: 0.642544; R: 0.642544; P: 0.645229; F1: 0.640018\n",
      "(valid @ 20): L: 0.750347; A: 0.719298; R: 0.719298; P: 0.766184; F1: 0.713929\n",
      "(train @ 21): L: 0.735984; A: 0.598684; R: 0.598684; P: 0.611767; F1: 0.587788\n",
      "(valid @ 21): L: 1.088968; A: 0.482456; R: 0.482456; P: 0.767831; F1: 0.353243\n",
      "(train @ 22): L: 0.704254; A: 0.614035; R: 0.614035; P: 0.621643; F1: 0.610571\n",
      "(valid @ 22): L: 0.670103; A: 0.745614; R: 0.745614; P: 0.783914; F1: 0.745673\n",
      "(train @ 23): L: 0.614229; A: 0.701754; R: 0.701754; P: 0.706278; F1: 0.699423\n",
      "(valid @ 23): L: 0.753699; A: 0.728070; R: 0.728070; P: 0.752797; F1: 0.725556\n",
      "(train @ 24): L: 0.568063; A: 0.754386; R: 0.754386; P: 0.756968; F1: 0.753968\n",
      "(valid @ 24): L: 0.668666; A: 0.697368; R: 0.697368; P: 0.721809; F1: 0.698968\n",
      "(train @ 25): L: 0.591824; A: 0.699561; R: 0.699561; P: 0.711414; F1: 0.697914\n",
      "(valid @ 25): L: 0.613343; A: 0.741228; R: 0.741228; P: 0.758226; F1: 0.741467\n",
      "(train @ 26): L: 0.563237; A: 0.719298; R: 0.719298; P: 0.743164; F1: 0.716134\n",
      "(valid @ 26): L: 0.649390; A: 0.763158; R: 0.763158; P: 0.781045; F1: 0.761760\n",
      "(train @ 27): L: 0.520712; A: 0.767544; R: 0.767544; P: 0.774753; F1: 0.767240\n",
      "(valid @ 27): L: 0.630302; A: 0.736842; R: 0.736842; P: 0.755739; F1: 0.737786\n",
      "(train @ 28): L: 0.533448; A: 0.743421; R: 0.743421; P: 0.754375; F1: 0.743293\n",
      "(valid @ 28): L: 0.604746; A: 0.754386; R: 0.754386; P: 0.791226; F1: 0.753304\n",
      "(train @ 29): L: 0.505240; A: 0.774123; R: 0.774123; P: 0.774215; F1: 0.773810\n",
      "(valid @ 29): L: 0.785024; A: 0.719298; R: 0.719298; P: 0.747214; F1: 0.709927\n",
      "(train @ 30): L: 0.542941; A: 0.741228; R: 0.741228; P: 0.743799; F1: 0.741637\n",
      "(valid @ 30): L: 0.764392; A: 0.710526; R: 0.710526; P: 0.769035; F1: 0.702188\n",
      "Best val Metric 0.761760 @ 26\n",
      "\n",
      "models are saved @ ./predictions/211209022331/flvl_2_3_5_6_8_9_pretrained_model_c1.pt\n",
      "Saving predictions @ ./predictions/211209022331/flvl_train_2_3_5_6_8_9_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209022331/flvl_valid_1_4_7_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209022331/flvl_public_test_trained_on_2_3_5_6_8_9_r21d_rgb.csv\n",
      "Average of Best Metrics on Each Valid Set: 0.752383, 211209022331\n",
      "Saved test results @ ./predictions/211209022331/flvl_public_test_agg_r21d_rgb.csv\n"
     ]
    }
   ],
   "source": [
    "# View 0,1,2\n",
    "from main_3Views import Config, run_kfold\n",
    "\n",
    "# define architecture and run k-fold training\n",
    "cfg = Config()\n",
    "cfg.assign_variable('task', 'flvl')\n",
    "cfg.assign_variable('output_dim', 3)\n",
    "cfg.assign_variable('model_type', 'GRU')\n",
    "cfg.assign_variable('bi_dir', False)\n",
    "cfg.assign_variable('device', 'cuda:0')\n",
    "cfg.assign_variable('data_root', './r21d_rgb_features')\n",
    "cfg.assign_variable('drop_p', 0.0) # results will be irreproducible\n",
    "cfg.assign_variable('batch_size', 64)\n",
    "cfg.assign_variable('input_dim', 512)\n",
    "cfg.assign_variable('hidden_dim', 512)\n",
    "cfg.assign_variable('n_layers', 3)\n",
    "cfg.assign_variable('num_epochs', 30)\n",
    "cfg.assign_variable('seed', 1337)\n",
    "\n",
    "run_kfold(cfg,[0,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': [1, 2, 4, 5, 7, 8], 'valid': [3, 6, 9], 'public_test': [10, 11, 12], 'private_test': [13, 14, 15]}\n",
      "(train @ 1): L: 1.410031; A: 0.401316; R: 0.401316; P: 0.400674; F1: 0.392184\n",
      "(valid @ 1): L: 1.181156; A: 0.508772; R: 0.508772; P: 0.572172; F1: 0.479817\n",
      "(train @ 2): L: 1.016077; A: 0.473684; R: 0.473684; P: 0.522547; F1: 0.444212\n",
      "(valid @ 2): L: 1.072403; A: 0.469298; R: 0.469298; P: 0.426429; F1: 0.404527\n",
      "(train @ 3): L: 0.946185; A: 0.508772; R: 0.508772; P: 0.536387; F1: 0.469896\n",
      "(valid @ 3): L: 1.059538; A: 0.451754; R: 0.451754; P: 0.509654; F1: 0.418853\n",
      "(train @ 4): L: 0.911989; A: 0.550439; R: 0.550439; P: 0.593012; F1: 0.542074\n",
      "(valid @ 4): L: 1.042794; A: 0.478070; R: 0.478070; P: 0.435605; F1: 0.415544\n",
      "(train @ 5): L: 0.880510; A: 0.557018; R: 0.557018; P: 0.609568; F1: 0.537709\n",
      "(valid @ 5): L: 1.014172; A: 0.517544; R: 0.517544; P: 0.589007; F1: 0.484359\n",
      "(train @ 6): L: 0.863581; A: 0.557018; R: 0.557018; P: 0.642481; F1: 0.525792\n",
      "(valid @ 6): L: 0.955615; A: 0.570175; R: 0.570175; P: 0.608508; F1: 0.573121\n",
      "(train @ 7): L: 0.801510; A: 0.592105; R: 0.592105; P: 0.611386; F1: 0.589093\n",
      "(valid @ 7): L: 0.949728; A: 0.574561; R: 0.574561; P: 0.580909; F1: 0.548590\n",
      "(train @ 8): L: 0.780995; A: 0.592105; R: 0.592105; P: 0.595630; F1: 0.592228\n",
      "(valid @ 8): L: 0.995883; A: 0.530702; R: 0.530702; P: 0.592412; F1: 0.505166\n",
      "(train @ 9): L: 0.784263; A: 0.583333; R: 0.583333; P: 0.600673; F1: 0.579180\n",
      "(valid @ 9): L: 0.922890; A: 0.596491; R: 0.596491; P: 0.624078; F1: 0.581636\n",
      "(train @ 10): L: 0.735924; A: 0.609649; R: 0.609649; P: 0.639881; F1: 0.597636\n",
      "(valid @ 10): L: 0.899446; A: 0.609649; R: 0.609649; P: 0.609679; F1: 0.605087\n",
      "(train @ 11): L: 0.715344; A: 0.666667; R: 0.666667; P: 0.677854; F1: 0.664544\n",
      "(valid @ 11): L: 0.923741; A: 0.583333; R: 0.583333; P: 0.586115; F1: 0.558644\n",
      "(train @ 12): L: 0.739189; A: 0.594298; R: 0.594298; P: 0.598465; F1: 0.589669\n",
      "(valid @ 12): L: 0.829236; A: 0.535088; R: 0.535088; P: 0.740363; F1: 0.415730\n",
      "(train @ 13): L: 0.715082; A: 0.557018; R: 0.557018; P: 0.562600; F1: 0.549154\n",
      "(valid @ 13): L: 0.820999; A: 0.535088; R: 0.535088; P: 0.733314; F1: 0.420117\n",
      "(train @ 14): L: 0.665041; A: 0.651316; R: 0.651316; P: 0.653047; F1: 0.646678\n",
      "(valid @ 14): L: 0.804825; A: 0.605263; R: 0.605263; P: 0.613731; F1: 0.589659\n",
      "(train @ 15): L: 0.659433; A: 0.649123; R: 0.649123; P: 0.649413; F1: 0.648942\n",
      "(valid @ 15): L: 0.843356; A: 0.596491; R: 0.596491; P: 0.605068; F1: 0.581481\n",
      "(train @ 16): L: 0.654140; A: 0.657895; R: 0.657895; P: 0.680559; F1: 0.650843\n",
      "(valid @ 16): L: 0.979203; A: 0.526316; R: 0.526316; P: 0.311111; F1: 0.388798\n",
      "(train @ 17): L: 0.680455; A: 0.633772; R: 0.633772; P: 0.632525; F1: 0.631346\n",
      "(valid @ 17): L: 0.758194; A: 0.684211; R: 0.684211; P: 0.684810; F1: 0.684448\n",
      "(train @ 18): L: 0.622878; A: 0.717105; R: 0.717105; P: 0.724815; F1: 0.716986\n",
      "(valid @ 18): L: 0.791003; A: 0.631579; R: 0.631579; P: 0.659103; F1: 0.633241\n",
      "(train @ 19): L: 0.669144; A: 0.673246; R: 0.673246; P: 0.698423; F1: 0.669429\n",
      "(valid @ 19): L: 0.835696; A: 0.587719; R: 0.587719; P: 0.724702; F1: 0.519322\n",
      "(train @ 20): L: 0.557815; A: 0.723684; R: 0.723684; P: 0.738418; F1: 0.721817\n",
      "(valid @ 20): L: 0.760600; A: 0.644737; R: 0.644737; P: 0.663022; F1: 0.631297\n",
      "(train @ 21): L: 0.544500; A: 0.754386; R: 0.754386; P: 0.766058; F1: 0.751598\n",
      "(valid @ 21): L: 0.767788; A: 0.618421; R: 0.618421; P: 0.651111; F1: 0.589841\n",
      "(train @ 22): L: 0.525110; A: 0.747807; R: 0.747807; P: 0.760820; F1: 0.746116\n",
      "(valid @ 22): L: 0.747239; A: 0.706140; R: 0.706140; P: 0.709051; F1: 0.704332\n",
      "(train @ 23): L: 0.541189; A: 0.728070; R: 0.728070; P: 0.738179; F1: 0.726993\n",
      "(valid @ 23): L: 0.770203; A: 0.662281; R: 0.662281; P: 0.690759; F1: 0.651149\n",
      "(train @ 24): L: 0.494547; A: 0.774123; R: 0.774123; P: 0.775467; F1: 0.773593\n",
      "(valid @ 24): L: 0.718341; A: 0.697368; R: 0.697368; P: 0.699248; F1: 0.694570\n",
      "(train @ 25): L: 0.491323; A: 0.745614; R: 0.745614; P: 0.749650; F1: 0.745102\n",
      "(valid @ 25): L: 0.740880; A: 0.679825; R: 0.679825; P: 0.682263; F1: 0.675179\n",
      "(train @ 26): L: 0.515449; A: 0.736842; R: 0.736842; P: 0.742992; F1: 0.735255\n",
      "(valid @ 26): L: 0.782494; A: 0.649123; R: 0.649123; P: 0.666751; F1: 0.631182\n",
      "(train @ 27): L: 0.546923; A: 0.714912; R: 0.714912; P: 0.715121; F1: 0.714961\n",
      "(valid @ 27): L: 0.781601; A: 0.644737; R: 0.644737; P: 0.666132; F1: 0.625772\n",
      "(train @ 28): L: 0.542552; A: 0.734649; R: 0.734649; P: 0.740813; F1: 0.735348\n",
      "(valid @ 28): L: 0.913447; A: 0.657895; R: 0.657895; P: 0.701272; F1: 0.656970\n",
      "(train @ 29): L: 0.464775; A: 0.800439; R: 0.800439; P: 0.812389; F1: 0.800169\n",
      "(valid @ 29): L: 0.689668; A: 0.679825; R: 0.679825; P: 0.684362; F1: 0.677597\n",
      "(train @ 30): L: 0.382029; A: 0.828947; R: 0.828947; P: 0.828415; F1: 0.828490\n",
      "(valid @ 30): L: 0.707120; A: 0.679825; R: 0.679825; P: 0.693185; F1: 0.676735\n",
      "Best val Metric 0.704332 @ 22\n",
      "\n",
      "models are saved @ ./predictions/211209025535/flvl_1_2_4_5_7_8_pretrained_model_c1.pt\n",
      "Saving predictions @ ./predictions/211209025535/flvl_train_1_2_4_5_7_8_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209025535/flvl_valid_3_6_9_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209025535/flvl_public_test_trained_on_1_2_4_5_7_8_r21d_rgb.csv\n",
      "{'train': [1, 3, 4, 6, 7, 9], 'valid': [2, 5, 8], 'public_test': [10, 11, 12], 'private_test': [13, 14, 15]}\n",
      "(train @ 1): L: 1.438661; A: 0.381579; R: 0.381579; P: 0.368678; F1: 0.369259\n",
      "(valid @ 1): L: 0.991607; A: 0.521930; R: 0.521930; P: 0.475281; F1: 0.467501\n",
      "(train @ 2): L: 0.984772; A: 0.475877; R: 0.475877; P: 0.397519; F1: 0.432101\n",
      "(valid @ 2): L: 0.999040; A: 0.504386; R: 0.504386; P: 0.460449; F1: 0.447305\n",
      "(train @ 3): L: 0.945252; A: 0.462719; R: 0.462719; P: 0.517175; F1: 0.436010\n",
      "(valid @ 3): L: 0.977309; A: 0.596491; R: 0.596491; P: 0.625194; F1: 0.585635\n",
      "(train @ 4): L: 1.023705; A: 0.484649; R: 0.484649; P: 0.484998; F1: 0.471069\n",
      "(valid @ 4): L: 1.102370; A: 0.557018; R: 0.557018; P: 0.474666; F1: 0.509692\n",
      "(train @ 5): L: 0.962538; A: 0.500000; R: 0.500000; P: 0.530569; F1: 0.480027\n",
      "(valid @ 5): L: 0.952554; A: 0.592105; R: 0.592105; P: 0.674858; F1: 0.576738\n",
      "(train @ 6): L: 0.875908; A: 0.517544; R: 0.517544; P: 0.591240; F1: 0.473972\n",
      "(valid @ 6): L: 0.945549; A: 0.535088; R: 0.535088; P: 0.660580; F1: 0.452907\n",
      "(train @ 7): L: 0.819405; A: 0.587719; R: 0.587719; P: 0.603148; F1: 0.583598\n",
      "(valid @ 7): L: 0.911207; A: 0.570175; R: 0.570175; P: 0.624396; F1: 0.564357\n",
      "(train @ 8): L: 0.815239; A: 0.572368; R: 0.572368; P: 0.572020; F1: 0.571733\n",
      "(valid @ 8): L: 0.923020; A: 0.491228; R: 0.491228; P: 0.591479; F1: 0.423385\n",
      "(train @ 9): L: 0.838534; A: 0.532895; R: 0.532895; P: 0.543779; F1: 0.533063\n",
      "(valid @ 9): L: 0.782990; A: 0.697368; R: 0.697368; P: 0.708784; F1: 0.692804\n",
      "(train @ 10): L: 0.852016; A: 0.532895; R: 0.532895; P: 0.542160; F1: 0.531505\n",
      "(valid @ 10): L: 0.786159; A: 0.622807; R: 0.622807; P: 0.658214; F1: 0.606543\n",
      "(train @ 11): L: 0.780152; A: 0.616228; R: 0.616228; P: 0.638115; F1: 0.605413\n",
      "(valid @ 11): L: 0.828505; A: 0.614035; R: 0.614035; P: 0.669276; F1: 0.608021\n",
      "(train @ 12): L: 0.761315; A: 0.605263; R: 0.605263; P: 0.634753; F1: 0.590667\n",
      "(valid @ 12): L: 0.820297; A: 0.504386; R: 0.504386; P: 0.350791; F1: 0.381708\n",
      "(train @ 13): L: 0.772941; A: 0.535088; R: 0.535088; P: 0.543117; F1: 0.527358\n",
      "(valid @ 13): L: 0.812050; A: 0.530702; R: 0.530702; P: 0.329206; F1: 0.395526\n",
      "(train @ 14): L: 0.767590; A: 0.557018; R: 0.557018; P: 0.556763; F1: 0.550973\n",
      "(valid @ 14): L: 0.733283; A: 0.631579; R: 0.631579; P: 0.679198; F1: 0.624784\n",
      "(train @ 15): L: 0.723825; A: 0.618421; R: 0.618421; P: 0.615841; F1: 0.613923\n",
      "(valid @ 15): L: 0.737071; A: 0.631579; R: 0.631579; P: 0.675278; F1: 0.627149\n",
      "(train @ 16): L: 0.720059; A: 0.614035; R: 0.614035; P: 0.628685; F1: 0.608477\n",
      "(valid @ 16): L: 0.843608; A: 0.530702; R: 0.530702; P: 0.318494; F1: 0.393985\n",
      "(train @ 17): L: 0.773209; A: 0.581140; R: 0.581140; P: 0.578571; F1: 0.578628\n",
      "(valid @ 17): L: 0.791419; A: 0.679825; R: 0.679825; P: 0.725159; F1: 0.670950\n",
      "(train @ 18): L: 0.704912; A: 0.662281; R: 0.662281; P: 0.664757; F1: 0.660834\n",
      "(valid @ 18): L: 0.701206; A: 0.706140; R: 0.706140; P: 0.733194; F1: 0.706212\n",
      "(train @ 19): L: 0.671994; A: 0.655702; R: 0.655702; P: 0.678759; F1: 0.652843\n",
      "(valid @ 19): L: 0.671557; A: 0.627193; R: 0.627193; P: 0.710705; F1: 0.581608\n",
      "(train @ 20): L: 0.654895; A: 0.668860; R: 0.668860; P: 0.683492; F1: 0.665345\n",
      "(valid @ 20): L: 0.678863; A: 0.688596; R: 0.688596; P: 0.717767; F1: 0.682672\n",
      "(train @ 21): L: 0.652954; A: 0.653509; R: 0.653509; P: 0.663257; F1: 0.648419\n",
      "(valid @ 21): L: 0.795109; A: 0.552632; R: 0.552632; P: 0.640991; F1: 0.481212\n",
      "(train @ 22): L: 0.640471; A: 0.649123; R: 0.649123; P: 0.664853; F1: 0.643387\n",
      "(valid @ 22): L: 0.632996; A: 0.706140; R: 0.706140; P: 0.753629; F1: 0.695240\n",
      "(train @ 23): L: 0.623264; A: 0.701754; R: 0.701754; P: 0.716619; F1: 0.695601\n",
      "(valid @ 23): L: 0.754369; A: 0.618421; R: 0.618421; P: 0.709645; F1: 0.587480\n",
      "(train @ 24): L: 0.611143; A: 0.730263; R: 0.730263; P: 0.732793; F1: 0.729389\n",
      "(valid @ 24): L: 0.665162; A: 0.706140; R: 0.706140; P: 0.751258; F1: 0.705271\n",
      "(train @ 25): L: 0.566117; A: 0.741228; R: 0.741228; P: 0.751265; F1: 0.740047\n",
      "(valid @ 25): L: 0.603276; A: 0.736842; R: 0.736842; P: 0.781555; F1: 0.731882\n",
      "(train @ 26): L: 0.554414; A: 0.736842; R: 0.736842; P: 0.757729; F1: 0.733508\n",
      "(valid @ 26): L: 0.585422; A: 0.701754; R: 0.701754; P: 0.762971; F1: 0.689469\n",
      "(train @ 27): L: 0.561877; A: 0.743421; R: 0.743421; P: 0.743204; F1: 0.742775\n",
      "(valid @ 27): L: 0.683886; A: 0.649123; R: 0.649123; P: 0.730024; F1: 0.630357\n",
      "(train @ 28): L: 0.583565; A: 0.725877; R: 0.725877; P: 0.728615; F1: 0.726320\n",
      "(valid @ 28): L: 0.556159; A: 0.754386; R: 0.754386; P: 0.754192; F1: 0.753532\n",
      "(train @ 29): L: 0.509074; A: 0.763158; R: 0.763158; P: 0.762393; F1: 0.762296\n",
      "(valid @ 29): L: 0.664929; A: 0.662281; R: 0.662281; P: 0.717604; F1: 0.655236\n",
      "(train @ 30): L: 0.500028; A: 0.771930; R: 0.771930; P: 0.772164; F1: 0.771686\n",
      "(valid @ 30): L: 0.620296; A: 0.719298; R: 0.719298; P: 0.745049; F1: 0.716716\n",
      "Best val Metric 0.753532 @ 28\n",
      "\n",
      "models are saved @ ./predictions/211209025535/flvl_1_3_4_6_7_9_pretrained_model_c1.pt\n",
      "Saving predictions @ ./predictions/211209025535/flvl_train_1_3_4_6_7_9_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209025535/flvl_valid_2_5_8_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209025535/flvl_public_test_trained_on_1_3_4_6_7_9_r21d_rgb.csv\n",
      "{'train': [2, 3, 5, 6, 8, 9], 'valid': [1, 4, 7], 'public_test': [10, 11, 12], 'private_test': [13, 14, 15]}\n",
      "(train @ 1): L: 1.433253; A: 0.381579; R: 0.381579; P: 0.369447; F1: 0.369534\n",
      "(valid @ 1): L: 1.011422; A: 0.530702; R: 0.530702; P: 0.477885; F1: 0.479621\n",
      "(train @ 2): L: 0.998568; A: 0.491228; R: 0.491228; P: 0.411483; F1: 0.441997\n",
      "(valid @ 2): L: 1.020827; A: 0.495614; R: 0.495614; P: 0.452560; F1: 0.436923\n",
      "(train @ 3): L: 0.974175; A: 0.489035; R: 0.489035; P: 0.564677; F1: 0.459905\n",
      "(valid @ 3): L: 0.979279; A: 0.539474; R: 0.539474; P: 0.555927; F1: 0.536487\n",
      "(train @ 4): L: 0.989813; A: 0.460526; R: 0.460526; P: 0.454853; F1: 0.430982\n",
      "(valid @ 4): L: 1.107316; A: 0.421053; R: 0.421053; P: 0.177285; F1: 0.249513\n",
      "(train @ 5): L: 0.943916; A: 0.442982; R: 0.442982; P: 0.441966; F1: 0.431513\n",
      "(valid @ 5): L: 0.970516; A: 0.592105; R: 0.592105; P: 0.651370; F1: 0.583345\n",
      "(train @ 6): L: 0.852202; A: 0.543860; R: 0.543860; P: 0.641545; F1: 0.500786\n",
      "(valid @ 6): L: 0.988999; A: 0.521930; R: 0.521930; P: 0.633805; F1: 0.458260\n",
      "(train @ 7): L: 0.808390; A: 0.572368; R: 0.572368; P: 0.576376; F1: 0.571086\n",
      "(valid @ 7): L: 0.911004; A: 0.561404; R: 0.561404; P: 0.630816; F1: 0.548758\n",
      "(train @ 8): L: 0.815920; A: 0.541667; R: 0.541667; P: 0.543456; F1: 0.542456\n",
      "(valid @ 8): L: 0.968910; A: 0.504386; R: 0.504386; P: 0.598268; F1: 0.446884\n",
      "(train @ 9): L: 0.854800; A: 0.526316; R: 0.526316; P: 0.535712; F1: 0.525674\n",
      "(valid @ 9): L: 0.806146; A: 0.684211; R: 0.684211; P: 0.693850; F1: 0.686535\n",
      "(train @ 10): L: 0.838202; A: 0.535088; R: 0.535088; P: 0.547807; F1: 0.531619\n",
      "(valid @ 10): L: 0.843282; A: 0.561404; R: 0.561404; P: 0.688186; F1: 0.489999\n",
      "(train @ 11): L: 0.782702; A: 0.585526; R: 0.585526; P: 0.596150; F1: 0.579277\n",
      "(valid @ 11): L: 0.902542; A: 0.583333; R: 0.583333; P: 0.647813; F1: 0.572070\n",
      "(train @ 12): L: 0.760097; A: 0.578947; R: 0.578947; P: 0.648204; F1: 0.546385\n",
      "(valid @ 12): L: 0.908317; A: 0.456140; R: 0.456140; P: 0.339525; F1: 0.344184\n",
      "(train @ 13): L: 0.783477; A: 0.554825; R: 0.554825; P: 0.564725; F1: 0.555229\n",
      "(valid @ 13): L: 0.803235; A: 0.535088; R: 0.535088; P: 0.357999; F1: 0.403710\n",
      "(train @ 14): L: 0.749769; A: 0.563596; R: 0.563596; P: 0.561179; F1: 0.552124\n",
      "(valid @ 14): L: 0.792852; A: 0.618421; R: 0.618421; P: 0.670356; F1: 0.612576\n",
      "(train @ 15): L: 0.724596; A: 0.611842; R: 0.611842; P: 0.609645; F1: 0.609795\n",
      "(valid @ 15): L: 0.852545; A: 0.618421; R: 0.618421; P: 0.674498; F1: 0.611348\n",
      "(train @ 16): L: 0.700817; A: 0.625000; R: 0.625000; P: 0.650048; F1: 0.608474\n",
      "(valid @ 16): L: 0.880666; A: 0.539474; R: 0.539474; P: 0.358995; F1: 0.407534\n",
      "(train @ 17): L: 0.748815; A: 0.594298; R: 0.594298; P: 0.590185; F1: 0.590713\n",
      "(valid @ 17): L: 0.876312; A: 0.627193; R: 0.627193; P: 0.681287; F1: 0.620350\n",
      "(train @ 18): L: 0.671632; A: 0.607456; R: 0.607456; P: 0.604947; F1: 0.600911\n",
      "(valid @ 18): L: 0.824674; A: 0.653509; R: 0.653509; P: 0.689864; F1: 0.652132\n",
      "(train @ 19): L: 0.668627; A: 0.635965; R: 0.635965; P: 0.667941; F1: 0.627542\n",
      "(valid @ 19): L: 0.789618; A: 0.592105; R: 0.592105; P: 0.726732; F1: 0.544205\n",
      "(train @ 20): L: 0.636617; A: 0.673246; R: 0.673246; P: 0.690173; F1: 0.668478\n",
      "(valid @ 20): L: 0.792439; A: 0.657895; R: 0.657895; P: 0.689413; F1: 0.651950\n",
      "(train @ 21): L: 0.640123; A: 0.655702; R: 0.655702; P: 0.670644; F1: 0.650901\n",
      "(valid @ 21): L: 0.943453; A: 0.561404; R: 0.561404; P: 0.609174; F1: 0.523312\n",
      "(train @ 22): L: 0.612150; A: 0.668860; R: 0.668860; P: 0.684996; F1: 0.664167\n",
      "(valid @ 22): L: 0.738691; A: 0.675439; R: 0.675439; P: 0.707951; F1: 0.670638\n",
      "(train @ 23): L: 0.612634; A: 0.699561; R: 0.699561; P: 0.718824; F1: 0.693929\n",
      "(valid @ 23): L: 0.820647; A: 0.631579; R: 0.631579; P: 0.707075; F1: 0.617082\n",
      "(train @ 24): L: 0.560950; A: 0.745614; R: 0.745614; P: 0.747069; F1: 0.744704\n",
      "(valid @ 24): L: 0.772968; A: 0.666667; R: 0.666667; P: 0.705819; F1: 0.664172\n",
      "(train @ 25): L: 0.568782; A: 0.703947; R: 0.703947; P: 0.717176; F1: 0.702131\n",
      "(valid @ 25): L: 0.646435; A: 0.675439; R: 0.675439; P: 0.699613; F1: 0.678997\n",
      "(train @ 26): L: 0.541119; A: 0.741228; R: 0.741228; P: 0.759089; F1: 0.740218\n",
      "(valid @ 26): L: 0.671067; A: 0.719298; R: 0.719298; P: 0.750346; F1: 0.720100\n",
      "(train @ 27): L: 0.521095; A: 0.752193; R: 0.752193; P: 0.752207; F1: 0.751340\n",
      "(valid @ 27): L: 0.701189; A: 0.714912; R: 0.714912; P: 0.752590; F1: 0.712700\n",
      "(train @ 28): L: 0.522904; A: 0.734649; R: 0.734649; P: 0.746701; F1: 0.734028\n",
      "(valid @ 28): L: 0.722321; A: 0.675439; R: 0.675439; P: 0.718885; F1: 0.671656\n",
      "(train @ 29): L: 0.512367; A: 0.752193; R: 0.752193; P: 0.752834; F1: 0.751580\n",
      "(valid @ 29): L: 0.864259; A: 0.671053; R: 0.671053; P: 0.704795; F1: 0.667227\n",
      "(train @ 30): L: 0.521593; A: 0.758772; R: 0.758772; P: 0.762173; F1: 0.759043\n",
      "(valid @ 30): L: 0.951358; A: 0.605263; R: 0.605263; P: 0.654383; F1: 0.592169\n",
      "Best val Metric 0.720100 @ 26\n",
      "\n",
      "models are saved @ ./predictions/211209025535/flvl_2_3_5_6_8_9_pretrained_model_c1.pt\n",
      "Saving predictions @ ./predictions/211209025535/flvl_train_2_3_5_6_8_9_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209025535/flvl_valid_1_4_7_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209025535/flvl_public_test_trained_on_2_3_5_6_8_9_r21d_rgb.csv\n",
      "Average of Best Metrics on Each Valid Set: 0.725988, 211209025535\n",
      "Saved test results @ ./predictions/211209025535/flvl_public_test_agg_r21d_rgb.csv\n"
     ]
    }
   ],
   "source": [
    "# View 0,1,3\n",
    "from main_3Views import Config, run_kfold\n",
    "\n",
    "# define architecture and run k-fold training\n",
    "cfg = Config()\n",
    "cfg.assign_variable('task', 'flvl')\n",
    "cfg.assign_variable('output_dim', 3)\n",
    "cfg.assign_variable('model_type', 'GRU')\n",
    "cfg.assign_variable('bi_dir', False)\n",
    "cfg.assign_variable('device', 'cuda:0')\n",
    "cfg.assign_variable('data_root', './r21d_rgb_features')\n",
    "cfg.assign_variable('drop_p', 0.0) # results will be irreproducible\n",
    "cfg.assign_variable('batch_size', 64)\n",
    "cfg.assign_variable('input_dim', 512)\n",
    "cfg.assign_variable('hidden_dim', 512)\n",
    "cfg.assign_variable('n_layers', 3)\n",
    "cfg.assign_variable('num_epochs', 30)\n",
    "cfg.assign_variable('seed', 1337)\n",
    "\n",
    "run_kfold(cfg,[0,1,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': [1, 2, 4, 5, 7, 8], 'valid': [3, 6, 9], 'public_test': [10, 11, 12], 'private_test': [13, 14, 15]}\n",
      "(train @ 1): L: 1.406580; A: 0.394737; R: 0.394737; P: 0.394143; F1: 0.386342\n",
      "(valid @ 1): L: 1.155948; A: 0.513158; R: 0.513158; P: 0.602396; F1: 0.480317\n",
      "(train @ 2): L: 1.002382; A: 0.464912; R: 0.464912; P: 0.549262; F1: 0.428275\n",
      "(valid @ 2): L: 1.068077; A: 0.469298; R: 0.469298; P: 0.426429; F1: 0.404527\n",
      "(train @ 3): L: 0.945505; A: 0.508772; R: 0.508772; P: 0.587943; F1: 0.465069\n",
      "(valid @ 3): L: 1.054816; A: 0.478070; R: 0.478070; P: 0.558204; F1: 0.469382\n",
      "(train @ 4): L: 0.920173; A: 0.519737; R: 0.519737; P: 0.543947; F1: 0.512597\n",
      "(valid @ 4): L: 1.039019; A: 0.478070; R: 0.478070; P: 0.435605; F1: 0.415544\n",
      "(train @ 5): L: 0.882887; A: 0.552632; R: 0.552632; P: 0.604591; F1: 0.536836\n",
      "(valid @ 5): L: 0.991747; A: 0.521930; R: 0.521930; P: 0.580188; F1: 0.507212\n",
      "(train @ 6): L: 0.854502; A: 0.552632; R: 0.552632; P: 0.654584; F1: 0.513089\n",
      "(valid @ 6): L: 0.954590; A: 0.587719; R: 0.587719; P: 0.641627; F1: 0.564474\n",
      "(train @ 7): L: 0.790536; A: 0.592105; R: 0.592105; P: 0.606770; F1: 0.591660\n",
      "(valid @ 7): L: 0.943089; A: 0.596491; R: 0.596491; P: 0.602973; F1: 0.573767\n",
      "(train @ 8): L: 0.764586; A: 0.609649; R: 0.609649; P: 0.609851; F1: 0.609501\n",
      "(valid @ 8): L: 1.006714; A: 0.543860; R: 0.543860; P: 0.607433; F1: 0.522287\n",
      "(train @ 9): L: 0.772457; A: 0.570175; R: 0.570175; P: 0.580349; F1: 0.569410\n",
      "(valid @ 9): L: 0.899097; A: 0.627193; R: 0.627193; P: 0.626903; F1: 0.626438\n",
      "(train @ 10): L: 0.761123; A: 0.614035; R: 0.614035; P: 0.641525; F1: 0.609490\n",
      "(valid @ 10): L: 0.900702; A: 0.627193; R: 0.627193; P: 0.631715; F1: 0.623070\n",
      "(train @ 11): L: 0.726923; A: 0.671053; R: 0.671053; P: 0.687474; F1: 0.669112\n",
      "(valid @ 11): L: 0.873456; A: 0.574561; R: 0.574561; P: 0.582270; F1: 0.552135\n",
      "(train @ 12): L: 0.703202; A: 0.614035; R: 0.614035; P: 0.621481; F1: 0.608010\n",
      "(valid @ 12): L: 0.807840; A: 0.578947; R: 0.578947; P: 0.684609; F1: 0.503575\n",
      "(train @ 13): L: 0.692235; A: 0.603070; R: 0.603070; P: 0.608278; F1: 0.598115\n",
      "(valid @ 13): L: 0.812973; A: 0.539474; R: 0.539474; P: 0.665418; F1: 0.430013\n",
      "(train @ 14): L: 0.635567; A: 0.679825; R: 0.679825; P: 0.679775; F1: 0.679341\n",
      "(valid @ 14): L: 0.777879; A: 0.627193; R: 0.627193; P: 0.629762; F1: 0.621561\n",
      "(train @ 15): L: 0.626184; A: 0.703947; R: 0.703947; P: 0.708618; F1: 0.703904\n",
      "(valid @ 15): L: 0.830968; A: 0.605263; R: 0.605263; P: 0.612949; F1: 0.591859\n",
      "(train @ 16): L: 0.626645; A: 0.675439; R: 0.675439; P: 0.687637; F1: 0.673297\n",
      "(valid @ 16): L: 0.903432; A: 0.561404; R: 0.561404; P: 0.699690; F1: 0.467226\n",
      "(train @ 17): L: 0.619100; A: 0.701754; R: 0.701754; P: 0.708435; F1: 0.700041\n",
      "(valid @ 17): L: 0.744915; A: 0.640351; R: 0.640351; P: 0.672103; F1: 0.621186\n",
      "(train @ 18): L: 0.596912; A: 0.728070; R: 0.728070; P: 0.737845; F1: 0.727564\n",
      "(valid @ 18): L: 0.771649; A: 0.662281; R: 0.662281; P: 0.689824; F1: 0.659624\n",
      "(train @ 19): L: 0.632638; A: 0.714912; R: 0.714912; P: 0.723272; F1: 0.715661\n",
      "(valid @ 19): L: 0.729959; A: 0.692982; R: 0.692982; P: 0.727083; F1: 0.684598\n",
      "(train @ 20): L: 0.520947; A: 0.756579; R: 0.756579; P: 0.780283; F1: 0.753756\n",
      "(valid @ 20): L: 0.772511; A: 0.666667; R: 0.666667; P: 0.702174; F1: 0.652656\n",
      "(train @ 21): L: 0.523712; A: 0.739035; R: 0.739035; P: 0.752820; F1: 0.737341\n",
      "(valid @ 21): L: 0.779380; A: 0.631579; R: 0.631579; P: 0.723729; F1: 0.594838\n",
      "(train @ 22): L: 0.485776; A: 0.778509; R: 0.778509; P: 0.790225; F1: 0.777274\n",
      "(valid @ 22): L: 0.738836; A: 0.688596; R: 0.688596; P: 0.689008; F1: 0.686132\n",
      "(train @ 23): L: 0.560054; A: 0.708333; R: 0.708333; P: 0.711317; F1: 0.706625\n",
      "(valid @ 23): L: 0.733441; A: 0.706140; R: 0.706140; P: 0.756102; F1: 0.694535\n",
      "(train @ 24): L: 0.495602; A: 0.765351; R: 0.765351; P: 0.766383; F1: 0.763676\n",
      "(valid @ 24): L: 0.666008; A: 0.684211; R: 0.684211; P: 0.688503; F1: 0.682226\n",
      "(train @ 25): L: 0.477639; A: 0.765351; R: 0.765351; P: 0.773431; F1: 0.763908\n",
      "(valid @ 25): L: 0.653782; A: 0.697368; R: 0.697368; P: 0.707533; F1: 0.694188\n",
      "(train @ 26): L: 0.463514; A: 0.774123; R: 0.774123; P: 0.788639; F1: 0.773125\n",
      "(valid @ 26): L: 0.674020; A: 0.688596; R: 0.688596; P: 0.693204; F1: 0.685803\n",
      "(train @ 27): L: 0.414374; A: 0.820175; R: 0.820175; P: 0.828614; F1: 0.818513\n",
      "(valid @ 27): L: 0.667975; A: 0.697368; R: 0.697368; P: 0.699520; F1: 0.695797\n",
      "(train @ 28): L: 0.427210; A: 0.826754; R: 0.826754; P: 0.832752; F1: 0.826915\n",
      "(valid @ 28): L: 0.844494; A: 0.692982; R: 0.692982; P: 0.718455; F1: 0.693401\n",
      "(train @ 29): L: 0.358383; A: 0.842105; R: 0.842105; P: 0.848641; F1: 0.842076\n",
      "(valid @ 29): L: 0.732161; A: 0.697368; R: 0.697368; P: 0.705862; F1: 0.696244\n",
      "(train @ 30): L: 0.325395; A: 0.875000; R: 0.875000; P: 0.875903; F1: 0.874590\n",
      "(valid @ 30): L: 0.757314; A: 0.706140; R: 0.706140; P: 0.723097; F1: 0.703734\n",
      "Best val Metric 0.703734 @ 30\n",
      "\n",
      "models are saved @ ./predictions/211209032735/flvl_1_2_4_5_7_8_pretrained_model_c1.pt\n",
      "Saving predictions @ ./predictions/211209032735/flvl_train_1_2_4_5_7_8_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209032735/flvl_valid_3_6_9_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209032735/flvl_public_test_trained_on_1_2_4_5_7_8_r21d_rgb.csv\n",
      "{'train': [1, 3, 4, 6, 7, 9], 'valid': [2, 5, 8], 'public_test': [10, 11, 12], 'private_test': [13, 14, 15]}\n",
      "(train @ 1): L: 1.446587; A: 0.381579; R: 0.381579; P: 0.371265; F1: 0.367813\n",
      "(valid @ 1): L: 0.985107; A: 0.517544; R: 0.517544; P: 0.467426; F1: 0.465182\n",
      "(train @ 2): L: 0.975480; A: 0.482456; R: 0.482456; P: 0.403839; F1: 0.438987\n",
      "(valid @ 2): L: 0.998519; A: 0.504386; R: 0.504386; P: 0.460449; F1: 0.447305\n",
      "(train @ 3): L: 0.953266; A: 0.471491; R: 0.471491; P: 0.554189; F1: 0.433857\n",
      "(valid @ 3): L: 0.990386; A: 0.614035; R: 0.614035; P: 0.626303; F1: 0.603121\n",
      "(train @ 4): L: 1.026379; A: 0.469298; R: 0.469298; P: 0.467928; F1: 0.447048\n",
      "(valid @ 4): L: 1.105070; A: 0.570175; R: 0.570175; P: 0.494467; F1: 0.515840\n",
      "(train @ 5): L: 0.961110; A: 0.506579; R: 0.506579; P: 0.524068; F1: 0.491441\n",
      "(valid @ 5): L: 0.950303; A: 0.631579; R: 0.631579; P: 0.675823; F1: 0.625737\n",
      "(train @ 6): L: 0.877176; A: 0.517544; R: 0.517544; P: 0.626452; F1: 0.462766\n",
      "(valid @ 6): L: 0.963143; A: 0.521930; R: 0.521930; P: 0.738812; F1: 0.431956\n",
      "(train @ 7): L: 0.821734; A: 0.583333; R: 0.583333; P: 0.593421; F1: 0.576699\n",
      "(valid @ 7): L: 0.916284; A: 0.552632; R: 0.552632; P: 0.627214; F1: 0.536444\n",
      "(train @ 8): L: 0.823513; A: 0.559211; R: 0.559211; P: 0.558794; F1: 0.558749\n",
      "(valid @ 8): L: 0.930888; A: 0.491228; R: 0.491228; P: 0.589289; F1: 0.427199\n",
      "(train @ 9): L: 0.840253; A: 0.530702; R: 0.530702; P: 0.538882; F1: 0.530439\n",
      "(valid @ 9): L: 0.785058; A: 0.710526; R: 0.710526; P: 0.715175; F1: 0.707829\n",
      "(train @ 10): L: 0.856837; A: 0.521930; R: 0.521930; P: 0.529967; F1: 0.521513\n",
      "(valid @ 10): L: 0.782414; A: 0.640351; R: 0.640351; P: 0.676712; F1: 0.626089\n",
      "(train @ 11): L: 0.793144; A: 0.627193; R: 0.627193; P: 0.653732; F1: 0.617800\n",
      "(valid @ 11): L: 0.837493; A: 0.627193; R: 0.627193; P: 0.674797; F1: 0.620514\n",
      "(train @ 12): L: 0.756922; A: 0.589912; R: 0.589912; P: 0.640929; F1: 0.561897\n",
      "(valid @ 12): L: 0.825940; A: 0.504386; R: 0.504386; P: 0.350791; F1: 0.381708\n",
      "(train @ 13): L: 0.773513; A: 0.554825; R: 0.554825; P: 0.564860; F1: 0.551792\n",
      "(valid @ 13): L: 0.802102; A: 0.539474; R: 0.539474; P: 0.342240; F1: 0.404004\n",
      "(train @ 14): L: 0.753076; A: 0.557018; R: 0.557018; P: 0.555178; F1: 0.552337\n",
      "(valid @ 14): L: 0.726543; A: 0.688596; R: 0.688596; P: 0.718758; F1: 0.689130\n",
      "(train @ 15): L: 0.703485; A: 0.666667; R: 0.666667; P: 0.666886; F1: 0.664301\n",
      "(valid @ 15): L: 0.752180; A: 0.657895; R: 0.657895; P: 0.702154; F1: 0.655229\n",
      "(train @ 16): L: 0.690794; A: 0.635965; R: 0.635965; P: 0.647467; F1: 0.632867\n",
      "(valid @ 16): L: 0.797768; A: 0.539474; R: 0.539474; P: 0.323071; F1: 0.400000\n",
      "(train @ 17): L: 0.743391; A: 0.585526; R: 0.585526; P: 0.584929; F1: 0.583358\n",
      "(valid @ 17): L: 0.770995; A: 0.627193; R: 0.627193; P: 0.664464; F1: 0.611014\n",
      "(train @ 18): L: 0.678393; A: 0.708333; R: 0.708333; P: 0.714472; F1: 0.708703\n",
      "(valid @ 18): L: 0.733900; A: 0.710526; R: 0.710526; P: 0.744928; F1: 0.707012\n",
      "(train @ 19): L: 0.647021; A: 0.697368; R: 0.697368; P: 0.706270; F1: 0.697694\n",
      "(valid @ 19): L: 0.613040; A: 0.741228; R: 0.741228; P: 0.752438; F1: 0.739862\n",
      "(train @ 20): L: 0.630355; A: 0.679825; R: 0.679825; P: 0.698648; F1: 0.677486\n",
      "(valid @ 20): L: 0.615444; A: 0.728070; R: 0.728070; P: 0.738375; F1: 0.725697\n",
      "(train @ 21): L: 0.671545; A: 0.646930; R: 0.646930; P: 0.646903; F1: 0.642389\n",
      "(valid @ 21): L: 0.758675; A: 0.640351; R: 0.640351; P: 0.682670; F1: 0.622283\n",
      "(train @ 22): L: 0.642769; A: 0.682018; R: 0.682018; P: 0.697595; F1: 0.676452\n",
      "(valid @ 22): L: 0.644503; A: 0.688596; R: 0.688596; P: 0.750125; F1: 0.675203\n",
      "(train @ 23): L: 0.625654; A: 0.703947; R: 0.703947; P: 0.719326; F1: 0.698001\n",
      "(valid @ 23): L: 0.785175; A: 0.539474; R: 0.539474; P: 0.773534; F1: 0.441149\n",
      "(train @ 24): L: 0.613429; A: 0.688596; R: 0.688596; P: 0.688127; F1: 0.688298\n",
      "(valid @ 24): L: 0.612959; A: 0.714912; R: 0.714912; P: 0.752653; F1: 0.715356\n",
      "(train @ 25): L: 0.523557; A: 0.780702; R: 0.780702; P: 0.780022; F1: 0.780065\n",
      "(valid @ 25): L: 0.612070; A: 0.719298; R: 0.719298; P: 0.807956; F1: 0.710668\n",
      "(train @ 26): L: 0.537469; A: 0.756579; R: 0.756579; P: 0.769011; F1: 0.753955\n",
      "(valid @ 26): L: 0.579755; A: 0.723684; R: 0.723684; P: 0.804176; F1: 0.715220\n",
      "(train @ 27): L: 0.547741; A: 0.736842; R: 0.736842; P: 0.738197; F1: 0.735283\n",
      "(valid @ 27): L: 0.624619; A: 0.684211; R: 0.684211; P: 0.761462; F1: 0.673828\n",
      "(train @ 28): L: 0.545872; A: 0.750000; R: 0.750000; P: 0.751806; F1: 0.750139\n",
      "(valid @ 28): L: 0.517989; A: 0.785088; R: 0.785088; P: 0.786546; F1: 0.785522\n",
      "(train @ 29): L: 0.467592; A: 0.798246; R: 0.798246; P: 0.799435; F1: 0.797491\n",
      "(valid @ 29): L: 0.618154; A: 0.706140; R: 0.706140; P: 0.752468; F1: 0.703926\n",
      "(train @ 30): L: 0.453236; A: 0.807018; R: 0.807018; P: 0.807060; F1: 0.806801\n",
      "(valid @ 30): L: 0.578387; A: 0.745614; R: 0.745614; P: 0.773270; F1: 0.743515\n",
      "Best val Metric 0.785522 @ 28\n",
      "\n",
      "models are saved @ ./predictions/211209032735/flvl_1_3_4_6_7_9_pretrained_model_c1.pt\n",
      "Saving predictions @ ./predictions/211209032735/flvl_train_1_3_4_6_7_9_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209032735/flvl_valid_2_5_8_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209032735/flvl_public_test_trained_on_1_3_4_6_7_9_r21d_rgb.csv\n",
      "{'train': [2, 3, 5, 6, 8, 9], 'valid': [1, 4, 7], 'public_test': [10, 11, 12], 'private_test': [13, 14, 15]}\n",
      "(train @ 1): L: 1.441198; A: 0.379386; R: 0.379386; P: 0.368530; F1: 0.366380\n",
      "(valid @ 1): L: 1.013864; A: 0.539474; R: 0.539474; P: 0.482547; F1: 0.489964\n",
      "(train @ 2): L: 0.997656; A: 0.502193; R: 0.502193; P: 0.421086; F1: 0.455836\n",
      "(valid @ 2): L: 1.023182; A: 0.495614; R: 0.495614; P: 0.452560; F1: 0.436923\n",
      "(train @ 3): L: 0.979707; A: 0.471491; R: 0.471491; P: 0.396536; F1: 0.430251\n",
      "(valid @ 3): L: 0.997642; A: 0.557018; R: 0.557018; P: 0.577231; F1: 0.551936\n",
      "(train @ 4): L: 0.988655; A: 0.475877; R: 0.475877; P: 0.485379; F1: 0.450026\n",
      "(valid @ 4): L: 1.110107; A: 0.421053; R: 0.421053; P: 0.177285; F1: 0.249513\n",
      "(train @ 5): L: 0.940696; A: 0.449561; R: 0.449561; P: 0.449060; F1: 0.438680\n",
      "(valid @ 5): L: 0.966353; A: 0.592105; R: 0.592105; P: 0.646459; F1: 0.584671\n",
      "(train @ 6): L: 0.854966; A: 0.539474; R: 0.539474; P: 0.642889; F1: 0.491790\n",
      "(valid @ 6): L: 0.990335; A: 0.535088; R: 0.535088; P: 0.677699; F1: 0.467924\n",
      "(train @ 7): L: 0.810198; A: 0.570175; R: 0.570175; P: 0.576742; F1: 0.568456\n",
      "(valid @ 7): L: 0.906453; A: 0.565789; R: 0.565789; P: 0.635194; F1: 0.552939\n",
      "(train @ 8): L: 0.816340; A: 0.548246; R: 0.548246; P: 0.550808; F1: 0.549256\n",
      "(valid @ 8): L: 0.930032; A: 0.500000; R: 0.500000; P: 0.586145; F1: 0.450691\n",
      "(train @ 9): L: 0.840147; A: 0.539474; R: 0.539474; P: 0.552861; F1: 0.537471\n",
      "(valid @ 9): L: 0.813246; A: 0.662281; R: 0.662281; P: 0.664020; F1: 0.662286\n",
      "(train @ 10): L: 0.866028; A: 0.539474; R: 0.539474; P: 0.550248; F1: 0.536579\n",
      "(valid @ 10): L: 0.864499; A: 0.596491; R: 0.596491; P: 0.697294; F1: 0.555084\n",
      "(train @ 11): L: 0.777325; A: 0.633772; R: 0.633772; P: 0.656741; F1: 0.625957\n",
      "(valid @ 11): L: 0.897266; A: 0.587719; R: 0.587719; P: 0.653078; F1: 0.574423\n",
      "(train @ 12): L: 0.768792; A: 0.598684; R: 0.598684; P: 0.670747; F1: 0.570245\n",
      "(valid @ 12): L: 0.852533; A: 0.486842; R: 0.486842; P: 0.346426; F1: 0.371527\n",
      "(train @ 13): L: 0.769692; A: 0.559211; R: 0.559211; P: 0.577174; F1: 0.560538\n",
      "(valid @ 13): L: 0.827769; A: 0.521930; R: 0.521930; P: 0.355071; F1: 0.391683\n",
      "(train @ 14): L: 0.745029; A: 0.570175; R: 0.570175; P: 0.566092; F1: 0.552933\n",
      "(valid @ 14): L: 0.840537; A: 0.583333; R: 0.583333; P: 0.656094; F1: 0.568556\n",
      "(train @ 15): L: 0.705549; A: 0.609649; R: 0.609649; P: 0.607805; F1: 0.608436\n",
      "(valid @ 15): L: 0.748030; A: 0.649123; R: 0.649123; P: 0.688618; F1: 0.648400\n",
      "(train @ 16): L: 0.718469; A: 0.594298; R: 0.594298; P: 0.624906; F1: 0.588935\n",
      "(valid @ 16): L: 0.819608; A: 0.539474; R: 0.539474; P: 0.354361; F1: 0.406339\n",
      "(train @ 17): L: 0.746804; A: 0.581140; R: 0.581140; P: 0.581377; F1: 0.577044\n",
      "(valid @ 17): L: 0.970113; A: 0.627193; R: 0.627193; P: 0.678812; F1: 0.607078\n",
      "(train @ 18): L: 0.691299; A: 0.690789; R: 0.690789; P: 0.696354; F1: 0.691315\n",
      "(valid @ 18): L: 0.723913; A: 0.719298; R: 0.719298; P: 0.750557; F1: 0.716271\n",
      "(train @ 19): L: 0.627969; A: 0.697368; R: 0.697368; P: 0.722789; F1: 0.696419\n",
      "(valid @ 19): L: 0.702535; A: 0.719298; R: 0.719298; P: 0.740249; F1: 0.720706\n",
      "(train @ 20): L: 0.658945; A: 0.638158; R: 0.638158; P: 0.636932; F1: 0.634144\n",
      "(valid @ 20): L: 0.841337; A: 0.679825; R: 0.679825; P: 0.750323; F1: 0.662363\n",
      "(train @ 21): L: 0.705720; A: 0.625000; R: 0.625000; P: 0.639396; F1: 0.610065\n",
      "(valid @ 21): L: 1.089276; A: 0.486842; R: 0.486842; P: 0.347665; F1: 0.354504\n",
      "(train @ 22): L: 0.674845; A: 0.611842; R: 0.611842; P: 0.618738; F1: 0.612126\n",
      "(valid @ 22): L: 0.695731; A: 0.745614; R: 0.745614; P: 0.774342; F1: 0.744446\n",
      "(train @ 23): L: 0.583513; A: 0.750000; R: 0.750000; P: 0.752443; F1: 0.748630\n",
      "(valid @ 23): L: 0.755668; A: 0.697368; R: 0.697368; P: 0.723089; F1: 0.695227\n",
      "(train @ 24): L: 0.548154; A: 0.756579; R: 0.756579; P: 0.759034; F1: 0.756264\n",
      "(valid @ 24): L: 0.694848; A: 0.688596; R: 0.688596; P: 0.713740; F1: 0.688431\n",
      "(train @ 25): L: 0.576025; A: 0.699561; R: 0.699561; P: 0.719122; F1: 0.697167\n",
      "(valid @ 25): L: 0.643778; A: 0.692982; R: 0.692982; P: 0.727767; F1: 0.688514\n",
      "(train @ 26): L: 0.534345; A: 0.745614; R: 0.745614; P: 0.763732; F1: 0.742425\n",
      "(valid @ 26): L: 0.680693; A: 0.732456; R: 0.732456; P: 0.754599; F1: 0.730471\n",
      "(train @ 27): L: 0.495633; A: 0.785088; R: 0.785088; P: 0.789580; F1: 0.784340\n",
      "(valid @ 27): L: 0.664646; A: 0.714912; R: 0.714912; P: 0.746289; F1: 0.712934\n",
      "(train @ 28): L: 0.510112; A: 0.756579; R: 0.756579; P: 0.770522; F1: 0.756959\n",
      "(valid @ 28): L: 0.645616; A: 0.732456; R: 0.732456; P: 0.797283; F1: 0.728019\n",
      "(train @ 29): L: 0.468755; A: 0.782895; R: 0.782895; P: 0.782513; F1: 0.782513\n",
      "(valid @ 29): L: 0.746539; A: 0.692982; R: 0.692982; P: 0.722113; F1: 0.685978\n",
      "(train @ 30): L: 0.460516; A: 0.802632; R: 0.802632; P: 0.803213; F1: 0.802406\n",
      "(valid @ 30): L: 0.897778; A: 0.640351; R: 0.640351; P: 0.703672; F1: 0.619293\n",
      "Best val Metric 0.744446 @ 22\n",
      "\n",
      "models are saved @ ./predictions/211209032735/flvl_2_3_5_6_8_9_pretrained_model_c1.pt\n",
      "Saving predictions @ ./predictions/211209032735/flvl_train_2_3_5_6_8_9_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209032735/flvl_valid_1_4_7_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209032735/flvl_public_test_trained_on_2_3_5_6_8_9_r21d_rgb.csv\n",
      "Average of Best Metrics on Each Valid Set: 0.744567, 211209032735\n",
      "Saved test results @ ./predictions/211209032735/flvl_public_test_agg_r21d_rgb.csv\n"
     ]
    }
   ],
   "source": [
    "# View 0,2,3\n",
    "from main_3Views import Config, run_kfold\n",
    "\n",
    "# define architecture and run k-fold training\n",
    "cfg = Config()\n",
    "cfg.assign_variable('task', 'flvl')\n",
    "cfg.assign_variable('output_dim', 3)\n",
    "cfg.assign_variable('model_type', 'GRU')\n",
    "cfg.assign_variable('bi_dir', False)\n",
    "cfg.assign_variable('device', 'cuda:0')\n",
    "cfg.assign_variable('data_root', './r21d_rgb_features')\n",
    "cfg.assign_variable('drop_p', 0.0) # results will be irreproducible\n",
    "cfg.assign_variable('batch_size', 64)\n",
    "cfg.assign_variable('input_dim', 512)\n",
    "cfg.assign_variable('hidden_dim', 512)\n",
    "cfg.assign_variable('n_layers', 3)\n",
    "cfg.assign_variable('num_epochs', 30)\n",
    "cfg.assign_variable('seed', 1337)\n",
    "\n",
    "run_kfold(cfg,[0,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': [1, 2, 4, 5, 7, 8], 'valid': [3, 6, 9], 'public_test': [10, 11, 12], 'private_test': [13, 14, 15]}\n",
      "(train @ 1): L: 1.404959; A: 0.394737; R: 0.394737; P: 0.395633; F1: 0.387736\n",
      "(valid @ 1): L: 1.174589; A: 0.500000; R: 0.500000; P: 0.591887; F1: 0.462254\n",
      "(train @ 2): L: 1.008855; A: 0.467105; R: 0.467105; P: 0.551611; F1: 0.430464\n",
      "(valid @ 2): L: 1.081001; A: 0.469298; R: 0.469298; P: 0.426429; F1: 0.404527\n",
      "(train @ 3): L: 0.945270; A: 0.521930; R: 0.521930; P: 0.566411; F1: 0.484444\n",
      "(valid @ 3): L: 1.083692; A: 0.456140; R: 0.456140; P: 0.549653; F1: 0.452221\n",
      "(train @ 4): L: 0.935398; A: 0.508772; R: 0.508772; P: 0.531676; F1: 0.503588\n",
      "(valid @ 4): L: 1.068087; A: 0.478070; R: 0.478070; P: 0.435605; F1: 0.415544\n",
      "(train @ 5): L: 0.903052; A: 0.513158; R: 0.513158; P: 0.564859; F1: 0.501585\n",
      "(valid @ 5): L: 1.001945; A: 0.513158; R: 0.513158; P: 0.576231; F1: 0.492731\n",
      "(train @ 6): L: 0.864593; A: 0.537281; R: 0.537281; P: 0.649008; F1: 0.492585\n",
      "(valid @ 6): L: 0.973432; A: 0.561404; R: 0.561404; P: 0.618576; F1: 0.524026\n",
      "(train @ 7): L: 0.796586; A: 0.581140; R: 0.581140; P: 0.602841; F1: 0.579319\n",
      "(valid @ 7): L: 0.954830; A: 0.583333; R: 0.583333; P: 0.602168; F1: 0.562346\n",
      "(train @ 8): L: 0.772144; A: 0.594298; R: 0.594298; P: 0.594260; F1: 0.594233\n",
      "(valid @ 8): L: 1.012496; A: 0.539474; R: 0.539474; P: 0.606174; F1: 0.517005\n",
      "(train @ 9): L: 0.780017; A: 0.576754; R: 0.576754; P: 0.591014; F1: 0.575070\n",
      "(valid @ 9): L: 0.914763; A: 0.609649; R: 0.609649; P: 0.612236; F1: 0.603591\n",
      "(train @ 10): L: 0.762776; A: 0.622807; R: 0.622807; P: 0.653347; F1: 0.618048\n",
      "(valid @ 10): L: 0.891082; A: 0.649123; R: 0.649123; P: 0.647993; F1: 0.644131\n",
      "(train @ 11): L: 0.730150; A: 0.660088; R: 0.660088; P: 0.676027; F1: 0.657721\n",
      "(valid @ 11): L: 0.881721; A: 0.583333; R: 0.583333; P: 0.593990; F1: 0.561359\n",
      "(train @ 12): L: 0.706610; A: 0.620614; R: 0.620614; P: 0.629897; F1: 0.612264\n",
      "(valid @ 12): L: 0.821569; A: 0.548246; R: 0.548246; P: 0.631036; F1: 0.451891\n",
      "(train @ 13): L: 0.706906; A: 0.587719; R: 0.587719; P: 0.592666; F1: 0.580625\n",
      "(valid @ 13): L: 0.806858; A: 0.552632; R: 0.552632; P: 0.688922; F1: 0.450989\n",
      "(train @ 14): L: 0.643398; A: 0.679825; R: 0.679825; P: 0.680664; F1: 0.677401\n",
      "(valid @ 14): L: 0.793241; A: 0.609649; R: 0.609649; P: 0.622887; F1: 0.596494\n",
      "(train @ 15): L: 0.636514; A: 0.664474; R: 0.664474; P: 0.666183; F1: 0.665009\n",
      "(valid @ 15): L: 0.851543; A: 0.605263; R: 0.605263; P: 0.614657; F1: 0.591405\n",
      "(train @ 16): L: 0.646322; A: 0.662281; R: 0.662281; P: 0.677165; F1: 0.659219\n",
      "(valid @ 16): L: 0.932985; A: 0.552632; R: 0.552632; P: 0.687667; F1: 0.450784\n",
      "(train @ 17): L: 0.644295; A: 0.692982; R: 0.692982; P: 0.696482; F1: 0.691905\n",
      "(valid @ 17): L: 0.747473; A: 0.675439; R: 0.675439; P: 0.680674; F1: 0.673287\n",
      "(train @ 18): L: 0.598732; A: 0.747807; R: 0.747807; P: 0.753771; F1: 0.748043\n",
      "(valid @ 18): L: 0.788217; A: 0.671053; R: 0.671053; P: 0.695952; F1: 0.670997\n",
      "(train @ 19): L: 0.639765; A: 0.703947; R: 0.703947; P: 0.721189; F1: 0.702778\n",
      "(valid @ 19): L: 0.780092; A: 0.631579; R: 0.631579; P: 0.696493; F1: 0.599663\n",
      "(train @ 20): L: 0.529115; A: 0.741228; R: 0.741228; P: 0.758825; F1: 0.740276\n",
      "(valid @ 20): L: 0.754942; A: 0.679825; R: 0.679825; P: 0.717795; F1: 0.666203\n",
      "(train @ 21): L: 0.525336; A: 0.736842; R: 0.736842; P: 0.743422; F1: 0.735660\n",
      "(valid @ 21): L: 0.741790; A: 0.671053; R: 0.671053; P: 0.701501; F1: 0.655139\n",
      "(train @ 22): L: 0.492018; A: 0.767544; R: 0.767544; P: 0.776920; F1: 0.766507\n",
      "(valid @ 22): L: 0.741725; A: 0.684211; R: 0.684211; P: 0.684265; F1: 0.681540\n",
      "(train @ 23): L: 0.582166; A: 0.697368; R: 0.697368; P: 0.703790; F1: 0.696385\n",
      "(valid @ 23): L: 0.744130; A: 0.684211; R: 0.684211; P: 0.748442; F1: 0.665943\n",
      "(train @ 24): L: 0.501781; A: 0.769737; R: 0.769737; P: 0.771703; F1: 0.768178\n",
      "(valid @ 24): L: 0.677917; A: 0.675439; R: 0.675439; P: 0.677092; F1: 0.673160\n",
      "(train @ 25): L: 0.485342; A: 0.767544; R: 0.767544; P: 0.775589; F1: 0.765043\n",
      "(valid @ 25): L: 0.686842; A: 0.688596; R: 0.688596; P: 0.716215; F1: 0.678976\n",
      "(train @ 26): L: 0.451387; A: 0.791667; R: 0.791667; P: 0.805224; F1: 0.790062\n",
      "(valid @ 26): L: 0.666946; A: 0.697368; R: 0.697368; P: 0.700784; F1: 0.694808\n",
      "(train @ 27): L: 0.417591; A: 0.804825; R: 0.804825; P: 0.811899; F1: 0.802936\n",
      "(valid @ 27): L: 0.678198; A: 0.688596; R: 0.688596; P: 0.689682; F1: 0.686628\n",
      "(train @ 28): L: 0.431221; A: 0.811404; R: 0.811404; P: 0.817543; F1: 0.811804\n",
      "(valid @ 28): L: 0.839265; A: 0.684211; R: 0.684211; P: 0.705249; F1: 0.684118\n",
      "(train @ 29): L: 0.381905; A: 0.844298; R: 0.844298; P: 0.848009; F1: 0.844188\n",
      "(valid @ 29): L: 0.712516; A: 0.719298; R: 0.719298; P: 0.725882; F1: 0.718377\n",
      "(train @ 30): L: 0.333570; A: 0.861842; R: 0.861842; P: 0.861907; F1: 0.861269\n",
      "(valid @ 30): L: 0.754360; A: 0.692982; R: 0.692982; P: 0.723031; F1: 0.686730\n",
      "Best val Metric 0.718377 @ 29\n",
      "\n",
      "models are saved @ ./predictions/211209035906/flvl_1_2_4_5_7_8_pretrained_model_c1.pt\n",
      "Saving predictions @ ./predictions/211209035906/flvl_train_1_2_4_5_7_8_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209035906/flvl_valid_3_6_9_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209035906/flvl_public_test_trained_on_1_2_4_5_7_8_r21d_rgb.csv\n",
      "{'train': [1, 3, 4, 6, 7, 9], 'valid': [2, 5, 8], 'public_test': [10, 11, 12], 'private_test': [13, 14, 15]}\n",
      "(train @ 1): L: 1.446853; A: 0.385965; R: 0.385965; P: 0.373589; F1: 0.373245\n",
      "(valid @ 1): L: 0.995837; A: 0.521930; R: 0.521930; P: 0.473061; F1: 0.468810\n",
      "(train @ 2): L: 0.982841; A: 0.495614; R: 0.495614; P: 0.415097; F1: 0.450851\n",
      "(valid @ 2): L: 1.012016; A: 0.508772; R: 0.508772; P: 0.464268; F1: 0.452423\n",
      "(train @ 3): L: 0.947000; A: 0.475877; R: 0.475877; P: 0.517299; F1: 0.443622\n",
      "(valid @ 3): L: 0.983779; A: 0.609649; R: 0.609649; P: 0.631522; F1: 0.599422\n",
      "(train @ 4): L: 1.019037; A: 0.486842; R: 0.486842; P: 0.487740; F1: 0.469129\n",
      "(valid @ 4): L: 1.100303; A: 0.561404; R: 0.561404; P: 0.475726; F1: 0.514228\n",
      "(train @ 5): L: 0.954497; A: 0.491228; R: 0.491228; P: 0.524464; F1: 0.470246\n",
      "(valid @ 5): L: 0.952751; A: 0.600877; R: 0.600877; P: 0.679457; F1: 0.583529\n",
      "(train @ 6): L: 0.879977; A: 0.510965; R: 0.510965; P: 0.595613; F1: 0.461771\n",
      "(valid @ 6): L: 0.949011; A: 0.543860; R: 0.543860; P: 0.712854; F1: 0.473197\n",
      "(train @ 7): L: 0.821509; A: 0.578947; R: 0.578947; P: 0.589116; F1: 0.573523\n",
      "(valid @ 7): L: 0.912952; A: 0.561404; R: 0.561404; P: 0.634675; F1: 0.546185\n",
      "(train @ 8): L: 0.820668; A: 0.537281; R: 0.537281; P: 0.536678; F1: 0.536833\n",
      "(valid @ 8): L: 0.932662; A: 0.500000; R: 0.500000; P: 0.605540; F1: 0.436826\n",
      "(train @ 9): L: 0.839277; A: 0.532895; R: 0.532895; P: 0.542164; F1: 0.531646\n",
      "(valid @ 9): L: 0.785660; A: 0.714912; R: 0.714912; P: 0.728356; F1: 0.709763\n",
      "(train @ 10): L: 0.844043; A: 0.543860; R: 0.543860; P: 0.552297; F1: 0.543602\n",
      "(valid @ 10): L: 0.768199; A: 0.679825; R: 0.679825; P: 0.705328; F1: 0.676885\n",
      "(train @ 11): L: 0.792757; A: 0.635965; R: 0.635965; P: 0.662911; F1: 0.630975\n",
      "(valid @ 11): L: 0.838488; A: 0.614035; R: 0.614035; P: 0.665670; F1: 0.607551\n",
      "(train @ 12): L: 0.764220; A: 0.603070; R: 0.603070; P: 0.653505; F1: 0.579731\n",
      "(valid @ 12): L: 0.832274; A: 0.504386; R: 0.504386; P: 0.350791; F1: 0.381708\n",
      "(train @ 13): L: 0.781260; A: 0.546053; R: 0.546053; P: 0.558924; F1: 0.543089\n",
      "(valid @ 13): L: 0.785017; A: 0.535088; R: 0.535088; P: 0.330833; F1: 0.399720\n",
      "(train @ 14): L: 0.755009; A: 0.557018; R: 0.557018; P: 0.556144; F1: 0.547623\n",
      "(valid @ 14): L: 0.736715; A: 0.649123; R: 0.649123; P: 0.688103; F1: 0.646404\n",
      "(train @ 15): L: 0.717651; A: 0.638158; R: 0.638158; P: 0.639613; F1: 0.634323\n",
      "(valid @ 15): L: 0.729881; A: 0.662281; R: 0.662281; P: 0.700703; F1: 0.660759\n",
      "(train @ 16): L: 0.707607; A: 0.625000; R: 0.625000; P: 0.641198; F1: 0.619981\n",
      "(valid @ 16): L: 0.822487; A: 0.530702; R: 0.530702; P: 0.317055; F1: 0.393385\n",
      "(train @ 17): L: 0.765277; A: 0.581140; R: 0.581140; P: 0.579626; F1: 0.578419\n",
      "(valid @ 17): L: 0.775455; A: 0.671053; R: 0.671053; P: 0.704451; F1: 0.661591\n",
      "(train @ 18): L: 0.698531; A: 0.695175; R: 0.695175; P: 0.700814; F1: 0.694683\n",
      "(valid @ 18): L: 0.682541; A: 0.736842; R: 0.736842; P: 0.757221; F1: 0.737253\n",
      "(train @ 19): L: 0.648480; A: 0.695175; R: 0.695175; P: 0.713629; F1: 0.694549\n",
      "(valid @ 19): L: 0.623484; A: 0.706140; R: 0.706140; P: 0.734759; F1: 0.695852\n",
      "(train @ 20): L: 0.639311; A: 0.688596; R: 0.688596; P: 0.700790; F1: 0.685666\n",
      "(valid @ 20): L: 0.640241; A: 0.719298; R: 0.719298; P: 0.742947; F1: 0.717019\n",
      "(train @ 21): L: 0.648637; A: 0.655702; R: 0.655702; P: 0.660879; F1: 0.651204\n",
      "(valid @ 21): L: 0.729104; A: 0.631579; R: 0.631579; P: 0.680345; F1: 0.610730\n",
      "(train @ 22): L: 0.640740; A: 0.679825; R: 0.679825; P: 0.697047; F1: 0.675094\n",
      "(valid @ 22): L: 0.631940; A: 0.688596; R: 0.688596; P: 0.775181; F1: 0.665205\n",
      "(train @ 23): L: 0.626535; A: 0.695175; R: 0.695175; P: 0.708714; F1: 0.688492\n",
      "(valid @ 23): L: 0.740700; A: 0.596491; R: 0.596491; P: 0.731637; F1: 0.549378\n",
      "(train @ 24): L: 0.600986; A: 0.708333; R: 0.708333; P: 0.709558; F1: 0.707502\n",
      "(valid @ 24): L: 0.618553; A: 0.723684; R: 0.723684; P: 0.765104; F1: 0.723299\n",
      "(train @ 25): L: 0.543784; A: 0.771930; R: 0.771930; P: 0.775182; F1: 0.771082\n",
      "(valid @ 25): L: 0.586169; A: 0.728070; R: 0.728070; P: 0.803627; F1: 0.715236\n",
      "(train @ 26): L: 0.544423; A: 0.747807; R: 0.747807; P: 0.756851; F1: 0.743794\n",
      "(valid @ 26): L: 0.577144; A: 0.706140; R: 0.706140; P: 0.780702; F1: 0.694584\n",
      "(train @ 27): L: 0.557959; A: 0.739035; R: 0.739035; P: 0.738735; F1: 0.737377\n",
      "(valid @ 27): L: 0.656342; A: 0.675439; R: 0.675439; P: 0.750946; F1: 0.666587\n",
      "(train @ 28): L: 0.571685; A: 0.736842; R: 0.736842; P: 0.738092; F1: 0.736265\n",
      "(valid @ 28): L: 0.506219; A: 0.793860; R: 0.793860; P: 0.794517; F1: 0.794047\n",
      "(train @ 29): L: 0.470453; A: 0.804825; R: 0.804825; P: 0.805921; F1: 0.804002\n",
      "(valid @ 29): L: 0.613479; A: 0.723684; R: 0.723684; P: 0.769073; F1: 0.721620\n",
      "(train @ 30): L: 0.468990; A: 0.796053; R: 0.796053; P: 0.796099; F1: 0.796002\n",
      "(valid @ 30): L: 0.576119; A: 0.741228; R: 0.741228; P: 0.771384; F1: 0.739498\n",
      "Best val Metric 0.794047 @ 28\n",
      "\n",
      "models are saved @ ./predictions/211209035906/flvl_1_3_4_6_7_9_pretrained_model_c1.pt\n",
      "Saving predictions @ ./predictions/211209035906/flvl_train_1_3_4_6_7_9_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209035906/flvl_valid_2_5_8_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209035906/flvl_public_test_trained_on_1_3_4_6_7_9_r21d_rgb.csv\n",
      "{'train': [2, 3, 5, 6, 8, 9], 'valid': [1, 4, 7], 'public_test': [10, 11, 12], 'private_test': [13, 14, 15]}\n",
      "(train @ 1): L: 1.443947; A: 0.390351; R: 0.390351; P: 0.380804; F1: 0.381106\n",
      "(valid @ 1): L: 1.028246; A: 0.539474; R: 0.539474; P: 0.482547; F1: 0.489964\n",
      "(train @ 2): L: 1.003282; A: 0.489035; R: 0.489035; P: 0.408543; F1: 0.440265\n",
      "(valid @ 2): L: 1.038035; A: 0.495614; R: 0.495614; P: 0.452560; F1: 0.436923\n",
      "(train @ 3): L: 0.968741; A: 0.482456; R: 0.482456; P: 0.559867; F1: 0.451208\n",
      "(valid @ 3): L: 0.993464; A: 0.543860; R: 0.543860; P: 0.554448; F1: 0.540420\n",
      "(train @ 4): L: 0.983742; A: 0.453947; R: 0.453947; P: 0.453268; F1: 0.429325\n",
      "(valid @ 4): L: 1.114599; A: 0.421053; R: 0.421053; P: 0.177285; F1: 0.249513\n",
      "(train @ 5): L: 0.942563; A: 0.486842; R: 0.486842; P: 0.490271; F1: 0.471621\n",
      "(valid @ 5): L: 0.971139; A: 0.657895; R: 0.657895; P: 0.691741; F1: 0.654098\n",
      "(train @ 6): L: 0.856811; A: 0.537281; R: 0.537281; P: 0.640967; F1: 0.489043\n",
      "(valid @ 6): L: 1.002264; A: 0.500000; R: 0.500000; P: 0.725566; F1: 0.397866\n",
      "(train @ 7): L: 0.813802; A: 0.567982; R: 0.567982; P: 0.572393; F1: 0.567961\n",
      "(valid @ 7): L: 0.911562; A: 0.565789; R: 0.565789; P: 0.638331; F1: 0.550999\n",
      "(train @ 8): L: 0.818747; A: 0.541667; R: 0.541667; P: 0.542353; F1: 0.541952\n",
      "(valid @ 8): L: 0.946073; A: 0.508772; R: 0.508772; P: 0.600154; F1: 0.460099\n",
      "(train @ 9): L: 0.851571; A: 0.524123; R: 0.524123; P: 0.535341; F1: 0.523140\n",
      "(valid @ 9): L: 0.813162; A: 0.640351; R: 0.640351; P: 0.643289; F1: 0.641400\n",
      "(train @ 10): L: 0.867289; A: 0.519737; R: 0.519737; P: 0.528210; F1: 0.514939\n",
      "(valid @ 10): L: 0.867405; A: 0.552632; R: 0.552632; P: 0.714957; F1: 0.483831\n",
      "(train @ 11): L: 0.783265; A: 0.603070; R: 0.603070; P: 0.618358; F1: 0.595165\n",
      "(valid @ 11): L: 0.895256; A: 0.605263; R: 0.605263; P: 0.662127; F1: 0.594925\n",
      "(train @ 12): L: 0.767862; A: 0.594298; R: 0.594298; P: 0.658320; F1: 0.565673\n",
      "(valid @ 12): L: 0.842578; A: 0.478070; R: 0.478070; P: 0.344568; F1: 0.362567\n",
      "(train @ 13): L: 0.764134; A: 0.559211; R: 0.559211; P: 0.576368; F1: 0.559681\n",
      "(valid @ 13): L: 0.825026; A: 0.535088; R: 0.535088; P: 0.357999; F1: 0.403710\n",
      "(train @ 14): L: 0.746672; A: 0.567982; R: 0.567982; P: 0.562487; F1: 0.554262\n",
      "(valid @ 14): L: 0.816819; A: 0.614035; R: 0.614035; P: 0.678142; F1: 0.602433\n",
      "(train @ 15): L: 0.702734; A: 0.638158; R: 0.638158; P: 0.638027; F1: 0.637534\n",
      "(valid @ 15): L: 0.769953; A: 0.649123; R: 0.649123; P: 0.690283; F1: 0.647881\n",
      "(train @ 16): L: 0.709469; A: 0.614035; R: 0.614035; P: 0.638571; F1: 0.607655\n",
      "(valid @ 16): L: 0.851835; A: 0.539474; R: 0.539474; P: 0.354361; F1: 0.406339\n",
      "(train @ 17): L: 0.749273; A: 0.587719; R: 0.587719; P: 0.585863; F1: 0.583067\n",
      "(valid @ 17): L: 0.941795; A: 0.649123; R: 0.649123; P: 0.692028; F1: 0.628300\n",
      "(train @ 18): L: 0.685207; A: 0.695175; R: 0.695175; P: 0.697321; F1: 0.695322\n",
      "(valid @ 18): L: 0.766251; A: 0.701754; R: 0.701754; P: 0.725948; F1: 0.699137\n",
      "(train @ 19): L: 0.634541; A: 0.690789; R: 0.690789; P: 0.714049; F1: 0.690572\n",
      "(valid @ 19): L: 0.703487; A: 0.710526; R: 0.710526; P: 0.751175; F1: 0.710353\n",
      "(train @ 20): L: 0.641214; A: 0.664474; R: 0.664474; P: 0.669929; F1: 0.658652\n",
      "(valid @ 20): L: 0.779665; A: 0.684211; R: 0.684211; P: 0.746238; F1: 0.674108\n",
      "(train @ 21): L: 0.692316; A: 0.616228; R: 0.616228; P: 0.627005; F1: 0.607666\n",
      "(valid @ 21): L: 1.073847; A: 0.491228; R: 0.491228; P: 0.769613; F1: 0.375106\n",
      "(train @ 22): L: 0.660030; A: 0.625000; R: 0.625000; P: 0.635102; F1: 0.617350\n",
      "(valid @ 22): L: 0.737002; A: 0.701754; R: 0.701754; P: 0.807177; F1: 0.688158\n",
      "(train @ 23): L: 0.583198; A: 0.728070; R: 0.728070; P: 0.737432; F1: 0.725533\n",
      "(valid @ 23): L: 0.771783; A: 0.697368; R: 0.697368; P: 0.752524; F1: 0.687344\n",
      "(train @ 24): L: 0.537269; A: 0.791667; R: 0.791667; P: 0.793053; F1: 0.791060\n",
      "(valid @ 24): L: 0.715160; A: 0.671053; R: 0.671053; P: 0.701679; F1: 0.671617\n",
      "(train @ 25): L: 0.565460; A: 0.692982; R: 0.692982; P: 0.696518; F1: 0.692775\n",
      "(valid @ 25): L: 0.613675; A: 0.750000; R: 0.750000; P: 0.765721; F1: 0.750718\n",
      "(train @ 26): L: 0.547594; A: 0.725877; R: 0.725877; P: 0.738742; F1: 0.723770\n",
      "(valid @ 26): L: 0.637364; A: 0.763158; R: 0.763158; P: 0.779904; F1: 0.762603\n",
      "(train @ 27): L: 0.508564; A: 0.774123; R: 0.774123; P: 0.775884; F1: 0.772326\n",
      "(valid @ 27): L: 0.642805; A: 0.706140; R: 0.706140; P: 0.733007; F1: 0.707576\n",
      "(train @ 28): L: 0.502512; A: 0.769737; R: 0.769737; P: 0.775721; F1: 0.769890\n",
      "(valid @ 28): L: 0.640116; A: 0.723684; R: 0.723684; P: 0.786792; F1: 0.717799\n",
      "(train @ 29): L: 0.464853; A: 0.793860; R: 0.793860; P: 0.794263; F1: 0.793333\n",
      "(valid @ 29): L: 0.731144; A: 0.688596; R: 0.688596; P: 0.719223; F1: 0.685450\n",
      "(train @ 30): L: 0.452527; A: 0.804825; R: 0.804825; P: 0.804830; F1: 0.804133\n",
      "(valid @ 30): L: 0.826898; A: 0.684211; R: 0.684211; P: 0.722822; F1: 0.672604\n",
      "Best val Metric 0.762603 @ 26\n",
      "\n",
      "models are saved @ ./predictions/211209035906/flvl_2_3_5_6_8_9_pretrained_model_c1.pt\n",
      "Saving predictions @ ./predictions/211209035906/flvl_train_2_3_5_6_8_9_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209035906/flvl_valid_1_4_7_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209035906/flvl_public_test_trained_on_2_3_5_6_8_9_r21d_rgb.csv\n",
      "Average of Best Metrics on Each Valid Set: 0.758342, 211209035906\n",
      "Saved test results @ ./predictions/211209035906/flvl_public_test_agg_r21d_rgb.csv\n"
     ]
    }
   ],
   "source": [
    "# View 1,2,3\n",
    "from main_3Views import Config, run_kfold\n",
    "\n",
    "# define architecture and run k-fold training\n",
    "cfg = Config()\n",
    "cfg.assign_variable('task', 'flvl')\n",
    "cfg.assign_variable('output_dim', 3)\n",
    "cfg.assign_variable('model_type', 'GRU')\n",
    "cfg.assign_variable('bi_dir', False)\n",
    "cfg.assign_variable('device', 'cuda:0')\n",
    "cfg.assign_variable('data_root', './r21d_rgb_features')\n",
    "cfg.assign_variable('drop_p', 0.0) # results will be irreproducible\n",
    "cfg.assign_variable('batch_size', 64)\n",
    "cfg.assign_variable('input_dim', 512)\n",
    "cfg.assign_variable('hidden_dim', 512)\n",
    "cfg.assign_variable('n_layers', 3)\n",
    "cfg.assign_variable('num_epochs', 30)\n",
    "cfg.assign_variable('seed', 1337)\n",
    "\n",
    "run_kfold(cfg,[1,2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### C. EXPERIMENT WITH 2 VIEWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': [1, 2, 4, 5, 7, 8], 'valid': [3, 6, 9], 'public_test': [10, 11, 12], 'private_test': [13, 14, 15]}\n",
      "(train @ 1): L: 1.103147; A: 0.486842; R: 0.486842; P: 0.460977; F1: 0.455308\n",
      "(valid @ 1): L: 1.046896; A: 0.473684; R: 0.473684; P: 0.429518; F1: 0.412183\n",
      "(train @ 2): L: 0.983250; A: 0.491228; R: 0.491228; P: 0.413527; F1: 0.448904\n",
      "(valid @ 2): L: 1.027663; A: 0.508772; R: 0.508772; P: 0.536572; F1: 0.491815\n",
      "(train @ 3): L: 0.961489; A: 0.548246; R: 0.548246; P: 0.677529; F1: 0.497873\n",
      "(valid @ 3): L: 0.997512; A: 0.464912; R: 0.464912; P: 0.388528; F1: 0.392992\n",
      "(train @ 4): L: 0.926506; A: 0.515351; R: 0.515351; P: 0.524562; F1: 0.508198\n",
      "(valid @ 4): L: 1.003493; A: 0.491228; R: 0.491228; P: 0.591752; F1: 0.447366\n",
      "(train @ 5): L: 0.854107; A: 0.550439; R: 0.550439; P: 0.589519; F1: 0.540955\n",
      "(valid @ 5): L: 0.902974; A: 0.600877; R: 0.600877; P: 0.633907; F1: 0.605282\n",
      "(train @ 6): L: 0.807435; A: 0.625000; R: 0.625000; P: 0.684189; F1: 0.613851\n",
      "(valid @ 6): L: 1.001981; A: 0.491228; R: 0.491228; P: 0.699055; F1: 0.395777\n",
      "(train @ 7): L: 0.855797; A: 0.530702; R: 0.530702; P: 0.536545; F1: 0.530320\n",
      "(valid @ 7): L: 0.888251; A: 0.609649; R: 0.609649; P: 0.628293; F1: 0.600721\n",
      "(train @ 8): L: 0.737219; A: 0.633772; R: 0.633772; P: 0.634314; F1: 0.633992\n",
      "(valid @ 8): L: 0.896053; A: 0.605263; R: 0.605263; P: 0.630638; F1: 0.592657\n",
      "(train @ 9): L: 0.743034; A: 0.629386; R: 0.629386; P: 0.678537; F1: 0.607621\n",
      "(valid @ 9): L: 0.955100; A: 0.500000; R: 0.500000; P: 0.350376; F1: 0.369453\n",
      "(train @ 10): L: 0.830525; A: 0.550439; R: 0.550439; P: 0.555960; F1: 0.549448\n",
      "(valid @ 10): L: 0.919962; A: 0.561404; R: 0.561404; P: 0.597254; F1: 0.536488\n",
      "(train @ 11): L: 0.753179; A: 0.607456; R: 0.607456; P: 0.636314; F1: 0.597641\n",
      "(valid @ 11): L: 0.888987; A: 0.530702; R: 0.530702; P: 0.322228; F1: 0.394483\n",
      "(train @ 12): L: 0.681344; A: 0.622807; R: 0.622807; P: 0.637902; F1: 0.616250\n",
      "(valid @ 12): L: 0.932996; A: 0.578947; R: 0.578947; P: 0.581934; F1: 0.555770\n",
      "(train @ 13): L: 0.730587; A: 0.653509; R: 0.653509; P: 0.678647; F1: 0.643073\n",
      "(valid @ 13): L: 0.818421; A: 0.600877; R: 0.600877; P: 0.602988; F1: 0.593496\n",
      "(train @ 14): L: 0.637974; A: 0.699561; R: 0.699561; P: 0.726695; F1: 0.692794\n",
      "(valid @ 14): L: 0.894375; A: 0.609649; R: 0.609649; P: 0.616027; F1: 0.596742\n",
      "(train @ 15): L: 0.725905; A: 0.622807; R: 0.622807; P: 0.634259; F1: 0.622265\n",
      "(valid @ 15): L: 0.774170; A: 0.614035; R: 0.614035; P: 0.618485; F1: 0.598997\n",
      "(train @ 16): L: 0.639801; A: 0.675439; R: 0.675439; P: 0.711892; F1: 0.662222\n",
      "(valid @ 16): L: 0.731835; A: 0.662281; R: 0.662281; P: 0.663238; F1: 0.658382\n",
      "(train @ 17): L: 0.631418; A: 0.684211; R: 0.684211; P: 0.688113; F1: 0.682977\n",
      "(valid @ 17): L: 0.746625; A: 0.631579; R: 0.631579; P: 0.638201; F1: 0.622484\n",
      "(train @ 18): L: 0.669220; A: 0.640351; R: 0.640351; P: 0.648676; F1: 0.633017\n",
      "(valid @ 18): L: 0.773754; A: 0.644737; R: 0.644737; P: 0.651913; F1: 0.640307\n",
      "(train @ 19): L: 0.611040; A: 0.714912; R: 0.714912; P: 0.754176; F1: 0.708639\n",
      "(valid @ 19): L: 0.702929; A: 0.627193; R: 0.627193; P: 0.636442; F1: 0.619513\n",
      "(train @ 20): L: 0.596177; A: 0.708333; R: 0.708333; P: 0.708629; F1: 0.707850\n",
      "(valid @ 20): L: 0.711708; A: 0.635965; R: 0.635965; P: 0.648401; F1: 0.625805\n",
      "(train @ 21): L: 0.614633; A: 0.708333; R: 0.708333; P: 0.729898; F1: 0.705275\n",
      "(valid @ 21): L: 0.779030; A: 0.662281; R: 0.662281; P: 0.680921; F1: 0.662005\n",
      "(train @ 22): L: 0.616935; A: 0.714912; R: 0.714912; P: 0.750489; F1: 0.706993\n",
      "(valid @ 22): L: 0.731612; A: 0.662281; R: 0.662281; P: 0.666329; F1: 0.660120\n",
      "(train @ 23): L: 0.561697; A: 0.752193; R: 0.752193; P: 0.766193; F1: 0.750464\n",
      "(valid @ 23): L: 0.697438; A: 0.675439; R: 0.675439; P: 0.675068; F1: 0.674396\n",
      "(train @ 24): L: 0.518132; A: 0.765351; R: 0.765351; P: 0.765431; F1: 0.764724\n",
      "(valid @ 24): L: 0.776846; A: 0.622807; R: 0.622807; P: 0.626836; F1: 0.614304\n",
      "(train @ 25): L: 0.493767; A: 0.756579; R: 0.756579; P: 0.758063; F1: 0.755871\n",
      "(valid @ 25): L: 0.869322; A: 0.600877; R: 0.600877; P: 0.729224; F1: 0.547571\n",
      "(train @ 26): L: 0.659314; A: 0.649123; R: 0.649123; P: 0.657356; F1: 0.649010\n",
      "(valid @ 26): L: 0.937502; A: 0.570175; R: 0.570175; P: 0.587456; F1: 0.543027\n",
      "(train @ 27): L: 0.605693; A: 0.701754; R: 0.701754; P: 0.704849; F1: 0.699981\n",
      "(valid @ 27): L: 0.746353; A: 0.653509; R: 0.653509; P: 0.657378; F1: 0.649069\n",
      "(train @ 28): L: 0.497520; A: 0.765351; R: 0.765351; P: 0.765156; F1: 0.764376\n",
      "(valid @ 28): L: 0.672247; A: 0.657895; R: 0.657895; P: 0.675764; F1: 0.654817\n",
      "(train @ 29): L: 0.482974; A: 0.774123; R: 0.774123; P: 0.785550; F1: 0.772861\n",
      "(valid @ 29): L: 0.814835; A: 0.653509; R: 0.653509; P: 0.670529; F1: 0.652394\n",
      "(train @ 30): L: 0.461131; A: 0.798246; R: 0.798246; P: 0.819608; F1: 0.797061\n",
      "(valid @ 30): L: 0.948673; A: 0.635965; R: 0.635965; P: 0.705742; F1: 0.622022\n",
      "Best val Metric 0.674396 @ 23\n",
      "\n",
      "models are saved @ ./predictions/211209043050/flvl_1_2_4_5_7_8_pretrained_model_c1.pt\n",
      "Saving predictions @ ./predictions/211209043050/flvl_train_1_2_4_5_7_8_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209043050/flvl_valid_3_6_9_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209043050/flvl_public_test_trained_on_1_2_4_5_7_8_r21d_rgb.csv\n",
      "{'train': [1, 3, 4, 6, 7, 9], 'valid': [2, 5, 8], 'public_test': [10, 11, 12], 'private_test': [13, 14, 15]}\n",
      "(train @ 1): L: 1.086768; A: 0.484649; R: 0.484649; P: 0.462815; F1: 0.462918\n",
      "(valid @ 1): L: 0.943098; A: 0.526316; R: 0.526316; P: 0.470543; F1: 0.476935\n",
      "(train @ 2): L: 1.020997; A: 0.458333; R: 0.458333; P: 0.386062; F1: 0.418862\n",
      "(valid @ 2): L: 0.948073; A: 0.635965; R: 0.635965; P: 0.661981; F1: 0.635401\n",
      "(train @ 3): L: 1.004037; A: 0.495614; R: 0.495614; P: 0.580761; F1: 0.462703\n",
      "(valid @ 3): L: 1.053671; A: 0.399123; R: 0.399123; P: 0.171820; F1: 0.240224\n",
      "(train @ 4): L: 0.957736; A: 0.484649; R: 0.484649; P: 0.512357; F1: 0.465197\n",
      "(valid @ 4): L: 0.935413; A: 0.521930; R: 0.521930; P: 0.468986; F1: 0.471178\n",
      "(train @ 5): L: 0.898542; A: 0.526316; R: 0.526316; P: 0.560647; F1: 0.505964\n",
      "(valid @ 5): L: 0.921029; A: 0.478070; R: 0.478070; P: 0.590804; F1: 0.369515\n",
      "(train @ 6): L: 0.878868; A: 0.563596; R: 0.563596; P: 0.582142; F1: 0.554586\n",
      "(valid @ 6): L: 1.020772; A: 0.421053; R: 0.421053; P: 0.177285; F1: 0.249513\n",
      "(train @ 7): L: 0.854427; A: 0.524123; R: 0.524123; P: 0.576766; F1: 0.501956\n",
      "(valid @ 7): L: 0.830963; A: 0.635965; R: 0.635965; P: 0.637423; F1: 0.630256\n",
      "(train @ 8): L: 0.858288; A: 0.585526; R: 0.585526; P: 0.615303; F1: 0.586584\n",
      "(valid @ 8): L: 0.859205; A: 0.609649; R: 0.609649; P: 0.666801; F1: 0.598936\n",
      "(train @ 9): L: 0.805784; A: 0.592105; R: 0.592105; P: 0.634703; F1: 0.562245\n",
      "(valid @ 9): L: 0.844576; A: 0.513158; R: 0.513158; P: 0.774218; F1: 0.397587\n",
      "(train @ 10): L: 0.816015; A: 0.548246; R: 0.548246; P: 0.565126; F1: 0.550266\n",
      "(valid @ 10): L: 0.775796; A: 0.618421; R: 0.618421; P: 0.650314; F1: 0.607479\n",
      "(train @ 11): L: 0.793577; A: 0.587719; R: 0.587719; P: 0.624764; F1: 0.561548\n",
      "(valid @ 11): L: 0.983490; A: 0.517544; R: 0.517544; P: 0.354113; F1: 0.387471\n",
      "(train @ 12): L: 0.783285; A: 0.554825; R: 0.554825; P: 0.560201; F1: 0.550826\n",
      "(valid @ 12): L: 0.822185; A: 0.596491; R: 0.596491; P: 0.648252; F1: 0.585538\n",
      "(train @ 13): L: 0.756772; A: 0.625000; R: 0.625000; P: 0.638935; F1: 0.617690\n",
      "(valid @ 13): L: 0.924460; A: 0.513158; R: 0.513158; P: 0.355071; F1: 0.384922\n",
      "(train @ 14): L: 0.786185; A: 0.563596; R: 0.563596; P: 0.570208; F1: 0.563615\n",
      "(valid @ 14): L: 0.724438; A: 0.675439; R: 0.675439; P: 0.676266; F1: 0.672560\n",
      "(train @ 15): L: 0.788350; A: 0.616228; R: 0.616228; P: 0.616923; F1: 0.616424\n",
      "(valid @ 15): L: 0.715548; A: 0.692982; R: 0.692982; P: 0.714205; F1: 0.695116\n",
      "(train @ 16): L: 0.708597; A: 0.640351; R: 0.640351; P: 0.659756; F1: 0.633749\n",
      "(valid @ 16): L: 0.746049; A: 0.701754; R: 0.701754; P: 0.724439; F1: 0.702062\n",
      "(train @ 17): L: 0.689867; A: 0.664474; R: 0.664474; P: 0.664057; F1: 0.662904\n",
      "(valid @ 17): L: 0.680443; A: 0.728070; R: 0.728070; P: 0.738875; F1: 0.729821\n",
      "(train @ 18): L: 0.710951; A: 0.633772; R: 0.633772; P: 0.669068; F1: 0.622353\n",
      "(valid @ 18): L: 0.674015; A: 0.701754; R: 0.701754; P: 0.741854; F1: 0.689356\n",
      "(train @ 19): L: 0.703217; A: 0.664474; R: 0.664474; P: 0.715244; F1: 0.655012\n",
      "(valid @ 19): L: 0.766044; A: 0.618421; R: 0.618421; P: 0.655417; F1: 0.602386\n",
      "(train @ 20): L: 0.687989; A: 0.686404; R: 0.686404; P: 0.690452; F1: 0.686036\n",
      "(valid @ 20): L: 0.636400; A: 0.719298; R: 0.719298; P: 0.721807; F1: 0.717828\n",
      "(train @ 21): L: 0.692637; A: 0.642544; R: 0.642544; P: 0.670373; F1: 0.635828\n",
      "(valid @ 21): L: 0.646778; A: 0.679825; R: 0.679825; P: 0.684006; F1: 0.678478\n",
      "(train @ 22): L: 0.671568; A: 0.640351; R: 0.640351; P: 0.670776; F1: 0.622558\n",
      "(valid @ 22): L: 0.754164; A: 0.657895; R: 0.657895; P: 0.691866; F1: 0.660964\n",
      "(train @ 23): L: 0.661719; A: 0.695175; R: 0.695175; P: 0.718307; F1: 0.692383\n",
      "(valid @ 23): L: 0.733330; A: 0.631579; R: 0.631579; P: 0.657531; F1: 0.632397\n",
      "(train @ 24): L: 0.664209; A: 0.646930; R: 0.646930; P: 0.646145; F1: 0.645707\n",
      "(valid @ 24): L: 0.609111; A: 0.732456; R: 0.732456; P: 0.778976; F1: 0.723713\n",
      "(train @ 25): L: 0.678087; A: 0.675439; R: 0.675439; P: 0.706063; F1: 0.668638\n",
      "(valid @ 25): L: 0.769873; A: 0.618421; R: 0.618421; P: 0.771946; F1: 0.557745\n",
      "(train @ 26): L: 0.750963; A: 0.627193; R: 0.627193; P: 0.659103; F1: 0.598954\n",
      "(valid @ 26): L: 0.882049; A: 0.500000; R: 0.500000; P: 0.513145; F1: 0.424640\n",
      "(train @ 27): L: 0.745427; A: 0.585526; R: 0.585526; P: 0.588965; F1: 0.583783\n",
      "(valid @ 27): L: 0.654484; A: 0.653509; R: 0.653509; P: 0.784885; F1: 0.625154\n",
      "(train @ 28): L: 0.621799; A: 0.692982; R: 0.692982; P: 0.693189; F1: 0.692777\n",
      "(valid @ 28): L: 0.579287; A: 0.732456; R: 0.732456; P: 0.753380; F1: 0.735311\n",
      "(train @ 29): L: 0.578866; A: 0.706140; R: 0.706140; P: 0.736391; F1: 0.701461\n",
      "(valid @ 29): L: 0.574491; A: 0.741228; R: 0.741228; P: 0.776875; F1: 0.742284\n",
      "(train @ 30): L: 0.542168; A: 0.758772; R: 0.758772; P: 0.766843; F1: 0.758109\n",
      "(valid @ 30): L: 0.565011; A: 0.719298; R: 0.719298; P: 0.754422; F1: 0.711343\n",
      "Best val Metric 0.742284 @ 29\n",
      "\n",
      "models are saved @ ./predictions/211209043050/flvl_1_3_4_6_7_9_pretrained_model_c1.pt\n",
      "Saving predictions @ ./predictions/211209043050/flvl_train_1_3_4_6_7_9_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209043050/flvl_valid_2_5_8_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209043050/flvl_public_test_trained_on_1_3_4_6_7_9_r21d_rgb.csv\n",
      "{'train': [2, 3, 5, 6, 8, 9], 'valid': [1, 4, 7], 'public_test': [10, 11, 12], 'private_test': [13, 14, 15]}\n",
      "(train @ 1): L: 1.116051; A: 0.491228; R: 0.491228; P: 0.473681; F1: 0.475496\n",
      "(valid @ 1): L: 1.039074; A: 0.521930; R: 0.521930; P: 0.475281; F1: 0.467501\n",
      "(train @ 2): L: 1.038896; A: 0.453947; R: 0.453947; P: 0.382360; F1: 0.414850\n",
      "(valid @ 2): L: 0.986491; A: 0.491228; R: 0.491228; P: 0.491248; F1: 0.369576\n",
      "(train @ 3): L: 1.044793; A: 0.434211; R: 0.434211; P: 0.466571; F1: 0.421542\n",
      "(valid @ 3): L: 1.033089; A: 0.407895; R: 0.407895; P: 0.174035; F1: 0.243974\n",
      "(train @ 4): L: 0.982291; A: 0.447368; R: 0.447368; P: 0.536347; F1: 0.402890\n",
      "(valid @ 4): L: 0.949182; A: 0.500000; R: 0.500000; P: 0.611572; F1: 0.446380\n",
      "(train @ 5): L: 0.930563; A: 0.513158; R: 0.513158; P: 0.614714; F1: 0.469808\n",
      "(valid @ 5): L: 0.962476; A: 0.421053; R: 0.421053; P: 0.335673; F1: 0.257756\n",
      "(train @ 6): L: 0.911742; A: 0.475877; R: 0.475877; P: 0.481925; F1: 0.475186\n",
      "(valid @ 6): L: 0.892107; A: 0.596491; R: 0.596491; P: 0.649997; F1: 0.573000\n",
      "(train @ 7): L: 0.874419; A: 0.500000; R: 0.500000; P: 0.563731; F1: 0.449044\n",
      "(valid @ 7): L: 0.872601; A: 0.618421; R: 0.618421; P: 0.657081; F1: 0.610431\n",
      "(train @ 8): L: 0.888331; A: 0.561404; R: 0.561404; P: 0.667027; F1: 0.524789\n",
      "(valid @ 8): L: 0.906091; A: 0.469298; R: 0.469298; P: 0.344167; F1: 0.332190\n",
      "(train @ 9): L: 0.809649; A: 0.563596; R: 0.563596; P: 0.575135; F1: 0.561523\n",
      "(valid @ 9): L: 0.796548; A: 0.605263; R: 0.605263; P: 0.643725; F1: 0.605916\n",
      "(train @ 10): L: 0.803111; A: 0.524123; R: 0.524123; P: 0.529121; F1: 0.516966\n",
      "(valid @ 10): L: 0.803176; A: 0.605263; R: 0.605263; P: 0.646844; F1: 0.604973\n",
      "(train @ 11): L: 0.780556; A: 0.541667; R: 0.541667; P: 0.638564; F1: 0.465357\n",
      "(valid @ 11): L: 0.905922; A: 0.478070; R: 0.478070; P: 0.485380; F1: 0.432599\n",
      "(train @ 12): L: 0.806989; A: 0.550439; R: 0.550439; P: 0.557529; F1: 0.529504\n",
      "(valid @ 12): L: 0.833663; A: 0.574561; R: 0.574561; P: 0.694397; F1: 0.535029\n",
      "(train @ 13): L: 0.748660; A: 0.603070; R: 0.603070; P: 0.654446; F1: 0.572096\n",
      "(valid @ 13): L: 0.996983; A: 0.478070; R: 0.478070; P: 0.766952; F1: 0.347571\n",
      "(train @ 14): L: 0.812581; A: 0.521930; R: 0.521930; P: 0.521543; F1: 0.513750\n",
      "(valid @ 14): L: 0.740450; A: 0.644737; R: 0.644737; P: 0.678320; F1: 0.640048\n",
      "(train @ 15): L: 0.746794; A: 0.600877; R: 0.600877; P: 0.605533; F1: 0.599363\n",
      "(valid @ 15): L: 0.774899; A: 0.592105; R: 0.592105; P: 0.698006; F1: 0.555479\n",
      "(train @ 16): L: 0.683161; A: 0.616228; R: 0.616228; P: 0.620691; F1: 0.612876\n",
      "(valid @ 16): L: 0.734425; A: 0.644737; R: 0.644737; P: 0.682448; F1: 0.641211\n",
      "(train @ 17): L: 0.723312; A: 0.620614; R: 0.620614; P: 0.650799; F1: 0.605748\n",
      "(valid @ 17): L: 0.803043; A: 0.679825; R: 0.679825; P: 0.705179; F1: 0.677769\n",
      "(train @ 18): L: 0.694174; A: 0.640351; R: 0.640351; P: 0.669116; F1: 0.627503\n",
      "(valid @ 18): L: 0.763724; A: 0.644737; R: 0.644737; P: 0.676428; F1: 0.647050\n",
      "(train @ 19): L: 0.658233; A: 0.646930; R: 0.646930; P: 0.683538; F1: 0.630474\n",
      "(valid @ 19): L: 0.847411; A: 0.635965; R: 0.635965; P: 0.668476; F1: 0.628780\n",
      "(train @ 20): L: 0.729773; A: 0.620614; R: 0.620614; P: 0.632739; F1: 0.618683\n",
      "(valid @ 20): L: 0.671097; A: 0.688596; R: 0.688596; P: 0.715731; F1: 0.689193\n",
      "(train @ 21): L: 0.714285; A: 0.607456; R: 0.607456; P: 0.641399; F1: 0.592459\n",
      "(valid @ 21): L: 0.697543; A: 0.706140; R: 0.706140; P: 0.724875; F1: 0.708431\n",
      "(train @ 22): L: 0.682774; A: 0.620614; R: 0.620614; P: 0.688099; F1: 0.583120\n",
      "(valid @ 22): L: 0.885026; A: 0.675439; R: 0.675439; P: 0.754609; F1: 0.668622\n",
      "(train @ 23): L: 0.694624; A: 0.695175; R: 0.695175; P: 0.730845; F1: 0.687542\n",
      "(valid @ 23): L: 0.800583; A: 0.614035; R: 0.614035; P: 0.642342; F1: 0.612837\n",
      "(train @ 24): L: 0.669673; A: 0.622807; R: 0.622807; P: 0.624533; F1: 0.623501\n",
      "(valid @ 24): L: 0.648166; A: 0.692982; R: 0.692982; P: 0.771226; F1: 0.677856\n",
      "(train @ 25): L: 0.699400; A: 0.609649; R: 0.609649; P: 0.612615; F1: 0.607035\n",
      "(valid @ 25): L: 0.689176; A: 0.644737; R: 0.644737; P: 0.715778; F1: 0.629056\n",
      "(train @ 26): L: 0.686051; A: 0.622807; R: 0.622807; P: 0.717539; F1: 0.580579\n",
      "(valid @ 26): L: 0.918033; A: 0.535088; R: 0.535088; P: 0.618370; F1: 0.478043\n",
      "(train @ 27): L: 0.704827; A: 0.600877; R: 0.600877; P: 0.610285; F1: 0.588160\n",
      "(valid @ 27): L: 0.724680; A: 0.644737; R: 0.644737; P: 0.763769; F1: 0.619707\n",
      "(train @ 28): L: 0.646611; A: 0.657895; R: 0.657895; P: 0.670465; F1: 0.660973\n",
      "(valid @ 28): L: 0.645503; A: 0.679825; R: 0.679825; P: 0.696961; F1: 0.680904\n",
      "(train @ 29): L: 0.576109; A: 0.714912; R: 0.714912; P: 0.736674; F1: 0.708934\n",
      "(valid @ 29): L: 0.703199; A: 0.706140; R: 0.706140; P: 0.752396; F1: 0.704795\n",
      "(train @ 30): L: 0.554433; A: 0.754386; R: 0.754386; P: 0.760766; F1: 0.754478\n",
      "(valid @ 30): L: 0.580798; A: 0.732456; R: 0.732456; P: 0.746299; F1: 0.734111\n",
      "Best val Metric 0.734111 @ 30\n",
      "\n",
      "models are saved @ ./predictions/211209043050/flvl_2_3_5_6_8_9_pretrained_model_c1.pt\n",
      "Saving predictions @ ./predictions/211209043050/flvl_train_2_3_5_6_8_9_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209043050/flvl_valid_1_4_7_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209043050/flvl_public_test_trained_on_2_3_5_6_8_9_r21d_rgb.csv\n",
      "Average of Best Metrics on Each Valid Set: 0.716930, 211209043050\n",
      "Saved test results @ ./predictions/211209043050/flvl_public_test_agg_r21d_rgb.csv\n"
     ]
    }
   ],
   "source": [
    "# View 0,1\n",
    "from main_2Views import Config, run_kfold\n",
    "\n",
    "# define architecture and run k-fold training\n",
    "cfg = Config()\n",
    "cfg.assign_variable('task', 'flvl')\n",
    "cfg.assign_variable('output_dim', 3)\n",
    "cfg.assign_variable('model_type', 'GRU')\n",
    "cfg.assign_variable('bi_dir', False)\n",
    "cfg.assign_variable('device', 'cuda:0')\n",
    "cfg.assign_variable('data_root', './r21d_rgb_features')\n",
    "cfg.assign_variable('drop_p', 0.0) # results will be irreproducible\n",
    "cfg.assign_variable('batch_size', 64)\n",
    "cfg.assign_variable('input_dim', 512)\n",
    "cfg.assign_variable('hidden_dim', 512)\n",
    "cfg.assign_variable('n_layers', 3)\n",
    "cfg.assign_variable('num_epochs', 30)\n",
    "cfg.assign_variable('seed', 1337)\n",
    "\n",
    "run_kfold(cfg,[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': [1, 2, 4, 5, 7, 8], 'valid': [3, 6, 9], 'public_test': [10, 11, 12], 'private_test': [13, 14, 15]}\n",
      "(train @ 1): L: 1.116037; A: 0.473684; R: 0.473684; P: 0.426184; F1: 0.438835\n",
      "(valid @ 1): L: 1.088412; A: 0.464912; R: 0.464912; P: 0.421637; F1: 0.398932\n",
      "(train @ 2): L: 0.998028; A: 0.480263; R: 0.480263; P: 0.404167; F1: 0.438587\n",
      "(valid @ 2): L: 1.019114; A: 0.508772; R: 0.508772; P: 0.547315; F1: 0.488358\n",
      "(train @ 3): L: 0.954541; A: 0.526316; R: 0.526316; P: 0.694755; F1: 0.462235\n",
      "(valid @ 3): L: 0.991600; A: 0.469298; R: 0.469298; P: 0.569195; F1: 0.370239\n",
      "(train @ 4): L: 0.920327; A: 0.510965; R: 0.510965; P: 0.549107; F1: 0.495997\n",
      "(valid @ 4): L: 1.002889; A: 0.491228; R: 0.491228; P: 0.591752; F1: 0.447366\n",
      "(train @ 5): L: 0.849105; A: 0.539474; R: 0.539474; P: 0.572598; F1: 0.531391\n",
      "(valid @ 5): L: 0.890125; A: 0.587719; R: 0.587719; P: 0.629474; F1: 0.590219\n",
      "(train @ 6): L: 0.797527; A: 0.631579; R: 0.631579; P: 0.695074; F1: 0.619568\n",
      "(valid @ 6): L: 0.996914; A: 0.491228; R: 0.491228; P: 0.659903; F1: 0.392591\n",
      "(train @ 7): L: 0.857261; A: 0.535088; R: 0.535088; P: 0.539557; F1: 0.533476\n",
      "(valid @ 7): L: 0.879727; A: 0.614035; R: 0.614035; P: 0.633896; F1: 0.604496\n",
      "(train @ 8): L: 0.730847; A: 0.640351; R: 0.640351; P: 0.640184; F1: 0.640242\n",
      "(valid @ 8): L: 0.882162; A: 0.614035; R: 0.614035; P: 0.647308; F1: 0.602968\n",
      "(train @ 9): L: 0.739021; A: 0.644737; R: 0.644737; P: 0.692709; F1: 0.625547\n",
      "(valid @ 9): L: 0.944783; A: 0.504386; R: 0.504386; P: 0.772350; F1: 0.379001\n",
      "(train @ 10): L: 0.834006; A: 0.550439; R: 0.550439; P: 0.553989; F1: 0.550183\n",
      "(valid @ 10): L: 0.901169; A: 0.578947; R: 0.578947; P: 0.618100; F1: 0.559689\n",
      "(train @ 11): L: 0.731336; A: 0.620614; R: 0.620614; P: 0.643931; F1: 0.615520\n",
      "(valid @ 11): L: 0.863200; A: 0.543860; R: 0.543860; P: 0.340411; F1: 0.407335\n",
      "(train @ 12): L: 0.665466; A: 0.620614; R: 0.620614; P: 0.638901; F1: 0.615057\n",
      "(valid @ 12): L: 0.906939; A: 0.583333; R: 0.583333; P: 0.584684; F1: 0.559381\n",
      "(train @ 13): L: 0.709444; A: 0.666667; R: 0.666667; P: 0.683381; F1: 0.661608\n",
      "(valid @ 13): L: 0.793392; A: 0.622807; R: 0.622807; P: 0.621708; F1: 0.620477\n",
      "(train @ 14): L: 0.615527; A: 0.695175; R: 0.695175; P: 0.734577; F1: 0.686483\n",
      "(valid @ 14): L: 0.880112; A: 0.622807; R: 0.622807; P: 0.625222; F1: 0.612291\n",
      "(train @ 15): L: 0.708730; A: 0.653509; R: 0.653509; P: 0.662731; F1: 0.653877\n",
      "(valid @ 15): L: 0.769790; A: 0.627193; R: 0.627193; P: 0.630384; F1: 0.615812\n",
      "(train @ 16): L: 0.621686; A: 0.695175; R: 0.695175; P: 0.716177; F1: 0.689880\n",
      "(valid @ 16): L: 0.721263; A: 0.635965; R: 0.635965; P: 0.637206; F1: 0.628118\n",
      "(train @ 17): L: 0.608925; A: 0.732456; R: 0.732456; P: 0.737557; F1: 0.731692\n",
      "(valid @ 17): L: 0.737272; A: 0.627193; R: 0.627193; P: 0.634011; F1: 0.614753\n",
      "(train @ 18): L: 0.655591; A: 0.660088; R: 0.660088; P: 0.662800; F1: 0.657033\n",
      "(valid @ 18): L: 0.716900; A: 0.671053; R: 0.671053; P: 0.671819; F1: 0.668191\n",
      "(train @ 19): L: 0.575280; A: 0.725877; R: 0.725877; P: 0.767626; F1: 0.716973\n",
      "(valid @ 19): L: 0.693068; A: 0.631579; R: 0.631579; P: 0.717713; F1: 0.595436\n",
      "(train @ 20): L: 0.589110; A: 0.754386; R: 0.754386; P: 0.756620; F1: 0.754859\n",
      "(valid @ 20): L: 0.674878; A: 0.631579; R: 0.631579; P: 0.643002; F1: 0.622175\n",
      "(train @ 21): L: 0.577156; A: 0.712719; R: 0.712719; P: 0.725601; F1: 0.711646\n",
      "(valid @ 21): L: 0.672621; A: 0.701754; R: 0.701754; P: 0.725726; F1: 0.696405\n",
      "(train @ 22): L: 0.591436; A: 0.717105; R: 0.717105; P: 0.744888; F1: 0.710263\n",
      "(valid @ 22): L: 0.711572; A: 0.644737; R: 0.644737; P: 0.646663; F1: 0.636633\n",
      "(train @ 23): L: 0.525460; A: 0.782895; R: 0.782895; P: 0.801997; F1: 0.781496\n",
      "(valid @ 23): L: 0.774289; A: 0.627193; R: 0.627193; P: 0.632371; F1: 0.615746\n",
      "(train @ 24): L: 0.537472; A: 0.734649; R: 0.734649; P: 0.734067; F1: 0.734285\n",
      "(valid @ 24): L: 0.705310; A: 0.684211; R: 0.684211; P: 0.689095; F1: 0.681789\n",
      "(train @ 25): L: 0.498350; A: 0.765351; R: 0.765351; P: 0.777047; F1: 0.762450\n",
      "(valid @ 25): L: 0.970597; A: 0.592105; R: 0.592105; P: 0.724471; F1: 0.525734\n",
      "(train @ 26): L: 0.663245; A: 0.664474; R: 0.664474; P: 0.670997; F1: 0.659909\n",
      "(valid @ 26): L: 0.857005; A: 0.587719; R: 0.587719; P: 0.616015; F1: 0.560107\n",
      "(train @ 27): L: 0.594728; A: 0.706140; R: 0.706140; P: 0.707243; F1: 0.705636\n",
      "(valid @ 27): L: 0.646366; A: 0.706140; R: 0.706140; P: 0.715681; F1: 0.705859\n",
      "(train @ 28): L: 0.477860; A: 0.774123; R: 0.774123; P: 0.775541; F1: 0.773317\n",
      "(valid @ 28): L: 0.630409; A: 0.684211; R: 0.684211; P: 0.706290; F1: 0.680182\n",
      "(train @ 29): L: 0.468206; A: 0.771930; R: 0.771930; P: 0.787025; F1: 0.770715\n",
      "(valid @ 29): L: 0.768337; A: 0.675439; R: 0.675439; P: 0.692289; F1: 0.674518\n",
      "(train @ 30): L: 0.434067; A: 0.785088; R: 0.785088; P: 0.802752; F1: 0.783970\n",
      "(valid @ 30): L: 0.923750; A: 0.653509; R: 0.653509; P: 0.731898; F1: 0.641849\n",
      "Best val Metric 0.705859 @ 27\n",
      "\n",
      "models are saved @ ./predictions/211209045810/flvl_1_2_4_5_7_8_pretrained_model_c1.pt\n",
      "Saving predictions @ ./predictions/211209045810/flvl_train_1_2_4_5_7_8_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209045810/flvl_valid_3_6_9_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209045810/flvl_public_test_trained_on_1_2_4_5_7_8_r21d_rgb.csv\n",
      "{'train': [1, 3, 4, 6, 7, 9], 'valid': [2, 5, 8], 'public_test': [10, 11, 12], 'private_test': [13, 14, 15]}\n",
      "(train @ 1): L: 1.089141; A: 0.495614; R: 0.495614; P: 0.466790; F1: 0.464192\n",
      "(valid @ 1): L: 0.948345; A: 0.517544; R: 0.517544; P: 0.469491; F1: 0.463894\n",
      "(train @ 2): L: 1.016833; A: 0.460526; R: 0.460526; P: 0.387910; F1: 0.420909\n",
      "(valid @ 2): L: 0.950204; A: 0.666667; R: 0.666667; P: 0.695161; F1: 0.658649\n",
      "(train @ 3): L: 1.002670; A: 0.471491; R: 0.471491; P: 0.549730; F1: 0.435163\n",
      "(valid @ 3): L: 1.059910; A: 0.394737; R: 0.394737; P: 0.170697; F1: 0.238332\n",
      "(train @ 4): L: 0.959392; A: 0.497807; R: 0.497807; P: 0.522172; F1: 0.480909\n",
      "(valid @ 4): L: 0.954393; A: 0.513158; R: 0.513158; P: 0.468010; F1: 0.457494\n",
      "(train @ 5): L: 0.904694; A: 0.508772; R: 0.508772; P: 0.565689; F1: 0.484528\n",
      "(valid @ 5): L: 0.927787; A: 0.460526; R: 0.460526; P: 0.530296; F1: 0.350121\n",
      "(train @ 6): L: 0.881679; A: 0.554825; R: 0.554825; P: 0.571107; F1: 0.543755\n",
      "(valid @ 6): L: 0.998472; A: 0.421053; R: 0.421053; P: 0.177285; F1: 0.249513\n",
      "(train @ 7): L: 0.855847; A: 0.491228; R: 0.491228; P: 0.558729; F1: 0.451003\n",
      "(valid @ 7): L: 0.830842; A: 0.635965; R: 0.635965; P: 0.639078; F1: 0.627680\n",
      "(train @ 8): L: 0.860315; A: 0.585526; R: 0.585526; P: 0.624650; F1: 0.585124\n",
      "(valid @ 8): L: 0.848841; A: 0.622807; R: 0.622807; P: 0.672252; F1: 0.612007\n",
      "(train @ 9): L: 0.804893; A: 0.603070; R: 0.603070; P: 0.645393; F1: 0.577136\n",
      "(valid @ 9): L: 0.836516; A: 0.570175; R: 0.570175; P: 0.753573; F1: 0.520626\n",
      "(train @ 10): L: 0.817928; A: 0.554825; R: 0.554825; P: 0.572693; F1: 0.554682\n",
      "(valid @ 10): L: 0.775205; A: 0.618421; R: 0.618421; P: 0.655898; F1: 0.607789\n",
      "(train @ 11): L: 0.795010; A: 0.596491; R: 0.596491; P: 0.650317; F1: 0.564264\n",
      "(valid @ 11): L: 0.976377; A: 0.513158; R: 0.513158; P: 0.353166; F1: 0.383149\n",
      "(train @ 12): L: 0.774762; A: 0.565789; R: 0.565789; P: 0.573381; F1: 0.560754\n",
      "(valid @ 12): L: 0.821785; A: 0.592105; R: 0.592105; P: 0.646783; F1: 0.580752\n",
      "(train @ 13): L: 0.749024; A: 0.631579; R: 0.631579; P: 0.658345; F1: 0.618215\n",
      "(valid @ 13): L: 0.976713; A: 0.491228; R: 0.491228; P: 0.353166; F1: 0.363972\n",
      "(train @ 14): L: 0.791295; A: 0.565789; R: 0.565789; P: 0.570107; F1: 0.567278\n",
      "(valid @ 14): L: 0.720523; A: 0.688596; R: 0.688596; P: 0.690058; F1: 0.686232\n",
      "(train @ 15): L: 0.789547; A: 0.633772; R: 0.633772; P: 0.646184; F1: 0.631999\n",
      "(valid @ 15): L: 0.735743; A: 0.697368; R: 0.697368; P: 0.754451; F1: 0.688346\n",
      "(train @ 16): L: 0.701224; A: 0.673246; R: 0.673246; P: 0.684973; F1: 0.671978\n",
      "(valid @ 16): L: 0.728539; A: 0.671053; R: 0.671053; P: 0.702708; F1: 0.671800\n",
      "(train @ 17): L: 0.682835; A: 0.684211; R: 0.684211; P: 0.689292; F1: 0.680718\n",
      "(valid @ 17): L: 0.679000; A: 0.714912; R: 0.714912; P: 0.731654; F1: 0.717706\n",
      "(train @ 18): L: 0.718555; A: 0.640351; R: 0.640351; P: 0.665051; F1: 0.627641\n",
      "(valid @ 18): L: 0.690319; A: 0.649123; R: 0.649123; P: 0.739828; F1: 0.607522\n",
      "(train @ 19): L: 0.677472; A: 0.677632; R: 0.677632; P: 0.703980; F1: 0.673023\n",
      "(valid @ 19): L: 0.720721; A: 0.688596; R: 0.688596; P: 0.716571; F1: 0.682747\n",
      "(train @ 20): L: 0.666653; A: 0.690789; R: 0.690789; P: 0.690632; F1: 0.690605\n",
      "(valid @ 20): L: 0.618033; A: 0.719298; R: 0.719298; P: 0.730141; F1: 0.720103\n",
      "(train @ 21): L: 0.679478; A: 0.633772; R: 0.633772; P: 0.670179; F1: 0.623601\n",
      "(valid @ 21): L: 0.644572; A: 0.640351; R: 0.640351; P: 0.649350; F1: 0.636629\n",
      "(train @ 22): L: 0.667409; A: 0.646930; R: 0.646930; P: 0.673831; F1: 0.632968\n",
      "(valid @ 22): L: 0.723008; A: 0.692982; R: 0.692982; P: 0.726320; F1: 0.695302\n",
      "(train @ 23): L: 0.650005; A: 0.686404; R: 0.686404; P: 0.707772; F1: 0.685314\n",
      "(valid @ 23): L: 0.724880; A: 0.657895; R: 0.657895; P: 0.683628; F1: 0.657253\n",
      "(train @ 24): L: 0.632629; A: 0.679825; R: 0.679825; P: 0.678401; F1: 0.678737\n",
      "(valid @ 24): L: 0.575260; A: 0.763158; R: 0.763158; P: 0.789864; F1: 0.761417\n",
      "(train @ 25): L: 0.631407; A: 0.723684; R: 0.723684; P: 0.737320; F1: 0.721627\n",
      "(valid @ 25): L: 0.777655; A: 0.627193; R: 0.627193; P: 0.734723; F1: 0.580363\n",
      "(train @ 26): L: 0.721957; A: 0.642544; R: 0.642544; P: 0.651420; F1: 0.630662\n",
      "(valid @ 26): L: 0.814368; A: 0.530702; R: 0.530702; P: 0.583779; F1: 0.483127\n",
      "(train @ 27): L: 0.698409; A: 0.611842; R: 0.611842; P: 0.615921; F1: 0.613330\n",
      "(valid @ 27): L: 0.558591; A: 0.758772; R: 0.758772; P: 0.820796; F1: 0.754780\n",
      "(train @ 28): L: 0.576647; A: 0.725877; R: 0.725877; P: 0.733299; F1: 0.724461\n",
      "(valid @ 28): L: 0.596017; A: 0.741228; R: 0.741228; P: 0.819196; F1: 0.736389\n",
      "(train @ 29): L: 0.587019; A: 0.708333; R: 0.708333; P: 0.724580; F1: 0.704073\n",
      "(valid @ 29): L: 0.633298; A: 0.714912; R: 0.714912; P: 0.746062; F1: 0.713661\n",
      "(train @ 30): L: 0.522095; A: 0.756579; R: 0.756579; P: 0.767789; F1: 0.754525\n",
      "(valid @ 30): L: 0.513139; A: 0.776316; R: 0.776316; P: 0.808832; F1: 0.775694\n",
      "Best val Metric 0.775694 @ 30\n",
      "\n",
      "models are saved @ ./predictions/211209045810/flvl_1_3_4_6_7_9_pretrained_model_c1.pt\n",
      "Saving predictions @ ./predictions/211209045810/flvl_train_1_3_4_6_7_9_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209045810/flvl_valid_2_5_8_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209045810/flvl_public_test_trained_on_1_3_4_6_7_9_r21d_rgb.csv\n",
      "{'train': [2, 3, 5, 6, 8, 9], 'valid': [1, 4, 7], 'public_test': [10, 11, 12], 'private_test': [13, 14, 15]}\n",
      "(train @ 1): L: 1.119981; A: 0.480263; R: 0.480263; P: 0.460270; F1: 0.463938\n",
      "(valid @ 1): L: 1.048225; A: 0.521930; R: 0.521930; P: 0.475281; F1: 0.467501\n",
      "(train @ 2): L: 1.057171; A: 0.453947; R: 0.453947; P: 0.382360; F1: 0.414850\n",
      "(valid @ 2): L: 0.988797; A: 0.495614; R: 0.495614; P: 0.284823; F1: 0.357632\n",
      "(train @ 3): L: 1.060453; A: 0.436404; R: 0.436404; P: 0.468159; F1: 0.423071\n",
      "(valid @ 3): L: 1.043806; A: 0.403509; R: 0.403509; P: 0.172932; F1: 0.242105\n",
      "(train @ 4): L: 0.992911; A: 0.449561; R: 0.449561; P: 0.540509; F1: 0.400882\n",
      "(valid @ 4): L: 0.953614; A: 0.504386; R: 0.504386; P: 0.612703; F1: 0.455396\n",
      "(train @ 5): L: 0.933929; A: 0.504386; R: 0.504386; P: 0.618969; F1: 0.453022\n",
      "(valid @ 5): L: 0.972066; A: 0.421053; R: 0.421053; P: 0.334886; F1: 0.256982\n",
      "(train @ 6): L: 0.914410; A: 0.478070; R: 0.478070; P: 0.486531; F1: 0.477041\n",
      "(valid @ 6): L: 0.911560; A: 0.565789; R: 0.565789; P: 0.630370; F1: 0.538635\n",
      "(train @ 7): L: 0.872047; A: 0.484649; R: 0.484649; P: 0.534857; F1: 0.428345\n",
      "(valid @ 7): L: 0.893924; A: 0.618421; R: 0.618421; P: 0.632355; F1: 0.611445\n",
      "(train @ 8): L: 0.903837; A: 0.543860; R: 0.543860; P: 0.654735; F1: 0.510581\n",
      "(valid @ 8): L: 0.918213; A: 0.469298; R: 0.469298; P: 0.344167; F1: 0.332190\n",
      "(train @ 9): L: 0.807364; A: 0.561404; R: 0.561404; P: 0.573423; F1: 0.551852\n",
      "(valid @ 9): L: 0.810207; A: 0.600877; R: 0.600877; P: 0.648384; F1: 0.598771\n",
      "(train @ 10): L: 0.800680; A: 0.515351; R: 0.515351; P: 0.520981; F1: 0.507005\n",
      "(valid @ 10): L: 0.787324; A: 0.600877; R: 0.600877; P: 0.648384; F1: 0.598771\n",
      "(train @ 11): L: 0.783733; A: 0.552632; R: 0.552632; P: 0.664653; F1: 0.469579\n",
      "(valid @ 11): L: 0.894114; A: 0.482456; R: 0.482456; P: 0.490750; F1: 0.435431\n",
      "(train @ 12): L: 0.805549; A: 0.541667; R: 0.541667; P: 0.546761; F1: 0.509120\n",
      "(valid @ 12): L: 0.836200; A: 0.574561; R: 0.574561; P: 0.687055; F1: 0.542610\n",
      "(train @ 13): L: 0.745787; A: 0.603070; R: 0.603070; P: 0.656069; F1: 0.573731\n",
      "(valid @ 13): L: 1.000636; A: 0.478070; R: 0.478070; P: 0.347665; F1: 0.345406\n",
      "(train @ 14): L: 0.813008; A: 0.517544; R: 0.517544; P: 0.519373; F1: 0.512750\n",
      "(valid @ 14): L: 0.736938; A: 0.644737; R: 0.644737; P: 0.676161; F1: 0.639244\n",
      "(train @ 15): L: 0.741871; A: 0.609649; R: 0.609649; P: 0.613200; F1: 0.610675\n",
      "(valid @ 15): L: 0.765691; A: 0.635965; R: 0.635965; P: 0.700421; F1: 0.627999\n",
      "(train @ 16): L: 0.668867; A: 0.675439; R: 0.675439; P: 0.708506; F1: 0.663522\n",
      "(valid @ 16): L: 0.729498; A: 0.627193; R: 0.627193; P: 0.661170; F1: 0.625802\n",
      "(train @ 17): L: 0.693825; A: 0.677632; R: 0.677632; P: 0.702615; F1: 0.673018\n",
      "(valid @ 17): L: 0.787130; A: 0.666667; R: 0.666667; P: 0.694517; F1: 0.667938\n",
      "(train @ 18): L: 0.678326; A: 0.671053; R: 0.671053; P: 0.708390; F1: 0.658139\n",
      "(valid @ 18): L: 0.741594; A: 0.706140; R: 0.706140; P: 0.725929; F1: 0.707833\n",
      "(train @ 19): L: 0.633149; A: 0.697368; R: 0.697368; P: 0.738021; F1: 0.686461\n",
      "(valid @ 19): L: 0.811314; A: 0.710526; R: 0.710526; P: 0.736774; F1: 0.704088\n",
      "(train @ 20): L: 0.695222; A: 0.679825; R: 0.679825; P: 0.685554; F1: 0.679162\n",
      "(valid @ 20): L: 0.668801; A: 0.684211; R: 0.684211; P: 0.713643; F1: 0.683931\n",
      "(train @ 21): L: 0.676164; A: 0.631579; R: 0.631579; P: 0.655127; F1: 0.624168\n",
      "(valid @ 21): L: 0.651084; A: 0.728070; R: 0.728070; P: 0.777707; F1: 0.725750\n",
      "(train @ 22): L: 0.664871; A: 0.657895; R: 0.657895; P: 0.711406; F1: 0.634985\n",
      "(valid @ 22): L: 0.821228; A: 0.666667; R: 0.666667; P: 0.698600; F1: 0.665846\n",
      "(train @ 23): L: 0.662807; A: 0.666667; R: 0.666667; P: 0.671795; F1: 0.663919\n",
      "(valid @ 23): L: 0.796111; A: 0.653509; R: 0.653509; P: 0.682326; F1: 0.650748\n",
      "(train @ 24): L: 0.629799; A: 0.657895; R: 0.657895; P: 0.656652; F1: 0.656776\n",
      "(valid @ 24): L: 0.623734; A: 0.688596; R: 0.688596; P: 0.753110; F1: 0.678587\n",
      "(train @ 25): L: 0.649419; A: 0.664474; R: 0.664474; P: 0.677276; F1: 0.660611\n",
      "(valid @ 25): L: 0.674571; A: 0.627193; R: 0.627193; P: 0.714089; F1: 0.604215\n",
      "(train @ 26): L: 0.649041; A: 0.664474; R: 0.664474; P: 0.699403; F1: 0.646367\n",
      "(valid @ 26): L: 0.943989; A: 0.517544; R: 0.517544; P: 0.661025; F1: 0.426652\n",
      "(train @ 27): L: 0.693319; A: 0.633772; R: 0.633772; P: 0.637318; F1: 0.632705\n",
      "(valid @ 27): L: 0.642881; A: 0.671053; R: 0.671053; P: 0.734591; F1: 0.661663\n",
      "(train @ 28): L: 0.590584; A: 0.695175; R: 0.695175; P: 0.705994; F1: 0.694991\n",
      "(valid @ 28): L: 0.655643; A: 0.692982; R: 0.692982; P: 0.731812; F1: 0.693551\n",
      "(train @ 29): L: 0.568128; A: 0.725877; R: 0.725877; P: 0.752753; F1: 0.719782\n",
      "(valid @ 29): L: 0.749844; A: 0.697368; R: 0.697368; P: 0.726335; F1: 0.695405\n",
      "(train @ 30): L: 0.527433; A: 0.763158; R: 0.763158; P: 0.768517; F1: 0.762086\n",
      "(valid @ 30): L: 0.556761; A: 0.745614; R: 0.745614; P: 0.754361; F1: 0.747617\n",
      "Best val Metric 0.747617 @ 30\n",
      "\n",
      "models are saved @ ./predictions/211209045810/flvl_2_3_5_6_8_9_pretrained_model_c1.pt\n",
      "Saving predictions @ ./predictions/211209045810/flvl_train_2_3_5_6_8_9_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209045810/flvl_valid_1_4_7_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209045810/flvl_public_test_trained_on_2_3_5_6_8_9_r21d_rgb.csv\n",
      "Average of Best Metrics on Each Valid Set: 0.743057, 211209045810\n",
      "Saved test results @ ./predictions/211209045810/flvl_public_test_agg_r21d_rgb.csv\n"
     ]
    }
   ],
   "source": [
    "# View 0,2\n",
    "from main_2Views import Config, run_kfold\n",
    "\n",
    "# define architecture and run k-fold training\n",
    "cfg = Config()\n",
    "cfg.assign_variable('task', 'flvl')\n",
    "cfg.assign_variable('output_dim', 3)\n",
    "cfg.assign_variable('model_type', 'GRU')\n",
    "cfg.assign_variable('bi_dir', False)\n",
    "cfg.assign_variable('device', 'cuda:0')\n",
    "cfg.assign_variable('data_root', './r21d_rgb_features')\n",
    "cfg.assign_variable('drop_p', 0.0) # results will be irreproducible\n",
    "cfg.assign_variable('batch_size', 64)\n",
    "cfg.assign_variable('input_dim', 512)\n",
    "cfg.assign_variable('hidden_dim', 512)\n",
    "cfg.assign_variable('n_layers', 3)\n",
    "cfg.assign_variable('num_epochs', 30)\n",
    "cfg.assign_variable('seed', 1337)\n",
    "\n",
    "run_kfold(cfg,[0,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': [1, 2, 4, 5, 7, 8], 'valid': [3, 6, 9], 'public_test': [10, 11, 12], 'private_test': [13, 14, 15]}\n",
      "(train @ 1): L: 1.150007; A: 0.442982; R: 0.442982; P: 0.428320; F1: 0.430255\n",
      "(valid @ 1): L: 1.155613; A: 0.451754; R: 0.451754; P: 0.406327; F1: 0.381783\n",
      "(train @ 2): L: 1.046674; A: 0.451754; R: 0.451754; P: 0.380499; F1: 0.412689\n",
      "(valid @ 2): L: 1.046522; A: 0.434211; R: 0.434211; P: 0.694216; F1: 0.334013\n",
      "(train @ 3): L: 0.963399; A: 0.493421; R: 0.493421; P: 0.548779; F1: 0.475444\n",
      "(valid @ 3): L: 1.014773; A: 0.482456; R: 0.482456; P: 0.404444; F1: 0.439887\n",
      "(train @ 4): L: 0.941689; A: 0.532895; R: 0.532895; P: 0.537694; F1: 0.526065\n",
      "(valid @ 4): L: 1.016452; A: 0.495614; R: 0.495614; P: 0.597845; F1: 0.450747\n",
      "(train @ 5): L: 0.870185; A: 0.515351; R: 0.515351; P: 0.612911; F1: 0.471038\n",
      "(valid @ 5): L: 0.935666; A: 0.535088; R: 0.535088; P: 0.605123; F1: 0.455544\n",
      "(train @ 6): L: 0.837957; A: 0.552632; R: 0.552632; P: 0.569926; F1: 0.547655\n",
      "(valid @ 6): L: 0.951820; A: 0.482456; R: 0.482456; P: 0.543774; F1: 0.452212\n",
      "(train @ 7): L: 0.850320; A: 0.530702; R: 0.530702; P: 0.530720; F1: 0.516923\n",
      "(valid @ 7): L: 0.930473; A: 0.578947; R: 0.578947; P: 0.632502; F1: 0.566585\n",
      "(train @ 8): L: 0.737730; A: 0.655702; R: 0.655702; P: 0.679374; F1: 0.653025\n",
      "(valid @ 8): L: 0.895545; A: 0.640351; R: 0.640351; P: 0.641494; F1: 0.637892\n",
      "(train @ 9): L: 0.774642; A: 0.616228; R: 0.616228; P: 0.657132; F1: 0.596881\n",
      "(valid @ 9): L: 0.901130; A: 0.508772; R: 0.508772; P: 0.352227; F1: 0.378710\n",
      "(train @ 10): L: 0.808268; A: 0.557018; R: 0.557018; P: 0.574321; F1: 0.543713\n",
      "(valid @ 10): L: 0.932263; A: 0.552632; R: 0.552632; P: 0.613467; F1: 0.533018\n",
      "(train @ 11): L: 0.793921; A: 0.622807; R: 0.622807; P: 0.714557; F1: 0.598250\n",
      "(valid @ 11): L: 0.913564; A: 0.521930; R: 0.521930; P: 0.312758; F1: 0.386166\n",
      "(train @ 12): L: 0.723912; A: 0.574561; R: 0.574561; P: 0.601759; F1: 0.537032\n",
      "(valid @ 12): L: 0.927059; A: 0.574561; R: 0.574561; P: 0.581262; F1: 0.551741\n",
      "(train @ 13): L: 0.748695; A: 0.638158; R: 0.638158; P: 0.711425; F1: 0.614326\n",
      "(valid @ 13): L: 0.853986; A: 0.561404; R: 0.561404; P: 0.662856; F1: 0.486954\n",
      "(train @ 14): L: 0.677740; A: 0.618421; R: 0.618421; P: 0.617927; F1: 0.611861\n",
      "(valid @ 14): L: 0.937536; A: 0.587719; R: 0.587719; P: 0.590042; F1: 0.567037\n",
      "(train @ 15): L: 0.747898; A: 0.620614; R: 0.620614; P: 0.632375; F1: 0.620473\n",
      "(valid @ 15): L: 0.786408; A: 0.657895; R: 0.657895; P: 0.657268; F1: 0.653926\n",
      "(train @ 16): L: 0.637226; A: 0.706140; R: 0.706140; P: 0.736144; F1: 0.698827\n",
      "(valid @ 16): L: 0.742646; A: 0.635965; R: 0.635965; P: 0.640377; F1: 0.627540\n",
      "(train @ 17): L: 0.627921; A: 0.695175; R: 0.695175; P: 0.700093; F1: 0.693870\n",
      "(valid @ 17): L: 0.744072; A: 0.649123; R: 0.649123; P: 0.652015; F1: 0.643284\n",
      "(train @ 18): L: 0.668723; A: 0.620614; R: 0.620614; P: 0.649628; F1: 0.609582\n",
      "(valid @ 18): L: 0.795998; A: 0.657895; R: 0.657895; P: 0.657986; F1: 0.654592\n",
      "(train @ 19): L: 0.617274; A: 0.710526; R: 0.710526; P: 0.740074; F1: 0.706387\n",
      "(valid @ 19): L: 0.709858; A: 0.671053; R: 0.671053; P: 0.678493; F1: 0.672762\n",
      "(train @ 20): L: 0.600897; A: 0.750000; R: 0.750000; P: 0.763760; F1: 0.748483\n",
      "(valid @ 20): L: 0.732003; A: 0.675439; R: 0.675439; P: 0.686288; F1: 0.673802\n",
      "(train @ 21): L: 0.584662; A: 0.714912; R: 0.714912; P: 0.760509; F1: 0.710301\n",
      "(valid @ 21): L: 0.762173; A: 0.671053; R: 0.671053; P: 0.684596; F1: 0.669977\n",
      "(train @ 22): L: 0.576584; A: 0.714912; R: 0.714912; P: 0.735772; F1: 0.708695\n",
      "(valid @ 22): L: 0.756194; A: 0.675439; R: 0.675439; P: 0.681267; F1: 0.672798\n",
      "(train @ 23): L: 0.551031; A: 0.743421; R: 0.743421; P: 0.758248; F1: 0.742117\n",
      "(valid @ 23): L: 0.758013; A: 0.666667; R: 0.666667; P: 0.671727; F1: 0.663078\n",
      "(train @ 24): L: 0.490864; A: 0.771930; R: 0.771930; P: 0.775954; F1: 0.770780\n",
      "(valid @ 24): L: 0.780642; A: 0.649123; R: 0.649123; P: 0.660452; F1: 0.641329\n",
      "(train @ 25): L: 0.467741; A: 0.793860; R: 0.793860; P: 0.795161; F1: 0.792723\n",
      "(valid @ 25): L: 0.914444; A: 0.618421; R: 0.618421; P: 0.702166; F1: 0.583090\n",
      "(train @ 26): L: 0.638192; A: 0.666667; R: 0.666667; P: 0.673897; F1: 0.668724\n",
      "(valid @ 26): L: 0.807975; A: 0.653509; R: 0.653509; P: 0.672341; F1: 0.646166\n",
      "(train @ 27): L: 0.507748; A: 0.778509; R: 0.778509; P: 0.779605; F1: 0.778676\n",
      "(valid @ 27): L: 0.723784; A: 0.649123; R: 0.649123; P: 0.653107; F1: 0.644672\n",
      "(train @ 28): L: 0.465090; A: 0.787281; R: 0.787281; P: 0.797632; F1: 0.786368\n",
      "(valid @ 28): L: 0.758304; A: 0.657895; R: 0.657895; P: 0.662238; F1: 0.655448\n",
      "(train @ 29): L: 0.445040; A: 0.782895; R: 0.782895; P: 0.786773; F1: 0.782984\n",
      "(valid @ 29): L: 0.790764; A: 0.671053; R: 0.671053; P: 0.694723; F1: 0.671731\n",
      "(train @ 30): L: 0.411108; A: 0.813596; R: 0.813596; P: 0.818448; F1: 0.813766\n",
      "(valid @ 30): L: 1.121980; A: 0.605263; R: 0.605263; P: 0.694505; F1: 0.593726\n",
      "Best val Metric 0.673802 @ 20\n",
      "\n",
      "models are saved @ ./predictions/211209052552/flvl_1_2_4_5_7_8_pretrained_model_c1.pt\n",
      "Saving predictions @ ./predictions/211209052552/flvl_train_1_2_4_5_7_8_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209052552/flvl_valid_3_6_9_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209052552/flvl_public_test_trained_on_1_2_4_5_7_8_r21d_rgb.csv\n",
      "{'train': [1, 3, 4, 6, 7, 9], 'valid': [2, 5, 8], 'public_test': [10, 11, 12], 'private_test': [13, 14, 15]}\n",
      "(train @ 1): L: 1.117059; A: 0.464912; R: 0.464912; P: 0.445723; F1: 0.441402\n",
      "(valid @ 1): L: 0.990491; A: 0.508772; R: 0.508772; P: 0.464268; F1: 0.452423\n",
      "(train @ 2): L: 1.038086; A: 0.460526; R: 0.460526; P: 0.387910; F1: 0.420909\n",
      "(valid @ 2): L: 0.970641; A: 0.517544; R: 0.517544; P: 0.758329; F1: 0.389583\n",
      "(train @ 3): L: 1.000937; A: 0.445175; R: 0.445175; P: 0.484378; F1: 0.435678\n",
      "(valid @ 3): L: 1.050450; A: 0.429825; R: 0.429825; P: 0.236437; F1: 0.280447\n",
      "(train @ 4): L: 0.962146; A: 0.475877; R: 0.475877; P: 0.484896; F1: 0.462669\n",
      "(valid @ 4): L: 0.944327; A: 0.517544; R: 0.517544; P: 0.469491; F1: 0.463894\n",
      "(train @ 5): L: 0.907605; A: 0.495614; R: 0.495614; P: 0.601752; F1: 0.447095\n",
      "(valid @ 5): L: 0.950061; A: 0.421053; R: 0.421053; P: 0.177285; F1: 0.249513\n",
      "(train @ 6): L: 0.896623; A: 0.524123; R: 0.524123; P: 0.527180; F1: 0.520102\n",
      "(valid @ 6): L: 0.942848; A: 0.456140; R: 0.456140; P: 0.460334; F1: 0.329168\n",
      "(train @ 7): L: 0.855092; A: 0.513158; R: 0.513158; P: 0.580810; F1: 0.474205\n",
      "(valid @ 7): L: 0.839021; A: 0.631579; R: 0.631579; P: 0.650346; F1: 0.622666\n",
      "(train @ 8): L: 0.858307; A: 0.594298; R: 0.594298; P: 0.638265; F1: 0.590626\n",
      "(valid @ 8): L: 0.867356; A: 0.618421; R: 0.618421; P: 0.657279; F1: 0.610286\n",
      "(train @ 9): L: 0.798929; A: 0.574561; R: 0.574561; P: 0.605529; F1: 0.546500\n",
      "(valid @ 9): L: 0.823593; A: 0.535088; R: 0.535088; P: 0.779052; F1: 0.433600\n",
      "(train @ 10): L: 0.801747; A: 0.530702; R: 0.530702; P: 0.546176; F1: 0.525808\n",
      "(valid @ 10): L: 0.794824; A: 0.614035; R: 0.614035; P: 0.654327; F1: 0.603542\n",
      "(train @ 11): L: 0.801893; A: 0.581140; R: 0.581140; P: 0.695623; F1: 0.523763\n",
      "(valid @ 11): L: 0.926887; A: 0.521930; R: 0.521930; P: 0.355071; F1: 0.391683\n",
      "(train @ 12): L: 0.783410; A: 0.554825; R: 0.554825; P: 0.571604; F1: 0.518178\n",
      "(valid @ 12): L: 0.839191; A: 0.574561; R: 0.574561; P: 0.641098; F1: 0.560248\n",
      "(train @ 13): L: 0.764513; A: 0.607456; R: 0.607456; P: 0.661878; F1: 0.584070\n",
      "(valid @ 13): L: 0.997440; A: 0.508772; R: 0.508772; P: 0.353166; F1: 0.379588\n",
      "(train @ 14): L: 0.821856; A: 0.524123; R: 0.524123; P: 0.524157; F1: 0.518398\n",
      "(valid @ 14): L: 0.723647; A: 0.644737; R: 0.644737; P: 0.647842; F1: 0.632256\n",
      "(train @ 15): L: 0.771849; A: 0.616228; R: 0.616228; P: 0.623774; F1: 0.611876\n",
      "(valid @ 15): L: 0.752193; A: 0.535088; R: 0.535088; P: 0.357999; F1: 0.403710\n",
      "(train @ 16): L: 0.704880; A: 0.616228; R: 0.616228; P: 0.615927; F1: 0.612202\n",
      "(valid @ 16): L: 0.744983; A: 0.662281; R: 0.662281; P: 0.699418; F1: 0.661733\n",
      "(train @ 17): L: 0.693932; A: 0.644737; R: 0.644737; P: 0.644524; F1: 0.641697\n",
      "(valid @ 17): L: 0.728044; A: 0.671053; R: 0.671053; P: 0.698942; F1: 0.666499\n",
      "(train @ 18): L: 0.693742; A: 0.640351; R: 0.640351; P: 0.678452; F1: 0.623937\n",
      "(valid @ 18): L: 0.665702; A: 0.706140; R: 0.706140; P: 0.712872; F1: 0.703277\n",
      "(train @ 19): L: 0.669326; A: 0.690789; R: 0.690789; P: 0.730027; F1: 0.681755\n",
      "(valid @ 19): L: 0.737963; A: 0.671053; R: 0.671053; P: 0.701017; F1: 0.664434\n",
      "(train @ 20): L: 0.657566; A: 0.662281; R: 0.662281; P: 0.684652; F1: 0.654246\n",
      "(valid @ 20): L: 0.648268; A: 0.714912; R: 0.714912; P: 0.728313; F1: 0.716545\n",
      "(train @ 21): L: 0.666508; A: 0.640351; R: 0.640351; P: 0.680473; F1: 0.624776\n",
      "(valid @ 21): L: 0.662131; A: 0.666667; R: 0.666667; P: 0.676049; F1: 0.664008\n",
      "(train @ 22): L: 0.659339; A: 0.684211; R: 0.684211; P: 0.702609; F1: 0.677487\n",
      "(valid @ 22): L: 0.737217; A: 0.649123; R: 0.649123; P: 0.784997; F1: 0.623842\n",
      "(train @ 23): L: 0.663381; A: 0.668860; R: 0.668860; P: 0.690160; F1: 0.663780\n",
      "(valid @ 23): L: 0.717778; A: 0.649123; R: 0.649123; P: 0.678248; F1: 0.649279\n",
      "(train @ 24): L: 0.611730; A: 0.695175; R: 0.695175; P: 0.699618; F1: 0.691010\n",
      "(valid @ 24): L: 0.603389; A: 0.736842; R: 0.736842; P: 0.744976; F1: 0.737838\n",
      "(train @ 25): L: 0.622871; A: 0.695175; R: 0.695175; P: 0.713271; F1: 0.690731\n",
      "(valid @ 25): L: 0.769442; A: 0.627193; R: 0.627193; P: 0.779163; F1: 0.572890\n",
      "(train @ 26): L: 0.707185; A: 0.644737; R: 0.644737; P: 0.663442; F1: 0.629434\n",
      "(valid @ 26): L: 0.789906; A: 0.521930; R: 0.521930; P: 0.556669; F1: 0.446147\n",
      "(train @ 27): L: 0.676866; A: 0.657895; R: 0.657895; P: 0.658635; F1: 0.656432\n",
      "(valid @ 27): L: 0.638730; A: 0.697368; R: 0.697368; P: 0.797763; F1: 0.685229\n",
      "(train @ 28): L: 0.578537; A: 0.719298; R: 0.719298; P: 0.719094; F1: 0.719066\n",
      "(valid @ 28): L: 0.598530; A: 0.714912; R: 0.714912; P: 0.787341; F1: 0.708927\n",
      "(train @ 29): L: 0.533687; A: 0.734649; R: 0.734649; P: 0.766185; F1: 0.729196\n",
      "(valid @ 29): L: 0.540000; A: 0.741228; R: 0.741228; P: 0.767614; F1: 0.740497\n",
      "(train @ 30): L: 0.509759; A: 0.771930; R: 0.771930; P: 0.779256; F1: 0.769551\n",
      "(valid @ 30): L: 0.610585; A: 0.675439; R: 0.675439; P: 0.700797; F1: 0.666596\n",
      "Best val Metric 0.740497 @ 29\n",
      "\n",
      "models are saved @ ./predictions/211209052552/flvl_1_3_4_6_7_9_pretrained_model_c1.pt\n",
      "Saving predictions @ ./predictions/211209052552/flvl_train_1_3_4_6_7_9_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209052552/flvl_valid_2_5_8_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209052552/flvl_public_test_trained_on_1_3_4_6_7_9_r21d_rgb.csv\n",
      "{'train': [2, 3, 5, 6, 8, 9], 'valid': [1, 4, 7], 'public_test': [10, 11, 12], 'private_test': [13, 14, 15]}\n",
      "(train @ 1): L: 1.144017; A: 0.453947; R: 0.453947; P: 0.444703; F1: 0.440321\n",
      "(valid @ 1): L: 1.038900; A: 0.508772; R: 0.508772; P: 0.464268; F1: 0.452423\n",
      "(train @ 2): L: 1.053896; A: 0.462719; R: 0.462719; P: 0.391530; F1: 0.422441\n",
      "(valid @ 2): L: 1.024555; A: 0.495614; R: 0.495614; P: 0.313881; F1: 0.359569\n",
      "(train @ 3): L: 1.043956; A: 0.440789; R: 0.440789; P: 0.483204; F1: 0.438629\n",
      "(valid @ 3): L: 1.050336; A: 0.535088; R: 0.535088; P: 0.479223; F1: 0.485323\n",
      "(train @ 4): L: 0.982053; A: 0.475877; R: 0.475877; P: 0.549254; F1: 0.428066\n",
      "(valid @ 4): L: 0.945626; A: 0.578947; R: 0.578947; P: 0.656742; F1: 0.559429\n",
      "(train @ 5): L: 0.927530; A: 0.532895; R: 0.532895; P: 0.688391; F1: 0.477408\n",
      "(valid @ 5): L: 0.974602; A: 0.425439; R: 0.425439; P: 0.267697; F1: 0.270666\n",
      "(train @ 6): L: 0.922493; A: 0.471491; R: 0.471491; P: 0.471698; F1: 0.470809\n",
      "(valid @ 6): L: 0.894274; A: 0.600877; R: 0.600877; P: 0.659299; F1: 0.571537\n",
      "(train @ 7): L: 0.876806; A: 0.486842; R: 0.486842; P: 0.555821; F1: 0.422032\n",
      "(valid @ 7): L: 0.878679; A: 0.618421; R: 0.618421; P: 0.657081; F1: 0.610431\n",
      "(train @ 8): L: 0.888332; A: 0.563596; R: 0.563596; P: 0.668628; F1: 0.526947\n",
      "(valid @ 8): L: 0.919107; A: 0.557018; R: 0.557018; P: 0.674143; F1: 0.518018\n",
      "(train @ 9): L: 0.813163; A: 0.552632; R: 0.552632; P: 0.561327; F1: 0.552129\n",
      "(valid @ 9): L: 0.794391; A: 0.631579; R: 0.631579; P: 0.654822; F1: 0.633423\n",
      "(train @ 10): L: 0.804340; A: 0.535088; R: 0.535088; P: 0.538635; F1: 0.533696\n",
      "(valid @ 10): L: 0.803896; A: 0.600877; R: 0.600877; P: 0.647992; F1: 0.599487\n",
      "(train @ 11): L: 0.788543; A: 0.532895; R: 0.532895; P: 0.630422; F1: 0.448972\n",
      "(valid @ 11): L: 0.879838; A: 0.473684; R: 0.473684; P: 0.484292; F1: 0.428143\n",
      "(train @ 12): L: 0.807593; A: 0.535088; R: 0.535088; P: 0.543538; F1: 0.490640\n",
      "(valid @ 12): L: 0.824182; A: 0.583333; R: 0.583333; P: 0.689226; F1: 0.556799\n",
      "(train @ 13): L: 0.751462; A: 0.603070; R: 0.603070; P: 0.661442; F1: 0.573235\n",
      "(valid @ 13): L: 1.003709; A: 0.473684; R: 0.473684; P: 0.348560; F1: 0.341422\n",
      "(train @ 14): L: 0.828310; A: 0.519737; R: 0.519737; P: 0.522423; F1: 0.512457\n",
      "(valid @ 14): L: 0.737747; A: 0.640351; R: 0.640351; P: 0.668680; F1: 0.632446\n",
      "(train @ 15): L: 0.762051; A: 0.592105; R: 0.592105; P: 0.597846; F1: 0.589621\n",
      "(valid @ 15): L: 0.781766; A: 0.513158; R: 0.513158; P: 0.353166; F1: 0.383149\n",
      "(train @ 16): L: 0.686180; A: 0.651316; R: 0.651316; P: 0.651387; F1: 0.649297\n",
      "(valid @ 16): L: 0.754012; A: 0.631579; R: 0.631579; P: 0.677311; F1: 0.627305\n",
      "(train @ 17): L: 0.687062; A: 0.651316; R: 0.651316; P: 0.660786; F1: 0.647568\n",
      "(valid @ 17): L: 0.788149; A: 0.657895; R: 0.657895; P: 0.687037; F1: 0.658176\n",
      "(train @ 18): L: 0.672283; A: 0.666667; R: 0.666667; P: 0.699967; F1: 0.652267\n",
      "(valid @ 18): L: 0.724611; A: 0.679825; R: 0.679825; P: 0.705025; F1: 0.681394\n",
      "(train @ 19): L: 0.631397; A: 0.701754; R: 0.701754; P: 0.736014; F1: 0.690287\n",
      "(valid @ 19): L: 0.810541; A: 0.662281; R: 0.662281; P: 0.692329; F1: 0.656775\n",
      "(train @ 20): L: 0.672137; A: 0.677632; R: 0.677632; P: 0.683555; F1: 0.673910\n",
      "(valid @ 20): L: 0.723858; A: 0.675439; R: 0.675439; P: 0.711184; F1: 0.674983\n",
      "(train @ 21): L: 0.685580; A: 0.635965; R: 0.635965; P: 0.666592; F1: 0.626300\n",
      "(valid @ 21): L: 0.662986; A: 0.675439; R: 0.675439; P: 0.710026; F1: 0.671551\n",
      "(train @ 22): L: 0.650168; A: 0.695175; R: 0.695175; P: 0.742795; F1: 0.682272\n",
      "(valid @ 22): L: 0.751966; A: 0.666667; R: 0.666667; P: 0.728580; F1: 0.662624\n",
      "(train @ 23): L: 0.667864; A: 0.684211; R: 0.684211; P: 0.699851; F1: 0.680977\n",
      "(valid @ 23): L: 0.869943; A: 0.614035; R: 0.614035; P: 0.648169; F1: 0.608759\n",
      "(train @ 24): L: 0.615535; A: 0.682018; R: 0.682018; P: 0.679153; F1: 0.679331\n",
      "(valid @ 24): L: 0.657784; A: 0.728070; R: 0.728070; P: 0.780819; F1: 0.725976\n",
      "(train @ 25): L: 0.602441; A: 0.725877; R: 0.725877; P: 0.731187; F1: 0.723622\n",
      "(valid @ 25): L: 0.718239; A: 0.627193; R: 0.627193; P: 0.723114; F1: 0.592877\n",
      "(train @ 26): L: 0.683165; A: 0.642544; R: 0.642544; P: 0.666649; F1: 0.623389\n",
      "(valid @ 26): L: 0.878130; A: 0.517544; R: 0.517544; P: 0.577892; F1: 0.445461\n",
      "(train @ 27): L: 0.633708; A: 0.666667; R: 0.666667; P: 0.670970; F1: 0.662048\n",
      "(valid @ 27): L: 0.728376; A: 0.706140; R: 0.706140; P: 0.769324; F1: 0.702759\n",
      "(train @ 28): L: 0.582063; A: 0.721491; R: 0.721491; P: 0.721309; F1: 0.720208\n",
      "(valid @ 28): L: 0.653379; A: 0.706140; R: 0.706140; P: 0.746642; F1: 0.706341\n",
      "(train @ 29): L: 0.526601; A: 0.741228; R: 0.741228; P: 0.768363; F1: 0.735322\n",
      "(valid @ 29): L: 0.644437; A: 0.697368; R: 0.697368; P: 0.717527; F1: 0.699677\n",
      "(train @ 30): L: 0.501289; A: 0.771930; R: 0.771930; P: 0.787848; F1: 0.768356\n",
      "(valid @ 30): L: 0.580193; A: 0.728070; R: 0.728070; P: 0.736443; F1: 0.728223\n",
      "Best val Metric 0.728223 @ 30\n",
      "\n",
      "models are saved @ ./predictions/211209052552/flvl_2_3_5_6_8_9_pretrained_model_c1.pt\n",
      "Saving predictions @ ./predictions/211209052552/flvl_train_2_3_5_6_8_9_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209052552/flvl_valid_1_4_7_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209052552/flvl_public_test_trained_on_2_3_5_6_8_9_r21d_rgb.csv\n",
      "Average of Best Metrics on Each Valid Set: 0.714174, 211209052552\n",
      "Saved test results @ ./predictions/211209052552/flvl_public_test_agg_r21d_rgb.csv\n"
     ]
    }
   ],
   "source": [
    "# View 0,3\n",
    "from main_2Views import Config, run_kfold\n",
    "\n",
    "# define architecture and run k-fold training\n",
    "cfg = Config()\n",
    "cfg.assign_variable('task', 'flvl')\n",
    "cfg.assign_variable('output_dim', 3)\n",
    "cfg.assign_variable('model_type', 'GRU')\n",
    "cfg.assign_variable('bi_dir', False)\n",
    "cfg.assign_variable('device', 'cuda:0')\n",
    "cfg.assign_variable('data_root', './r21d_rgb_features')\n",
    "cfg.assign_variable('drop_p', 0.0) # results will be irreproducible\n",
    "cfg.assign_variable('batch_size', 64)\n",
    "cfg.assign_variable('input_dim', 512)\n",
    "cfg.assign_variable('hidden_dim', 512)\n",
    "cfg.assign_variable('n_layers', 3)\n",
    "cfg.assign_variable('num_epochs', 30)\n",
    "cfg.assign_variable('seed', 1337)\n",
    "\n",
    "run_kfold(cfg,[0,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': [1, 2, 4, 5, 7, 8], 'valid': [3, 6, 9], 'public_test': [10, 11, 12], 'private_test': [13, 14, 15]}\n",
      "(train @ 1): L: 1.110252; A: 0.489035; R: 0.489035; P: 0.464249; F1: 0.464501\n",
      "(valid @ 1): L: 1.059976; A: 0.469298; R: 0.469298; P: 0.426429; F1: 0.404527\n",
      "(train @ 2): L: 0.988348; A: 0.484649; R: 0.484649; P: 0.407825; F1: 0.442455\n",
      "(valid @ 2): L: 1.043521; A: 0.478070; R: 0.478070; P: 0.504769; F1: 0.468135\n",
      "(train @ 3): L: 0.963728; A: 0.539474; R: 0.539474; P: 0.669642; F1: 0.490385\n",
      "(valid @ 3): L: 0.992552; A: 0.456140; R: 0.456140; P: 0.550423; F1: 0.357774\n",
      "(train @ 4): L: 0.920492; A: 0.530702; R: 0.530702; P: 0.539562; F1: 0.523440\n",
      "(valid @ 4): L: 1.002789; A: 0.486842; R: 0.486842; P: 0.590648; F1: 0.439168\n",
      "(train @ 5): L: 0.848248; A: 0.561404; R: 0.561404; P: 0.593042; F1: 0.553311\n",
      "(valid @ 5): L: 0.894371; A: 0.600877; R: 0.600877; P: 0.648932; F1: 0.599761\n",
      "(train @ 6): L: 0.796224; A: 0.640351; R: 0.640351; P: 0.705559; F1: 0.627513\n",
      "(valid @ 6): L: 1.003293; A: 0.482456; R: 0.482456; P: 0.691966; F1: 0.376530\n",
      "(train @ 7): L: 0.853213; A: 0.532895; R: 0.532895; P: 0.541497; F1: 0.532520\n",
      "(valid @ 7): L: 0.876850; A: 0.605263; R: 0.605263; P: 0.620672; F1: 0.599483\n",
      "(train @ 8): L: 0.733198; A: 0.649123; R: 0.649123; P: 0.649507; F1: 0.649025\n",
      "(valid @ 8): L: 0.886774; A: 0.609649; R: 0.609649; P: 0.629764; F1: 0.596012\n",
      "(train @ 9): L: 0.734946; A: 0.646930; R: 0.646930; P: 0.687453; F1: 0.630802\n",
      "(valid @ 9): L: 0.948915; A: 0.504386; R: 0.504386; P: 0.772350; F1: 0.379001\n",
      "(train @ 10): L: 0.825598; A: 0.550439; R: 0.550439; P: 0.552906; F1: 0.549458\n",
      "(valid @ 10): L: 0.930257; A: 0.561404; R: 0.561404; P: 0.616099; F1: 0.542864\n",
      "(train @ 11): L: 0.745656; A: 0.616228; R: 0.616228; P: 0.634589; F1: 0.612393\n",
      "(valid @ 11): L: 0.858096; A: 0.539474; R: 0.539474; P: 0.335899; F1: 0.402887\n",
      "(train @ 12): L: 0.658793; A: 0.616228; R: 0.616228; P: 0.631179; F1: 0.610449\n",
      "(valid @ 12): L: 0.896005; A: 0.583333; R: 0.583333; P: 0.587590; F1: 0.561799\n",
      "(train @ 13): L: 0.712033; A: 0.655702; R: 0.655702; P: 0.672975; F1: 0.649411\n",
      "(valid @ 13): L: 0.793923; A: 0.631579; R: 0.631579; P: 0.632318; F1: 0.630835\n",
      "(train @ 14): L: 0.615595; A: 0.701754; R: 0.701754; P: 0.737274; F1: 0.693788\n",
      "(valid @ 14): L: 0.857456; A: 0.622807; R: 0.622807; P: 0.626334; F1: 0.611247\n",
      "(train @ 15): L: 0.698343; A: 0.646930; R: 0.646930; P: 0.654177; F1: 0.647086\n",
      "(valid @ 15): L: 0.756809; A: 0.618421; R: 0.618421; P: 0.623488; F1: 0.604565\n",
      "(train @ 16): L: 0.615742; A: 0.697368; R: 0.697368; P: 0.713556; F1: 0.690819\n",
      "(valid @ 16): L: 0.712664; A: 0.657895; R: 0.657895; P: 0.658095; F1: 0.653089\n",
      "(train @ 17): L: 0.594068; A: 0.719298; R: 0.719298; P: 0.723709; F1: 0.718367\n",
      "(valid @ 17): L: 0.717666; A: 0.640351; R: 0.640351; P: 0.648254; F1: 0.629777\n",
      "(train @ 18): L: 0.631827; A: 0.675439; R: 0.675439; P: 0.678642; F1: 0.671847\n",
      "(valid @ 18): L: 0.712678; A: 0.688596; R: 0.688596; P: 0.692090; F1: 0.686018\n",
      "(train @ 19): L: 0.580010; A: 0.734649; R: 0.734649; P: 0.762746; F1: 0.730020\n",
      "(valid @ 19): L: 0.702565; A: 0.627193; R: 0.627193; P: 0.718824; F1: 0.595038\n",
      "(train @ 20): L: 0.580549; A: 0.728070; R: 0.728070; P: 0.728762; F1: 0.728325\n",
      "(valid @ 20): L: 0.662106; A: 0.657895; R: 0.657895; P: 0.664162; F1: 0.652778\n",
      "(train @ 21): L: 0.560817; A: 0.741228; R: 0.741228; P: 0.751971; F1: 0.739178\n",
      "(valid @ 21): L: 0.692759; A: 0.706140; R: 0.706140; P: 0.734777; F1: 0.699682\n",
      "(train @ 22): L: 0.586059; A: 0.730263; R: 0.730263; P: 0.745490; F1: 0.725780\n",
      "(valid @ 22): L: 0.699069; A: 0.714912; R: 0.714912; P: 0.718327; F1: 0.713416\n",
      "(train @ 23): L: 0.530313; A: 0.767544; R: 0.767544; P: 0.782587; F1: 0.766465\n",
      "(valid @ 23): L: 0.729774; A: 0.662281; R: 0.662281; P: 0.667309; F1: 0.656902\n",
      "(train @ 24): L: 0.527549; A: 0.752193; R: 0.752193; P: 0.752623; F1: 0.751867\n",
      "(valid @ 24): L: 0.680365; A: 0.684211; R: 0.684211; P: 0.691794; F1: 0.681455\n",
      "(train @ 25): L: 0.510636; A: 0.756579; R: 0.756579; P: 0.765272; F1: 0.754821\n",
      "(valid @ 25): L: 0.976440; A: 0.574561; R: 0.574561; P: 0.739732; F1: 0.499334\n",
      "(train @ 26): L: 0.701912; A: 0.653509; R: 0.653509; P: 0.660026; F1: 0.646814\n",
      "(valid @ 26): L: 0.860662; A: 0.561404; R: 0.561404; P: 0.583898; F1: 0.518636\n",
      "(train @ 27): L: 0.621619; A: 0.695175; R: 0.695175; P: 0.695337; F1: 0.694898\n",
      "(valid @ 27): L: 0.649979; A: 0.697368; R: 0.697368; P: 0.704422; F1: 0.697127\n",
      "(train @ 28): L: 0.504937; A: 0.758772; R: 0.758772; P: 0.759511; F1: 0.757510\n",
      "(valid @ 28): L: 0.664954; A: 0.662281; R: 0.662281; P: 0.703528; F1: 0.655148\n",
      "(train @ 29): L: 0.482815; A: 0.758772; R: 0.758772; P: 0.784999; F1: 0.756022\n",
      "(valid @ 29): L: 0.725738; A: 0.684211; R: 0.684211; P: 0.705285; F1: 0.683244\n",
      "(train @ 30): L: 0.448792; A: 0.780702; R: 0.780702; P: 0.790397; F1: 0.779742\n",
      "(valid @ 30): L: 0.847269; A: 0.657895; R: 0.657895; P: 0.728711; F1: 0.648854\n",
      "Best val Metric 0.713416 @ 22\n",
      "\n",
      "models are saved @ ./predictions/211209055314/flvl_1_2_4_5_7_8_pretrained_model_c1.pt\n",
      "Saving predictions @ ./predictions/211209055314/flvl_train_1_2_4_5_7_8_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209055314/flvl_valid_3_6_9_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209055314/flvl_public_test_trained_on_1_2_4_5_7_8_r21d_rgb.csv\n",
      "{'train': [1, 3, 4, 6, 7, 9], 'valid': [2, 5, 8], 'public_test': [10, 11, 12], 'private_test': [13, 14, 15]}\n",
      "(train @ 1): L: 1.086675; A: 0.482456; R: 0.482456; P: 0.463971; F1: 0.468851\n",
      "(valid @ 1): L: 0.944939; A: 0.521930; R: 0.521930; P: 0.467107; F1: 0.472239\n",
      "(train @ 2): L: 1.010043; A: 0.460526; R: 0.460526; P: 0.387910; F1: 0.420909\n",
      "(valid @ 2): L: 0.953850; A: 0.649123; R: 0.649123; P: 0.668016; F1: 0.644937\n",
      "(train @ 3): L: 0.998103; A: 0.489035; R: 0.489035; P: 0.585889; F1: 0.458606\n",
      "(valid @ 3): L: 1.069262; A: 0.394737; R: 0.394737; P: 0.170697; F1: 0.238332\n",
      "(train @ 4): L: 0.965237; A: 0.475877; R: 0.475877; P: 0.478655; F1: 0.461085\n",
      "(valid @ 4): L: 0.957280; A: 0.517544; R: 0.517544; P: 0.465473; F1: 0.466386\n",
      "(train @ 5): L: 0.901560; A: 0.497807; R: 0.497807; P: 0.576787; F1: 0.466140\n",
      "(valid @ 5): L: 0.915972; A: 0.495614; R: 0.495614; P: 0.584301; F1: 0.408567\n",
      "(train @ 6): L: 0.875302; A: 0.559211; R: 0.559211; P: 0.589477; F1: 0.547296\n",
      "(valid @ 6): L: 1.068216; A: 0.421053; R: 0.421053; P: 0.177285; F1: 0.249513\n",
      "(train @ 7): L: 0.859951; A: 0.561404; R: 0.561404; P: 0.608905; F1: 0.542328\n",
      "(valid @ 7): L: 0.839236; A: 0.657895; R: 0.657895; P: 0.660654; F1: 0.654710\n",
      "(train @ 8): L: 0.871260; A: 0.559211; R: 0.559211; P: 0.577491; F1: 0.561303\n",
      "(valid @ 8): L: 0.864149; A: 0.583333; R: 0.583333; P: 0.654661; F1: 0.566402\n",
      "(train @ 9): L: 0.810706; A: 0.616228; R: 0.616228; P: 0.643696; F1: 0.603127\n",
      "(valid @ 9): L: 0.849611; A: 0.521930; R: 0.521930; P: 0.776123; F1: 0.415530\n",
      "(train @ 10): L: 0.818433; A: 0.557018; R: 0.557018; P: 0.571528; F1: 0.558423\n",
      "(valid @ 10): L: 0.757416; A: 0.614035; R: 0.614035; P: 0.652632; F1: 0.605128\n",
      "(train @ 11): L: 0.781825; A: 0.578947; R: 0.578947; P: 0.612627; F1: 0.555993\n",
      "(valid @ 11): L: 0.949535; A: 0.517544; R: 0.517544; P: 0.354113; F1: 0.387471\n",
      "(train @ 12): L: 0.769645; A: 0.572368; R: 0.572368; P: 0.574353; F1: 0.563549\n",
      "(valid @ 12): L: 0.804551; A: 0.587719; R: 0.587719; P: 0.653158; F1: 0.576483\n",
      "(train @ 13): L: 0.737605; A: 0.653509; R: 0.653509; P: 0.667341; F1: 0.652015\n",
      "(valid @ 13): L: 0.830117; A: 0.557018; R: 0.557018; P: 0.571716; F1: 0.490475\n",
      "(train @ 14): L: 0.769807; A: 0.589912; R: 0.589912; P: 0.604073; F1: 0.590643\n",
      "(valid @ 14): L: 0.691998; A: 0.710526; R: 0.710526; P: 0.709869; F1: 0.708639\n",
      "(train @ 15): L: 0.759461; A: 0.603070; R: 0.603070; P: 0.604179; F1: 0.599841\n",
      "(valid @ 15): L: 0.682358; A: 0.701754; R: 0.701754; P: 0.721252; F1: 0.703582\n",
      "(train @ 16): L: 0.677752; A: 0.662281; R: 0.662281; P: 0.681882; F1: 0.657633\n",
      "(valid @ 16): L: 0.697375; A: 0.710526; R: 0.710526; P: 0.735365; F1: 0.707556\n",
      "(train @ 17): L: 0.652731; A: 0.725877; R: 0.725877; P: 0.729779; F1: 0.725257\n",
      "(valid @ 17): L: 0.630807; A: 0.758772; R: 0.758772; P: 0.766113; F1: 0.760549\n",
      "(train @ 18): L: 0.660009; A: 0.662281; R: 0.662281; P: 0.678188; F1: 0.657139\n",
      "(valid @ 18): L: 0.605627; A: 0.750000; R: 0.750000; P: 0.755207; F1: 0.748580\n",
      "(train @ 19): L: 0.664406; A: 0.684211; R: 0.684211; P: 0.711476; F1: 0.680330\n",
      "(valid @ 19): L: 0.698731; A: 0.653509; R: 0.653509; P: 0.691799; F1: 0.638997\n",
      "(train @ 20): L: 0.627769; A: 0.703947; R: 0.703947; P: 0.707755; F1: 0.702494\n",
      "(valid @ 20): L: 0.588903; A: 0.754386; R: 0.754386; P: 0.765419; F1: 0.756754\n",
      "(train @ 21): L: 0.617346; A: 0.706140; R: 0.706140; P: 0.715405; F1: 0.705626\n",
      "(valid @ 21): L: 0.582559; A: 0.750000; R: 0.750000; P: 0.762541; F1: 0.745945\n",
      "(train @ 22): L: 0.616953; A: 0.708333; R: 0.708333; P: 0.717339; F1: 0.705909\n",
      "(valid @ 22): L: 0.660444; A: 0.719298; R: 0.719298; P: 0.787327; F1: 0.711581\n",
      "(train @ 23): L: 0.613970; A: 0.692982; R: 0.692982; P: 0.703876; F1: 0.689426\n",
      "(valid @ 23): L: 0.650439; A: 0.706140; R: 0.706140; P: 0.722432; F1: 0.709465\n",
      "(train @ 24): L: 0.611793; A: 0.710526; R: 0.710526; P: 0.710561; F1: 0.709716\n",
      "(valid @ 24): L: 0.540980; A: 0.763158; R: 0.763158; P: 0.780179; F1: 0.764206\n",
      "(train @ 25): L: 0.575542; A: 0.732456; R: 0.732456; P: 0.745852; F1: 0.729937\n",
      "(valid @ 25): L: 0.805407; A: 0.635965; R: 0.635965; P: 0.804763; F1: 0.586312\n",
      "(train @ 26): L: 0.682723; A: 0.664474; R: 0.664474; P: 0.665578; F1: 0.661970\n",
      "(valid @ 26): L: 0.635676; A: 0.679825; R: 0.679825; P: 0.709377; F1: 0.678263\n",
      "(train @ 27): L: 0.604803; A: 0.675439; R: 0.675439; P: 0.677415; F1: 0.674604\n",
      "(valid @ 27): L: 0.533064; A: 0.750000; R: 0.750000; P: 0.762640; F1: 0.752562\n",
      "(train @ 28): L: 0.516672; A: 0.769737; R: 0.769737; P: 0.772125; F1: 0.769350\n",
      "(valid @ 28): L: 0.557831; A: 0.736842; R: 0.736842; P: 0.804450; F1: 0.732287\n",
      "(train @ 29): L: 0.524932; A: 0.725877; R: 0.725877; P: 0.730943; F1: 0.724747\n",
      "(valid @ 29): L: 0.580007; A: 0.750000; R: 0.750000; P: 0.807604; F1: 0.747989\n",
      "(train @ 30): L: 0.483192; A: 0.760965; R: 0.760965; P: 0.761632; F1: 0.760617\n",
      "(valid @ 30): L: 0.535492; A: 0.758772; R: 0.758772; P: 0.815851; F1: 0.748026\n",
      "Best val Metric 0.764206 @ 24\n",
      "\n",
      "models are saved @ ./predictions/211209055314/flvl_1_3_4_6_7_9_pretrained_model_c1.pt\n",
      "Saving predictions @ ./predictions/211209055314/flvl_train_1_3_4_6_7_9_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209055314/flvl_valid_2_5_8_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209055314/flvl_public_test_trained_on_1_3_4_6_7_9_r21d_rgb.csv\n",
      "{'train': [2, 3, 5, 6, 8, 9], 'valid': [1, 4, 7], 'public_test': [10, 11, 12], 'private_test': [13, 14, 15]}\n",
      "(train @ 1): L: 1.106987; A: 0.464912; R: 0.464912; P: 0.440101; F1: 0.447867\n",
      "(valid @ 1): L: 1.039745; A: 0.521930; R: 0.521930; P: 0.475281; F1: 0.467501\n",
      "(train @ 2): L: 1.036227; A: 0.453947; R: 0.453947; P: 0.382360; F1: 0.414850\n",
      "(valid @ 2): L: 0.995288; A: 0.495614; R: 0.495614; P: 0.283151; F1: 0.359466\n",
      "(train @ 3): L: 1.058349; A: 0.427632; R: 0.427632; P: 0.461818; F1: 0.416023\n",
      "(valid @ 3): L: 1.047564; A: 0.399123; R: 0.399123; P: 0.172594; F1: 0.240980\n",
      "(train @ 4): L: 0.989604; A: 0.449561; R: 0.449561; P: 0.536215; F1: 0.410581\n",
      "(valid @ 4): L: 0.968740; A: 0.500000; R: 0.500000; P: 0.611572; F1: 0.446380\n",
      "(train @ 5): L: 0.928621; A: 0.482456; R: 0.482456; P: 0.566908; F1: 0.449042\n",
      "(valid @ 5): L: 0.963643; A: 0.425439; R: 0.425439; P: 0.335961; F1: 0.258820\n",
      "(train @ 6): L: 0.904688; A: 0.478070; R: 0.478070; P: 0.486099; F1: 0.477415\n",
      "(valid @ 6): L: 0.938635; A: 0.583333; R: 0.583333; P: 0.676066; F1: 0.540224\n",
      "(train @ 7): L: 0.863175; A: 0.486842; R: 0.486842; P: 0.538705; F1: 0.418546\n",
      "(valid @ 7): L: 0.911523; A: 0.614035; R: 0.614035; P: 0.631104; F1: 0.603507\n",
      "(train @ 8): L: 0.902418; A: 0.567982; R: 0.567982; P: 0.662058; F1: 0.541308\n",
      "(valid @ 8): L: 0.939829; A: 0.460526; R: 0.460526; P: 0.342466; F1: 0.319799\n",
      "(train @ 9): L: 0.811017; A: 0.550439; R: 0.550439; P: 0.556376; F1: 0.549557\n",
      "(valid @ 9): L: 0.809007; A: 0.596491; R: 0.596491; P: 0.644665; F1: 0.594090\n",
      "(train @ 10): L: 0.801896; A: 0.526316; R: 0.526316; P: 0.538322; F1: 0.508882\n",
      "(valid @ 10): L: 0.800597; A: 0.596491; R: 0.596491; P: 0.647315; F1: 0.592256\n",
      "(train @ 11): L: 0.789620; A: 0.543860; R: 0.543860; P: 0.695379; F1: 0.449842\n",
      "(valid @ 11): L: 0.889015; A: 0.482456; R: 0.482456; P: 0.486481; F1: 0.436939\n",
      "(train @ 12): L: 0.802696; A: 0.543860; R: 0.543860; P: 0.558013; F1: 0.506002\n",
      "(valid @ 12): L: 0.821903; A: 0.570175; R: 0.570175; P: 0.656619; F1: 0.550431\n",
      "(train @ 13): L: 0.746743; A: 0.614035; R: 0.614035; P: 0.667670; F1: 0.582943\n",
      "(valid @ 13): L: 1.003721; A: 0.478070; R: 0.478070; P: 0.346778; F1: 0.344562\n",
      "(train @ 14): L: 0.815883; A: 0.517544; R: 0.517544; P: 0.518293; F1: 0.505282\n",
      "(valid @ 14): L: 0.745180; A: 0.649123; R: 0.649123; P: 0.680083; F1: 0.644269\n",
      "(train @ 15): L: 0.747780; A: 0.616228; R: 0.616228; P: 0.633972; F1: 0.609615\n",
      "(valid @ 15): L: 0.795053; A: 0.552632; R: 0.552632; P: 0.783094; F1: 0.459533\n",
      "(train @ 16): L: 0.678371; A: 0.618421; R: 0.618421; P: 0.618814; F1: 0.616676\n",
      "(valid @ 16): L: 0.744653; A: 0.627193; R: 0.627193; P: 0.667133; F1: 0.621306\n",
      "(train @ 17): L: 0.709175; A: 0.631579; R: 0.631579; P: 0.657530; F1: 0.621189\n",
      "(valid @ 17): L: 0.804774; A: 0.706140; R: 0.706140; P: 0.752924; F1: 0.695238\n",
      "(train @ 18): L: 0.670830; A: 0.664474; R: 0.664474; P: 0.697501; F1: 0.653209\n",
      "(valid @ 18): L: 0.746081; A: 0.653509; R: 0.653509; P: 0.684698; F1: 0.655519\n",
      "(train @ 19): L: 0.636832; A: 0.688596; R: 0.688596; P: 0.719760; F1: 0.679134\n",
      "(valid @ 19): L: 0.817687; A: 0.697368; R: 0.697368; P: 0.723199; F1: 0.693302\n",
      "(train @ 20): L: 0.687914; A: 0.679825; R: 0.679825; P: 0.688033; F1: 0.678308\n",
      "(valid @ 20): L: 0.671730; A: 0.692982; R: 0.692982; P: 0.714506; F1: 0.695402\n",
      "(train @ 21): L: 0.667783; A: 0.646930; R: 0.646930; P: 0.678177; F1: 0.639554\n",
      "(valid @ 21): L: 0.645894; A: 0.714912; R: 0.714912; P: 0.755505; F1: 0.713238\n",
      "(train @ 22): L: 0.656591; A: 0.673246; R: 0.673246; P: 0.724839; F1: 0.655652\n",
      "(valid @ 22): L: 0.788000; A: 0.671053; R: 0.671053; P: 0.698806; F1: 0.674183\n",
      "(train @ 23): L: 0.663645; A: 0.673246; R: 0.673246; P: 0.693168; F1: 0.669004\n",
      "(valid @ 23): L: 0.813226; A: 0.631579; R: 0.631579; P: 0.663537; F1: 0.627811\n",
      "(train @ 24): L: 0.624678; A: 0.679825; R: 0.679825; P: 0.679253; F1: 0.679055\n",
      "(valid @ 24): L: 0.619330; A: 0.719298; R: 0.719298; P: 0.743641; F1: 0.719394\n",
      "(train @ 25): L: 0.617897; A: 0.706140; R: 0.706140; P: 0.708765; F1: 0.705770\n",
      "(valid @ 25): L: 0.677850; A: 0.640351; R: 0.640351; P: 0.720858; F1: 0.618525\n",
      "(train @ 26): L: 0.666829; A: 0.660088; R: 0.660088; P: 0.675582; F1: 0.647384\n",
      "(valid @ 26): L: 0.879047; A: 0.552632; R: 0.552632; P: 0.686017; F1: 0.480824\n",
      "(train @ 27): L: 0.676395; A: 0.644737; R: 0.644737; P: 0.642766; F1: 0.643350\n",
      "(valid @ 27): L: 0.633148; A: 0.697368; R: 0.697368; P: 0.732905; F1: 0.697779\n",
      "(train @ 28): L: 0.554735; A: 0.741228; R: 0.741228; P: 0.756188; F1: 0.741051\n",
      "(valid @ 28): L: 0.657690; A: 0.697368; R: 0.697368; P: 0.743863; F1: 0.696367\n",
      "(train @ 29): L: 0.564151; A: 0.721491; R: 0.721491; P: 0.738098; F1: 0.718134\n",
      "(valid @ 29): L: 0.710909; A: 0.714912; R: 0.714912; P: 0.741105; F1: 0.714742\n",
      "(train @ 30): L: 0.512629; A: 0.769737; R: 0.769737; P: 0.777743; F1: 0.767504\n",
      "(valid @ 30): L: 0.557810; A: 0.745614; R: 0.745614; P: 0.766375; F1: 0.747360\n",
      "Best val Metric 0.747360 @ 30\n",
      "\n",
      "models are saved @ ./predictions/211209055314/flvl_2_3_5_6_8_9_pretrained_model_c1.pt\n",
      "Saving predictions @ ./predictions/211209055314/flvl_train_2_3_5_6_8_9_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209055314/flvl_valid_1_4_7_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209055314/flvl_public_test_trained_on_2_3_5_6_8_9_r21d_rgb.csv\n",
      "Average of Best Metrics on Each Valid Set: 0.741661, 211209055314\n",
      "Saved test results @ ./predictions/211209055314/flvl_public_test_agg_r21d_rgb.csv\n"
     ]
    }
   ],
   "source": [
    "# View 1,2\n",
    "from main_2Views import Config, run_kfold\n",
    "\n",
    "# define architecture and run k-fold training\n",
    "cfg = Config()\n",
    "cfg.assign_variable('task', 'flvl')\n",
    "cfg.assign_variable('output_dim', 3)\n",
    "cfg.assign_variable('model_type', 'GRU')\n",
    "cfg.assign_variable('bi_dir', False)\n",
    "cfg.assign_variable('device', 'cuda:0')\n",
    "cfg.assign_variable('data_root', './r21d_rgb_features')\n",
    "cfg.assign_variable('drop_p', 0.0) # results will be irreproducible\n",
    "cfg.assign_variable('batch_size', 64)\n",
    "cfg.assign_variable('input_dim', 512)\n",
    "cfg.assign_variable('hidden_dim', 512)\n",
    "cfg.assign_variable('n_layers', 3)\n",
    "cfg.assign_variable('num_epochs', 30)\n",
    "cfg.assign_variable('seed', 1337)\n",
    "\n",
    "run_kfold(cfg,[1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': [1, 2, 4, 5, 7, 8], 'valid': [3, 6, 9], 'public_test': [10, 11, 12], 'private_test': [13, 14, 15]}\n",
      "(train @ 1): L: 1.133573; A: 0.432018; R: 0.432018; P: 0.415964; F1: 0.418686\n",
      "(valid @ 1): L: 1.120119; A: 0.456140; R: 0.456140; P: 0.411598; F1: 0.387561\n",
      "(train @ 2): L: 1.020206; A: 0.467105; R: 0.467105; P: 0.393420; F1: 0.427057\n",
      "(valid @ 2): L: 1.058852; A: 0.403509; R: 0.403509; P: 0.553973; F1: 0.335972\n",
      "(train @ 3): L: 0.967067; A: 0.489035; R: 0.489035; P: 0.571939; F1: 0.466138\n",
      "(valid @ 3): L: 1.007116; A: 0.464912; R: 0.464912; P: 0.383573; F1: 0.417821\n",
      "(train @ 4): L: 0.940093; A: 0.510965; R: 0.510965; P: 0.509579; F1: 0.505763\n",
      "(valid @ 4): L: 1.008855; A: 0.482456; R: 0.482456; P: 0.589557; F1: 0.430592\n",
      "(train @ 5): L: 0.866718; A: 0.524123; R: 0.524123; P: 0.615182; F1: 0.485499\n",
      "(valid @ 5): L: 0.937370; A: 0.491228; R: 0.491228; P: 0.441941; F1: 0.408799\n",
      "(train @ 6): L: 0.829808; A: 0.574561; R: 0.574561; P: 0.598661; F1: 0.570844\n",
      "(valid @ 6): L: 0.948295; A: 0.482456; R: 0.482456; P: 0.539348; F1: 0.446477\n",
      "(train @ 7): L: 0.845010; A: 0.526316; R: 0.526316; P: 0.523467; F1: 0.514122\n",
      "(valid @ 7): L: 0.917031; A: 0.552632; R: 0.552632; P: 0.624297; F1: 0.535019\n",
      "(train @ 8): L: 0.738892; A: 0.651316; R: 0.651316; P: 0.673402; F1: 0.648906\n",
      "(valid @ 8): L: 0.884696; A: 0.635965; R: 0.635965; P: 0.636542; F1: 0.634671\n",
      "(train @ 9): L: 0.766571; A: 0.620614; R: 0.620614; P: 0.658881; F1: 0.601074\n",
      "(valid @ 9): L: 0.912721; A: 0.504386; R: 0.504386; P: 0.351297; F1: 0.374147\n",
      "(train @ 10): L: 0.813733; A: 0.546053; R: 0.546053; P: 0.558527; F1: 0.535353\n",
      "(valid @ 10): L: 0.946823; A: 0.517544; R: 0.517544; P: 0.596491; F1: 0.488515\n",
      "(train @ 11): L: 0.797068; A: 0.609649; R: 0.609649; P: 0.688195; F1: 0.587838\n",
      "(valid @ 11): L: 0.923461; A: 0.521930; R: 0.521930; P: 0.312758; F1: 0.386166\n",
      "(train @ 12): L: 0.722790; A: 0.583333; R: 0.583333; P: 0.605776; F1: 0.551342\n",
      "(valid @ 12): L: 0.923268; A: 0.578947; R: 0.578947; P: 0.586508; F1: 0.556192\n",
      "(train @ 13): L: 0.750640; A: 0.642544; R: 0.642544; P: 0.691936; F1: 0.628732\n",
      "(valid @ 13): L: 0.853966; A: 0.548246; R: 0.548246; P: 0.572363; F1: 0.465716\n",
      "(train @ 14): L: 0.676784; A: 0.618421; R: 0.618421; P: 0.615518; F1: 0.615874\n",
      "(valid @ 14): L: 0.880051; A: 0.600877; R: 0.600877; P: 0.602300; F1: 0.580656\n",
      "(train @ 15): L: 0.716515; A: 0.627193; R: 0.627193; P: 0.634584; F1: 0.627351\n",
      "(valid @ 15): L: 0.773027; A: 0.662281; R: 0.662281; P: 0.662427; F1: 0.657490\n",
      "(train @ 16): L: 0.631171; A: 0.699561; R: 0.699561; P: 0.727109; F1: 0.692431\n",
      "(valid @ 16): L: 0.745202; A: 0.640351; R: 0.640351; P: 0.642514; F1: 0.631636\n",
      "(train @ 17): L: 0.632241; A: 0.668860; R: 0.668860; P: 0.670680; F1: 0.667262\n",
      "(valid @ 17): L: 0.748911; A: 0.644737; R: 0.644737; P: 0.646816; F1: 0.636643\n",
      "(train @ 18): L: 0.688393; A: 0.625000; R: 0.625000; P: 0.645099; F1: 0.614790\n",
      "(valid @ 18): L: 0.753175; A: 0.675439; R: 0.675439; P: 0.674328; F1: 0.672630\n",
      "(train @ 19): L: 0.631483; A: 0.719298; R: 0.719298; P: 0.756728; F1: 0.714056\n",
      "(valid @ 19): L: 0.743376; A: 0.640351; R: 0.640351; P: 0.650028; F1: 0.641634\n",
      "(train @ 20): L: 0.615647; A: 0.732456; R: 0.732456; P: 0.750625; F1: 0.730518\n",
      "(valid @ 20): L: 0.728674; A: 0.657895; R: 0.657895; P: 0.664162; F1: 0.655160\n",
      "(train @ 21): L: 0.590206; A: 0.692982; R: 0.692982; P: 0.718400; F1: 0.690309\n",
      "(valid @ 21): L: 0.714020; A: 0.675439; R: 0.675439; P: 0.682187; F1: 0.671186\n",
      "(train @ 22): L: 0.596652; A: 0.697368; R: 0.697368; P: 0.715315; F1: 0.691263\n",
      "(valid @ 22): L: 0.728313; A: 0.675439; R: 0.675439; P: 0.677778; F1: 0.673605\n",
      "(train @ 23): L: 0.566674; A: 0.723684; R: 0.723684; P: 0.744946; F1: 0.720782\n",
      "(valid @ 23): L: 0.746262; A: 0.657895; R: 0.657895; P: 0.657520; F1: 0.653737\n",
      "(train @ 24): L: 0.516098; A: 0.758772; R: 0.758772; P: 0.762885; F1: 0.755970\n",
      "(valid @ 24): L: 0.740363; A: 0.662281; R: 0.662281; P: 0.664786; F1: 0.657950\n",
      "(train @ 25): L: 0.500360; A: 0.782895; R: 0.782895; P: 0.788429; F1: 0.781214\n",
      "(valid @ 25): L: 0.882471; A: 0.635965; R: 0.635965; P: 0.726255; F1: 0.595404\n",
      "(train @ 26): L: 0.628677; A: 0.668860; R: 0.668860; P: 0.677104; F1: 0.670311\n",
      "(valid @ 26): L: 0.798518; A: 0.631579; R: 0.631579; P: 0.640304; F1: 0.622705\n",
      "(train @ 27): L: 0.535390; A: 0.730263; R: 0.730263; P: 0.732694; F1: 0.730569\n",
      "(valid @ 27): L: 0.731802; A: 0.627193; R: 0.627193; P: 0.625168; F1: 0.622546\n",
      "(train @ 28): L: 0.483879; A: 0.793860; R: 0.793860; P: 0.798842; F1: 0.792759\n",
      "(valid @ 28): L: 0.718357; A: 0.657895; R: 0.657895; P: 0.658660; F1: 0.657323\n",
      "(train @ 29): L: 0.455367; A: 0.785088; R: 0.785088; P: 0.794056; F1: 0.784314\n",
      "(valid @ 29): L: 0.788579; A: 0.657895; R: 0.657895; P: 0.673802; F1: 0.656927\n",
      "(train @ 30): L: 0.417311; A: 0.811404; R: 0.811404; P: 0.819544; F1: 0.811041\n",
      "(valid @ 30): L: 1.038919; A: 0.627193; R: 0.627193; P: 0.698084; F1: 0.614744\n",
      "Best val Metric 0.673605 @ 22\n",
      "\n",
      "models are saved @ ./predictions/211209062040/flvl_1_2_4_5_7_8_pretrained_model_c1.pt\n",
      "Saving predictions @ ./predictions/211209062040/flvl_train_1_2_4_5_7_8_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209062040/flvl_valid_3_6_9_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209062040/flvl_public_test_trained_on_1_2_4_5_7_8_r21d_rgb.csv\n",
      "{'train': [1, 3, 4, 6, 7, 9], 'valid': [2, 5, 8], 'public_test': [10, 11, 12], 'private_test': [13, 14, 15]}\n",
      "(train @ 1): L: 1.095110; A: 0.489035; R: 0.489035; P: 0.464910; F1: 0.467387\n",
      "(valid @ 1): L: 0.971168; A: 0.517544; R: 0.517544; P: 0.471679; F1: 0.462520\n",
      "(train @ 2): L: 1.017956; A: 0.460526; R: 0.460526; P: 0.387910; F1: 0.420909\n",
      "(valid @ 2): L: 0.966536; A: 0.614035; R: 0.614035; P: 0.612407; F1: 0.596829\n",
      "(train @ 3): L: 1.008564; A: 0.469298; R: 0.469298; P: 0.558502; F1: 0.447577\n",
      "(valid @ 3): L: 1.083201; A: 0.407895; R: 0.407895; P: 0.176387; F1: 0.246276\n",
      "(train @ 4): L: 0.966331; A: 0.495614; R: 0.495614; P: 0.498510; F1: 0.481627\n",
      "(valid @ 4): L: 0.956561; A: 0.521930; R: 0.521930; P: 0.475281; F1: 0.467501\n",
      "(train @ 5): L: 0.910346; A: 0.491228; R: 0.491228; P: 0.586431; F1: 0.444356\n",
      "(valid @ 5): L: 0.956694; A: 0.421053; R: 0.421053; P: 0.177285; F1: 0.249513\n",
      "(train @ 6): L: 0.897365; A: 0.517544; R: 0.517544; P: 0.517466; F1: 0.514919\n",
      "(valid @ 6): L: 0.933979; A: 0.486842; R: 0.486842; P: 0.538353; F1: 0.381990\n",
      "(train @ 7): L: 0.852243; A: 0.517544; R: 0.517544; P: 0.574828; F1: 0.483479\n",
      "(valid @ 7): L: 0.843340; A: 0.631579; R: 0.631579; P: 0.650346; F1: 0.622666\n",
      "(train @ 8): L: 0.859089; A: 0.581140; R: 0.581140; P: 0.615021; F1: 0.578491\n",
      "(valid @ 8): L: 0.857293; A: 0.618421; R: 0.618421; P: 0.656728; F1: 0.614561\n",
      "(train @ 9): L: 0.800959; A: 0.587719; R: 0.587719; P: 0.630348; F1: 0.556167\n",
      "(valid @ 9): L: 0.830983; A: 0.508772; R: 0.508772; P: 0.352227; F1: 0.378710\n",
      "(train @ 10): L: 0.806444; A: 0.530702; R: 0.530702; P: 0.544667; F1: 0.522846\n",
      "(valid @ 10): L: 0.797383; A: 0.614035; R: 0.614035; P: 0.654327; F1: 0.603542\n",
      "(train @ 11): L: 0.800640; A: 0.594298; R: 0.594298; P: 0.694013; F1: 0.550717\n",
      "(valid @ 11): L: 0.936192; A: 0.526316; R: 0.526316; P: 0.356037; F1: 0.395789\n",
      "(train @ 12): L: 0.787105; A: 0.554825; R: 0.554825; P: 0.574120; F1: 0.522711\n",
      "(valid @ 12): L: 0.816372; A: 0.592105; R: 0.592105; P: 0.651865; F1: 0.581155\n",
      "(train @ 13): L: 0.759948; A: 0.614035; R: 0.614035; P: 0.671484; F1: 0.591097\n",
      "(valid @ 13): L: 0.981893; A: 0.521930; R: 0.521930; P: 0.356037; F1: 0.392578\n",
      "(train @ 14): L: 0.824157; A: 0.535088; R: 0.535088; P: 0.532431; F1: 0.527824\n",
      "(valid @ 14): L: 0.709137; A: 0.649123; R: 0.649123; P: 0.653719; F1: 0.638137\n",
      "(train @ 15): L: 0.758088; A: 0.618421; R: 0.618421; P: 0.622274; F1: 0.617910\n",
      "(valid @ 15): L: 0.719778; A: 0.552632; R: 0.552632; P: 0.762244; F1: 0.437642\n",
      "(train @ 16): L: 0.697437; A: 0.660088; R: 0.660088; P: 0.667748; F1: 0.656521\n",
      "(valid @ 16): L: 0.704040; A: 0.706140; R: 0.706140; P: 0.731894; F1: 0.707144\n",
      "(train @ 17): L: 0.696308; A: 0.631579; R: 0.631579; P: 0.630542; F1: 0.628741\n",
      "(valid @ 17): L: 0.678735; A: 0.692982; R: 0.692982; P: 0.714377; F1: 0.692508\n",
      "(train @ 18): L: 0.693989; A: 0.649123; R: 0.649123; P: 0.690154; F1: 0.631843\n",
      "(valid @ 18): L: 0.643523; A: 0.719298; R: 0.719298; P: 0.727616; F1: 0.716921\n",
      "(train @ 19): L: 0.665081; A: 0.695175; R: 0.695175; P: 0.716802; F1: 0.690489\n",
      "(valid @ 19): L: 0.700297; A: 0.684211; R: 0.684211; P: 0.709029; F1: 0.682105\n",
      "(train @ 20): L: 0.648123; A: 0.671053; R: 0.671053; P: 0.687114; F1: 0.665058\n",
      "(valid @ 20): L: 0.625462; A: 0.714912; R: 0.714912; P: 0.720227; F1: 0.715435\n",
      "(train @ 21): L: 0.652114; A: 0.673246; R: 0.673246; P: 0.702034; F1: 0.666502\n",
      "(valid @ 21): L: 0.643072; A: 0.701754; R: 0.701754; P: 0.699676; F1: 0.700138\n",
      "(train @ 22): L: 0.636970; A: 0.692982; R: 0.692982; P: 0.713046; F1: 0.688676\n",
      "(valid @ 22): L: 0.712291; A: 0.688596; R: 0.688596; P: 0.724812; F1: 0.685689\n",
      "(train @ 23): L: 0.652888; A: 0.666667; R: 0.666667; P: 0.676949; F1: 0.663375\n",
      "(valid @ 23): L: 0.661868; A: 0.719298; R: 0.719298; P: 0.742326; F1: 0.720384\n",
      "(train @ 24): L: 0.601787; A: 0.712719; R: 0.712719; P: 0.712391; F1: 0.709071\n",
      "(valid @ 24): L: 0.607096; A: 0.719298; R: 0.719298; P: 0.730930; F1: 0.719750\n",
      "(train @ 25): L: 0.603723; A: 0.717105; R: 0.717105; P: 0.721810; F1: 0.715114\n",
      "(valid @ 25): L: 0.797685; A: 0.618421; R: 0.618421; P: 0.779157; F1: 0.561599\n",
      "(train @ 26): L: 0.678848; A: 0.653509; R: 0.653509; P: 0.664211; F1: 0.648665\n",
      "(valid @ 26): L: 0.756727; A: 0.539474; R: 0.539474; P: 0.572017; F1: 0.502029\n",
      "(train @ 27): L: 0.652362; A: 0.657895; R: 0.657895; P: 0.661379; F1: 0.658115\n",
      "(valid @ 27): L: 0.584093; A: 0.736842; R: 0.736842; P: 0.759628; F1: 0.736028\n",
      "(train @ 28): L: 0.558092; A: 0.747807; R: 0.747807; P: 0.749228; F1: 0.745135\n",
      "(valid @ 28): L: 0.606535; A: 0.679825; R: 0.679825; P: 0.753469; F1: 0.666138\n",
      "(train @ 29): L: 0.553413; A: 0.719298; R: 0.719298; P: 0.743417; F1: 0.713938\n",
      "(valid @ 29): L: 0.553670; A: 0.736842; R: 0.736842; P: 0.739787; F1: 0.736656\n",
      "(train @ 30): L: 0.530890; A: 0.752193; R: 0.752193; P: 0.768072; F1: 0.749588\n",
      "(valid @ 30): L: 0.655749; A: 0.692982; R: 0.692982; P: 0.736403; F1: 0.682236\n",
      "Best val Metric 0.736656 @ 29\n",
      "\n",
      "models are saved @ ./predictions/211209062040/flvl_1_3_4_6_7_9_pretrained_model_c1.pt\n",
      "Saving predictions @ ./predictions/211209062040/flvl_train_1_3_4_6_7_9_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209062040/flvl_valid_2_5_8_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209062040/flvl_public_test_trained_on_1_3_4_6_7_9_r21d_rgb.csv\n",
      "{'train': [2, 3, 5, 6, 8, 9], 'valid': [1, 4, 7], 'public_test': [10, 11, 12], 'private_test': [13, 14, 15]}\n",
      "(train @ 1): L: 1.128388; A: 0.471491; R: 0.471491; P: 0.455606; F1: 0.452409\n",
      "(valid @ 1): L: 1.046506; A: 0.517544; R: 0.517544; P: 0.471679; F1: 0.462520\n",
      "(train @ 2): L: 1.057001; A: 0.456140; R: 0.456140; P: 0.384434; F1: 0.416837\n",
      "(valid @ 2): L: 1.026690; A: 0.504386; R: 0.504386; P: 0.294773; F1: 0.365263\n",
      "(train @ 3): L: 1.055913; A: 0.436404; R: 0.436404; P: 0.478032; F1: 0.432502\n",
      "(valid @ 3): L: 1.059395; A: 0.565789; R: 0.565789; P: 0.497858; F1: 0.518115\n",
      "(train @ 4): L: 0.991405; A: 0.469298; R: 0.469298; P: 0.546670; F1: 0.422706\n",
      "(valid @ 4): L: 0.953933; A: 0.557018; R: 0.557018; P: 0.645535; F1: 0.530816\n",
      "(train @ 5): L: 0.938491; A: 0.526316; R: 0.526316; P: 0.656914; F1: 0.470888\n",
      "(valid @ 5): L: 0.987651; A: 0.421053; R: 0.421053; P: 0.180451; F1: 0.252632\n",
      "(train @ 6): L: 0.931040; A: 0.456140; R: 0.456140; P: 0.461241; F1: 0.450629\n",
      "(valid @ 6): L: 0.892527; A: 0.574561; R: 0.574561; P: 0.642557; F1: 0.539762\n",
      "(train @ 7): L: 0.891528; A: 0.489035; R: 0.489035; P: 0.548009; F1: 0.422056\n",
      "(valid @ 7): L: 0.880849; A: 0.600877; R: 0.600877; P: 0.657237; F1: 0.591348\n",
      "(train @ 8): L: 0.886654; A: 0.574561; R: 0.574561; P: 0.666036; F1: 0.539592\n",
      "(valid @ 8): L: 0.913883; A: 0.539474; R: 0.539474; P: 0.675926; F1: 0.474939\n",
      "(train @ 9): L: 0.818754; A: 0.535088; R: 0.535088; P: 0.541962; F1: 0.536288\n",
      "(valid @ 9): L: 0.798346; A: 0.644737; R: 0.644737; P: 0.667307; F1: 0.646631\n",
      "(train @ 10): L: 0.812640; A: 0.515351; R: 0.515351; P: 0.516795; F1: 0.508679\n",
      "(valid @ 10): L: 0.801882; A: 0.605263; R: 0.605263; P: 0.663949; F1: 0.597930\n",
      "(train @ 11): L: 0.799298; A: 0.521930; R: 0.521930; P: 0.615272; F1: 0.442586\n",
      "(valid @ 11): L: 0.906046; A: 0.460526; R: 0.460526; P: 0.473069; F1: 0.417896\n",
      "(train @ 12): L: 0.821935; A: 0.537281; R: 0.537281; P: 0.541533; F1: 0.502825\n",
      "(valid @ 12): L: 0.826445; A: 0.561404; R: 0.561404; P: 0.683527; F1: 0.525706\n",
      "(train @ 13): L: 0.762117; A: 0.607456; R: 0.607456; P: 0.663663; F1: 0.579414\n",
      "(valid @ 13): L: 0.970840; A: 0.482456; R: 0.482456; P: 0.349464; F1: 0.351750\n",
      "(train @ 14): L: 0.829623; A: 0.508772; R: 0.508772; P: 0.510838; F1: 0.501566\n",
      "(valid @ 14): L: 0.731513; A: 0.649123; R: 0.649123; P: 0.678553; F1: 0.643588\n",
      "(train @ 15): L: 0.760219; A: 0.607456; R: 0.607456; P: 0.618656; F1: 0.606098\n",
      "(valid @ 15): L: 0.768082; A: 0.521930; R: 0.521930; P: 0.355071; F1: 0.391683\n",
      "(train @ 16): L: 0.689542; A: 0.622807; R: 0.622807; P: 0.620997; F1: 0.621531\n",
      "(valid @ 16): L: 0.749001; A: 0.635965; R: 0.635965; P: 0.680885; F1: 0.630963\n",
      "(train @ 17): L: 0.698836; A: 0.607456; R: 0.607456; P: 0.620143; F1: 0.598790\n",
      "(valid @ 17): L: 0.779466; A: 0.671053; R: 0.671053; P: 0.705156; F1: 0.664500\n",
      "(train @ 18): L: 0.674821; A: 0.657895; R: 0.657895; P: 0.689429; F1: 0.644686\n",
      "(valid @ 18): L: 0.720473; A: 0.671053; R: 0.671053; P: 0.697941; F1: 0.673430\n",
      "(train @ 19): L: 0.641770; A: 0.701754; R: 0.701754; P: 0.724914; F1: 0.695219\n",
      "(valid @ 19): L: 0.822758; A: 0.631579; R: 0.631579; P: 0.666631; F1: 0.621472\n",
      "(train @ 20): L: 0.678265; A: 0.666667; R: 0.666667; P: 0.677932; F1: 0.661953\n",
      "(valid @ 20): L: 0.711708; A: 0.688596; R: 0.688596; P: 0.715340; F1: 0.690182\n",
      "(train @ 21): L: 0.673301; A: 0.651316; R: 0.651316; P: 0.682707; F1: 0.643972\n",
      "(valid @ 21): L: 0.649397; A: 0.706140; R: 0.706140; P: 0.723687; F1: 0.708285\n",
      "(train @ 22): L: 0.650626; A: 0.699561; R: 0.699561; P: 0.742727; F1: 0.689451\n",
      "(valid @ 22): L: 0.745319; A: 0.671053; R: 0.671053; P: 0.733911; F1: 0.666228\n",
      "(train @ 23): L: 0.658949; A: 0.664474; R: 0.664474; P: 0.676989; F1: 0.662374\n",
      "(valid @ 23): L: 0.832302; A: 0.605263; R: 0.605263; P: 0.639654; F1: 0.599534\n",
      "(train @ 24): L: 0.621717; A: 0.690789; R: 0.690789; P: 0.692568; F1: 0.687978\n",
      "(valid @ 24): L: 0.650647; A: 0.701754; R: 0.701754; P: 0.730666; F1: 0.703209\n",
      "(train @ 25): L: 0.595813; A: 0.712719; R: 0.712719; P: 0.715784; F1: 0.710229\n",
      "(valid @ 25): L: 0.716276; A: 0.640351; R: 0.640351; P: 0.738851; F1: 0.616184\n",
      "(train @ 26): L: 0.665365; A: 0.666667; R: 0.666667; P: 0.676962; F1: 0.660252\n",
      "(valid @ 26): L: 0.754060; A: 0.574561; R: 0.574561; P: 0.606471; F1: 0.559625\n",
      "(train @ 27): L: 0.605297; A: 0.688596; R: 0.688596; P: 0.687185; F1: 0.687547\n",
      "(valid @ 27): L: 0.687404; A: 0.706140; R: 0.706140; P: 0.736714; F1: 0.707806\n",
      "(train @ 28): L: 0.562641; A: 0.721491; R: 0.721491; P: 0.721557; F1: 0.720407\n",
      "(valid @ 28): L: 0.713543; A: 0.697368; R: 0.697368; P: 0.734339; F1: 0.697227\n",
      "(train @ 29): L: 0.550835; A: 0.734649; R: 0.734649; P: 0.741651; F1: 0.730271\n",
      "(valid @ 29): L: 0.707636; A: 0.706140; R: 0.706140; P: 0.726964; F1: 0.708191\n",
      "(train @ 30): L: 0.489401; A: 0.778509; R: 0.778509; P: 0.789350; F1: 0.777034\n",
      "(valid @ 30): L: 0.608315; A: 0.692982; R: 0.692982; P: 0.697605; F1: 0.693800\n",
      "Best val Metric 0.708285 @ 21\n",
      "\n",
      "models are saved @ ./predictions/211209062040/flvl_2_3_5_6_8_9_pretrained_model_c1.pt\n",
      "Saving predictions @ ./predictions/211209062040/flvl_train_2_3_5_6_8_9_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209062040/flvl_valid_1_4_7_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209062040/flvl_public_test_trained_on_2_3_5_6_8_9_r21d_rgb.csv\n",
      "Average of Best Metrics on Each Valid Set: 0.706182, 211209062040\n",
      "Saved test results @ ./predictions/211209062040/flvl_public_test_agg_r21d_rgb.csv\n"
     ]
    }
   ],
   "source": [
    "# View 1,3\n",
    "from main_2Views import Config, run_kfold\n",
    "\n",
    "# define architecture and run k-fold training\n",
    "cfg = Config()\n",
    "cfg.assign_variable('task', 'flvl')\n",
    "cfg.assign_variable('output_dim', 3)\n",
    "cfg.assign_variable('model_type', 'GRU')\n",
    "cfg.assign_variable('bi_dir', False)\n",
    "cfg.assign_variable('device', 'cuda:0')\n",
    "cfg.assign_variable('data_root', './r21d_rgb_features')\n",
    "cfg.assign_variable('drop_p', 0.0) # results will be irreproducible\n",
    "cfg.assign_variable('batch_size', 64)\n",
    "cfg.assign_variable('input_dim', 512)\n",
    "cfg.assign_variable('hidden_dim', 512)\n",
    "cfg.assign_variable('n_layers', 3)\n",
    "cfg.assign_variable('num_epochs', 30)\n",
    "cfg.assign_variable('seed', 1337)\n",
    "\n",
    "run_kfold(cfg,[1,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': [1, 2, 4, 5, 7, 8], 'valid': [3, 6, 9], 'public_test': [10, 11, 12], 'private_test': [13, 14, 15]}\n",
      "(train @ 1): L: 1.124163; A: 0.458333; R: 0.458333; P: 0.428207; F1: 0.437066\n",
      "(valid @ 1): L: 1.116704; A: 0.456140; R: 0.456140; P: 0.411598; F1: 0.387561\n",
      "(train @ 2): L: 1.012374; A: 0.484649; R: 0.484649; P: 0.408000; F1: 0.442930\n",
      "(valid @ 2): L: 1.051522; A: 0.482456; R: 0.482456; P: 0.586287; F1: 0.490651\n",
      "(train @ 3): L: 0.961428; A: 0.524123; R: 0.524123; P: 0.627872; F1: 0.480008\n",
      "(valid @ 3): L: 1.007593; A: 0.478070; R: 0.478070; P: 0.402924; F1: 0.396898\n",
      "(train @ 4): L: 0.926826; A: 0.510965; R: 0.510965; P: 0.512286; F1: 0.504712\n",
      "(valid @ 4): L: 1.033457; A: 0.486842; R: 0.486842; P: 0.595648; F1: 0.433963\n",
      "(train @ 5): L: 0.867567; A: 0.532895; R: 0.532895; P: 0.605888; F1: 0.514931\n",
      "(valid @ 5): L: 0.931897; A: 0.521930; R: 0.521930; P: 0.521701; F1: 0.456963\n",
      "(train @ 6): L: 0.817560; A: 0.581140; R: 0.581140; P: 0.617389; F1: 0.574207\n",
      "(valid @ 6): L: 0.954300; A: 0.526316; R: 0.526316; P: 0.625950; F1: 0.465520\n",
      "(train @ 7): L: 0.840154; A: 0.550439; R: 0.550439; P: 0.553621; F1: 0.543946\n",
      "(valid @ 7): L: 0.913506; A: 0.552632; R: 0.552632; P: 0.625371; F1: 0.535987\n",
      "(train @ 8): L: 0.733775; A: 0.666667; R: 0.666667; P: 0.676362; F1: 0.666455\n",
      "(valid @ 8): L: 0.889402; A: 0.627193; R: 0.627193; P: 0.661699; F1: 0.623144\n",
      "(train @ 9): L: 0.754560; A: 0.629386; R: 0.629386; P: 0.683220; F1: 0.609644\n",
      "(valid @ 9): L: 0.955203; A: 0.495614; R: 0.495614; P: 0.349464; F1: 0.364620\n",
      "(train @ 10): L: 0.830056; A: 0.543860; R: 0.543860; P: 0.554844; F1: 0.537707\n",
      "(valid @ 10): L: 0.943989; A: 0.543860; R: 0.543860; P: 0.603824; F1: 0.521758\n",
      "(train @ 11): L: 0.763296; A: 0.618421; R: 0.618421; P: 0.666950; F1: 0.606333\n",
      "(valid @ 11): L: 0.904032; A: 0.530702; R: 0.530702; P: 0.320430; F1: 0.393818\n",
      "(train @ 12): L: 0.688106; A: 0.629386; R: 0.629386; P: 0.641916; F1: 0.617873\n",
      "(valid @ 12): L: 0.932404; A: 0.596491; R: 0.596491; P: 0.609074; F1: 0.573991\n",
      "(train @ 13): L: 0.729946; A: 0.649123; R: 0.649123; P: 0.667318; F1: 0.645495\n",
      "(valid @ 13): L: 0.809771; A: 0.631579; R: 0.631579; P: 0.641512; F1: 0.620483\n",
      "(train @ 14): L: 0.633663; A: 0.690789; R: 0.690789; P: 0.721581; F1: 0.685854\n",
      "(valid @ 14): L: 0.859071; A: 0.627193; R: 0.627193; P: 0.630366; F1: 0.613032\n",
      "(train @ 15): L: 0.692340; A: 0.640351; R: 0.640351; P: 0.640546; F1: 0.640434\n",
      "(valid @ 15): L: 0.818834; A: 0.596491; R: 0.596491; P: 0.597863; F1: 0.575385\n",
      "(train @ 16): L: 0.643145; A: 0.684211; R: 0.684211; P: 0.697736; F1: 0.678994\n",
      "(valid @ 16): L: 0.740946; A: 0.662281; R: 0.662281; P: 0.662093; F1: 0.656937\n",
      "(train @ 17): L: 0.618233; A: 0.708333; R: 0.708333; P: 0.720386; F1: 0.706853\n",
      "(valid @ 17): L: 0.732231; A: 0.640351; R: 0.640351; P: 0.641737; F1: 0.633261\n",
      "(train @ 18): L: 0.670336; A: 0.660088; R: 0.660088; P: 0.670372; F1: 0.656386\n",
      "(valid @ 18): L: 0.695933; A: 0.679825; R: 0.679825; P: 0.679055; F1: 0.677798\n",
      "(train @ 19): L: 0.575671; A: 0.734649; R: 0.734649; P: 0.766742; F1: 0.728140\n",
      "(valid @ 19): L: 0.729490; A: 0.688596; R: 0.688596; P: 0.714060; F1: 0.687867\n",
      "(train @ 20): L: 0.591385; A: 0.750000; R: 0.750000; P: 0.752552; F1: 0.749958\n",
      "(valid @ 20): L: 0.691088; A: 0.671053; R: 0.671053; P: 0.678849; F1: 0.668644\n",
      "(train @ 21): L: 0.550598; A: 0.743421; R: 0.743421; P: 0.770786; F1: 0.741200\n",
      "(valid @ 21): L: 0.708296; A: 0.701754; R: 0.701754; P: 0.718322; F1: 0.699804\n",
      "(train @ 22): L: 0.551430; A: 0.732456; R: 0.732456; P: 0.747193; F1: 0.727593\n",
      "(valid @ 22): L: 0.701910; A: 0.684211; R: 0.684211; P: 0.695850; F1: 0.679826\n",
      "(train @ 23): L: 0.531141; A: 0.756579; R: 0.756579; P: 0.775781; F1: 0.755079\n",
      "(valid @ 23): L: 0.713264; A: 0.684211; R: 0.684211; P: 0.685667; F1: 0.681769\n",
      "(train @ 24): L: 0.471341; A: 0.800439; R: 0.800439; P: 0.801764; F1: 0.799593\n",
      "(valid @ 24): L: 0.708426; A: 0.679825; R: 0.679825; P: 0.681575; F1: 0.676820\n",
      "(train @ 25): L: 0.443436; A: 0.817982; R: 0.817982; P: 0.818792; F1: 0.817538\n",
      "(valid @ 25): L: 0.970431; A: 0.618421; R: 0.618421; P: 0.739250; F1: 0.568404\n",
      "(train @ 26): L: 0.618292; A: 0.699561; R: 0.699561; P: 0.701824; F1: 0.700084\n",
      "(valid @ 26): L: 0.712693; A: 0.666667; R: 0.666667; P: 0.671329; F1: 0.664032\n",
      "(train @ 27): L: 0.524275; A: 0.758772; R: 0.758772; P: 0.761114; F1: 0.759118\n",
      "(valid @ 27): L: 0.667287; A: 0.675439; R: 0.675439; P: 0.678662; F1: 0.674147\n",
      "(train @ 28): L: 0.457353; A: 0.780702; R: 0.780702; P: 0.793005; F1: 0.778582\n",
      "(valid @ 28): L: 0.677929; A: 0.684211; R: 0.684211; P: 0.689215; F1: 0.685532\n",
      "(train @ 29): L: 0.442072; A: 0.791667; R: 0.791667; P: 0.797151; F1: 0.791530\n",
      "(valid @ 29): L: 0.657365; A: 0.688596; R: 0.688596; P: 0.695456; F1: 0.687160\n",
      "(train @ 30): L: 0.372689; A: 0.837719; R: 0.837719; P: 0.848310; F1: 0.837434\n",
      "(valid @ 30): L: 0.913409; A: 0.657895; R: 0.657895; P: 0.725650; F1: 0.648980\n",
      "Best val Metric 0.699804 @ 21\n",
      "\n",
      "models are saved @ ./predictions/211209064755/flvl_1_2_4_5_7_8_pretrained_model_c1.pt\n",
      "Saving predictions @ ./predictions/211209064755/flvl_train_1_2_4_5_7_8_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209064755/flvl_valid_3_6_9_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209064755/flvl_public_test_trained_on_1_2_4_5_7_8_r21d_rgb.csv\n",
      "{'train': [1, 3, 4, 6, 7, 9], 'valid': [2, 5, 8], 'public_test': [10, 11, 12], 'private_test': [13, 14, 15]}\n",
      "(train @ 1): L: 1.102309; A: 0.475877; R: 0.475877; P: 0.448603; F1: 0.454168\n",
      "(valid @ 1): L: 0.972480; A: 0.508772; R: 0.508772; P: 0.464268; F1: 0.452423\n",
      "(train @ 2): L: 1.021761; A: 0.460526; R: 0.460526; P: 0.387910; F1: 0.420909\n",
      "(valid @ 2): L: 0.955965; A: 0.614035; R: 0.614035; P: 0.647210; F1: 0.587063\n",
      "(train @ 3): L: 0.997453; A: 0.469298; R: 0.469298; P: 0.536883; F1: 0.445921\n",
      "(valid @ 3): L: 1.068181; A: 0.412281; R: 0.412281; P: 0.178284; F1: 0.248924\n",
      "(train @ 4): L: 0.957680; A: 0.469298; R: 0.469298; P: 0.476777; F1: 0.451330\n",
      "(valid @ 4): L: 0.958075; A: 0.508772; R: 0.508772; P: 0.464268; F1: 0.452423\n",
      "(train @ 5): L: 0.906284; A: 0.493421; R: 0.493421; P: 0.535405; F1: 0.459969\n",
      "(valid @ 5): L: 0.949634; A: 0.421053; R: 0.421053; P: 0.177285; F1: 0.249513\n",
      "(train @ 6): L: 0.888722; A: 0.524123; R: 0.524123; P: 0.526993; F1: 0.520214\n",
      "(valid @ 6): L: 0.952552; A: 0.438596; R: 0.438596; P: 0.342466; F1: 0.288221\n",
      "(train @ 7): L: 0.843187; A: 0.508772; R: 0.508772; P: 0.560899; F1: 0.475735\n",
      "(valid @ 7): L: 0.835103; A: 0.635965; R: 0.635965; P: 0.639909; F1: 0.628155\n",
      "(train @ 8): L: 0.848579; A: 0.598684; R: 0.598684; P: 0.631596; F1: 0.598949\n",
      "(valid @ 8): L: 0.858807; A: 0.618421; R: 0.618421; P: 0.675017; F1: 0.605063\n",
      "(train @ 9): L: 0.805935; A: 0.605263; R: 0.605263; P: 0.659956; F1: 0.581317\n",
      "(valid @ 9): L: 0.844062; A: 0.500000; R: 0.500000; P: 0.560729; F1: 0.377014\n",
      "(train @ 10): L: 0.812512; A: 0.546053; R: 0.546053; P: 0.556564; F1: 0.544650\n",
      "(valid @ 10): L: 0.781696; A: 0.622807; R: 0.622807; P: 0.657491; F1: 0.611945\n",
      "(train @ 11): L: 0.788287; A: 0.603070; R: 0.603070; P: 0.689737; F1: 0.567635\n",
      "(valid @ 11): L: 0.964572; A: 0.517544; R: 0.517544; P: 0.354113; F1: 0.387471\n",
      "(train @ 12): L: 0.777780; A: 0.581140; R: 0.581140; P: 0.587949; F1: 0.569404\n",
      "(valid @ 12): L: 0.838983; A: 0.574561; R: 0.574561; P: 0.643682; F1: 0.559918\n",
      "(train @ 13): L: 0.753166; A: 0.627193; R: 0.627193; P: 0.657360; F1: 0.619672\n",
      "(valid @ 13): L: 0.921477; A: 0.521930; R: 0.521930; P: 0.357013; F1: 0.393479\n",
      "(train @ 14): L: 0.791710; A: 0.572368; R: 0.572368; P: 0.570431; F1: 0.571165\n",
      "(valid @ 14): L: 0.701510; A: 0.684211; R: 0.684211; P: 0.685453; F1: 0.679788\n",
      "(train @ 15): L: 0.752682; A: 0.631579; R: 0.631579; P: 0.633669; F1: 0.630913\n",
      "(valid @ 15): L: 0.691072; A: 0.728070; R: 0.728070; P: 0.747101; F1: 0.729433\n",
      "(train @ 16): L: 0.670913; A: 0.695175; R: 0.695175; P: 0.719901; F1: 0.688102\n",
      "(valid @ 16): L: 0.697486; A: 0.714912; R: 0.714912; P: 0.734272; F1: 0.715541\n",
      "(train @ 17): L: 0.658917; A: 0.684211; R: 0.684211; P: 0.686317; F1: 0.682686\n",
      "(valid @ 17): L: 0.646733; A: 0.741228; R: 0.741228; P: 0.754039; F1: 0.743702\n",
      "(train @ 18): L: 0.675075; A: 0.657895; R: 0.657895; P: 0.684823; F1: 0.647451\n",
      "(valid @ 18): L: 0.633669; A: 0.723684; R: 0.723684; P: 0.738253; F1: 0.718258\n",
      "(train @ 19): L: 0.659775; A: 0.690789; R: 0.690789; P: 0.724962; F1: 0.685746\n",
      "(valid @ 19): L: 0.714015; A: 0.688596; R: 0.688596; P: 0.715034; F1: 0.684882\n",
      "(train @ 20): L: 0.631293; A: 0.701754; R: 0.701754; P: 0.708031; F1: 0.698963\n",
      "(valid @ 20): L: 0.597905; A: 0.750000; R: 0.750000; P: 0.758548; F1: 0.751886\n",
      "(train @ 21): L: 0.628862; A: 0.695175; R: 0.695175; P: 0.715722; F1: 0.691867\n",
      "(valid @ 21): L: 0.599200; A: 0.741228; R: 0.741228; P: 0.745824; F1: 0.739379\n",
      "(train @ 22): L: 0.629520; A: 0.706140; R: 0.706140; P: 0.719336; F1: 0.702156\n",
      "(valid @ 22): L: 0.691053; A: 0.710526; R: 0.710526; P: 0.770175; F1: 0.706883\n",
      "(train @ 23): L: 0.640992; A: 0.688596; R: 0.688596; P: 0.698158; F1: 0.686156\n",
      "(valid @ 23): L: 0.687587; A: 0.697368; R: 0.697368; P: 0.731187; F1: 0.698320\n",
      "(train @ 24): L: 0.569571; A: 0.728070; R: 0.728070; P: 0.728054; F1: 0.725570\n",
      "(valid @ 24): L: 0.584513; A: 0.745614; R: 0.745614; P: 0.758707; F1: 0.746689\n",
      "(train @ 25): L: 0.568643; A: 0.758772; R: 0.758772; P: 0.766480; F1: 0.758179\n",
      "(valid @ 25): L: 0.793767; A: 0.640351; R: 0.640351; P: 0.773927; F1: 0.591236\n",
      "(train @ 26): L: 0.646042; A: 0.712719; R: 0.712719; P: 0.711775; F1: 0.710187\n",
      "(valid @ 26): L: 0.585746; A: 0.754386; R: 0.754386; P: 0.785181; F1: 0.753826\n",
      "(train @ 27): L: 0.564240; A: 0.710526; R: 0.710526; P: 0.711005; F1: 0.708641\n",
      "(valid @ 27): L: 0.578346; A: 0.732456; R: 0.732456; P: 0.753267; F1: 0.733752\n",
      "(train @ 28): L: 0.510177; A: 0.760965; R: 0.760965; P: 0.762158; F1: 0.759698\n",
      "(valid @ 28): L: 0.589083; A: 0.732456; R: 0.732456; P: 0.771752; F1: 0.733178\n",
      "(train @ 29): L: 0.474798; A: 0.767544; R: 0.767544; P: 0.770813; F1: 0.766439\n",
      "(valid @ 29): L: 0.608225; A: 0.750000; R: 0.750000; P: 0.808838; F1: 0.748167\n",
      "(train @ 30): L: 0.437475; A: 0.813596; R: 0.813596; P: 0.817452; F1: 0.813059\n",
      "(valid @ 30): L: 0.540944; A: 0.758772; R: 0.758772; P: 0.809721; F1: 0.754670\n",
      "Best val Metric 0.754670 @ 30\n",
      "\n",
      "models are saved @ ./predictions/211209064755/flvl_1_3_4_6_7_9_pretrained_model_c1.pt\n",
      "Saving predictions @ ./predictions/211209064755/flvl_train_1_3_4_6_7_9_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209064755/flvl_valid_2_5_8_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209064755/flvl_public_test_trained_on_1_3_4_6_7_9_r21d_rgb.csv\n",
      "{'train': [2, 3, 5, 6, 8, 9], 'valid': [1, 4, 7], 'public_test': [10, 11, 12], 'private_test': [13, 14, 15]}\n",
      "(train @ 1): L: 1.123683; A: 0.480263; R: 0.480263; P: 0.461628; F1: 0.460909\n",
      "(valid @ 1): L: 1.045772; A: 0.517544; R: 0.517544; P: 0.471679; F1: 0.462520\n",
      "(train @ 2): L: 1.048952; A: 0.453947; R: 0.453947; P: 0.382360; F1: 0.414850\n",
      "(valid @ 2): L: 1.010119; A: 0.504386; R: 0.504386; P: 0.315489; F1: 0.368056\n",
      "(train @ 3): L: 1.040719; A: 0.434211; R: 0.434211; P: 0.476358; F1: 0.431689\n",
      "(valid @ 3): L: 1.041902; A: 0.521930; R: 0.521930; P: 0.457546; F1: 0.476999\n",
      "(train @ 4): L: 0.980200; A: 0.462719; R: 0.462719; P: 0.543920; F1: 0.407632\n",
      "(valid @ 4): L: 0.946757; A: 0.521930; R: 0.521930; P: 0.630760; F1: 0.472013\n",
      "(train @ 5): L: 0.925601; A: 0.515351; R: 0.515351; P: 0.650201; F1: 0.459997\n",
      "(valid @ 5): L: 0.979451; A: 0.425439; R: 0.425439; P: 0.335961; F1: 0.258820\n",
      "(train @ 6): L: 0.917489; A: 0.458333; R: 0.458333; P: 0.460378; F1: 0.455979\n",
      "(valid @ 6): L: 0.927014; A: 0.561404; R: 0.561404; P: 0.645933; F1: 0.498039\n",
      "(train @ 7): L: 0.873925; A: 0.475877; R: 0.475877; P: 0.531357; F1: 0.405905\n",
      "(valid @ 7): L: 0.896706; A: 0.622807; R: 0.622807; P: 0.643242; F1: 0.613347\n",
      "(train @ 8): L: 0.899971; A: 0.552632; R: 0.552632; P: 0.655748; F1: 0.521699\n",
      "(valid @ 8): L: 0.931496; A: 0.469298; R: 0.469298; P: 0.626601; F1: 0.344374\n",
      "(train @ 9): L: 0.811705; A: 0.572368; R: 0.572368; P: 0.578601; F1: 0.568171\n",
      "(valid @ 9): L: 0.800326; A: 0.622807; R: 0.622807; P: 0.657936; F1: 0.623862\n",
      "(train @ 10): L: 0.807044; A: 0.524123; R: 0.524123; P: 0.529987; F1: 0.518892\n",
      "(valid @ 10): L: 0.791033; A: 0.618421; R: 0.618421; P: 0.661132; F1: 0.617776\n",
      "(train @ 11): L: 0.793072; A: 0.546053; R: 0.546053; P: 0.675818; F1: 0.467653\n",
      "(valid @ 11): L: 0.915875; A: 0.464912; R: 0.464912; P: 0.478051; F1: 0.420734\n",
      "(train @ 12): L: 0.808647; A: 0.552632; R: 0.552632; P: 0.560934; F1: 0.529508\n",
      "(valid @ 12): L: 0.823097; A: 0.565789; R: 0.565789; P: 0.663841; F1: 0.538333\n",
      "(train @ 13): L: 0.744915; A: 0.607456; R: 0.607456; P: 0.661868; F1: 0.584222\n",
      "(valid @ 13): L: 0.984806; A: 0.478070; R: 0.478070; P: 0.349464; F1: 0.347110\n",
      "(train @ 14): L: 0.807578; A: 0.513158; R: 0.513158; P: 0.509452; F1: 0.503476\n",
      "(valid @ 14): L: 0.736260; A: 0.649123; R: 0.649123; P: 0.683952; F1: 0.643863\n",
      "(train @ 15): L: 0.735517; A: 0.614035; R: 0.614035; P: 0.622064; F1: 0.614163\n",
      "(valid @ 15): L: 0.759323; A: 0.557018; R: 0.557018; P: 0.784130; F1: 0.454869\n",
      "(train @ 16): L: 0.670178; A: 0.666667; R: 0.666667; P: 0.673378; F1: 0.665131\n",
      "(valid @ 16): L: 0.706017; A: 0.657895; R: 0.657895; P: 0.693498; F1: 0.654402\n",
      "(train @ 17): L: 0.699510; A: 0.646930; R: 0.646930; P: 0.657673; F1: 0.643234\n",
      "(valid @ 17): L: 0.774706; A: 0.697368; R: 0.697368; P: 0.730146; F1: 0.691282\n",
      "(train @ 18): L: 0.663500; A: 0.664474; R: 0.664474; P: 0.694217; F1: 0.651868\n",
      "(valid @ 18): L: 0.737203; A: 0.728070; R: 0.728070; P: 0.748284; F1: 0.726779\n",
      "(train @ 19): L: 0.614671; A: 0.708333; R: 0.708333; P: 0.734782; F1: 0.702975\n",
      "(valid @ 19): L: 0.802852; A: 0.692982; R: 0.692982; P: 0.722101; F1: 0.685163\n",
      "(train @ 20): L: 0.649715; A: 0.692982; R: 0.692982; P: 0.706518; F1: 0.690086\n",
      "(valid @ 20): L: 0.667917; A: 0.714912; R: 0.714912; P: 0.733987; F1: 0.715999\n",
      "(train @ 21): L: 0.626623; A: 0.714912; R: 0.714912; P: 0.744491; F1: 0.712547\n",
      "(valid @ 21): L: 0.611737; A: 0.723684; R: 0.723684; P: 0.738712; F1: 0.722986\n",
      "(train @ 22): L: 0.617236; A: 0.721491; R: 0.721491; P: 0.747120; F1: 0.717399\n",
      "(valid @ 22): L: 0.717603; A: 0.684211; R: 0.684211; P: 0.720909; F1: 0.685426\n",
      "(train @ 23): L: 0.627810; A: 0.692982; R: 0.692982; P: 0.704920; F1: 0.691588\n",
      "(valid @ 23): L: 0.780571; A: 0.706140; R: 0.706140; P: 0.736649; F1: 0.702519\n",
      "(train @ 24): L: 0.570476; A: 0.745614; R: 0.745614; P: 0.746044; F1: 0.744478\n",
      "(valid @ 24): L: 0.649778; A: 0.710526; R: 0.710526; P: 0.731324; F1: 0.707794\n",
      "(train @ 25): L: 0.541206; A: 0.756579; R: 0.756579; P: 0.756554; F1: 0.755617\n",
      "(valid @ 25): L: 0.684665; A: 0.671053; R: 0.671053; P: 0.746000; F1: 0.658428\n",
      "(train @ 26): L: 0.593254; A: 0.692982; R: 0.692982; P: 0.692637; F1: 0.690691\n",
      "(valid @ 26): L: 0.621353; A: 0.741228; R: 0.741228; P: 0.758962; F1: 0.741860\n",
      "(train @ 27): L: 0.540585; A: 0.767544; R: 0.767544; P: 0.771163; F1: 0.767779\n",
      "(valid @ 27): L: 0.655106; A: 0.754386; R: 0.754386; P: 0.771923; F1: 0.753404\n",
      "(train @ 28): L: 0.543211; A: 0.723684; R: 0.723684; P: 0.729508; F1: 0.719696\n",
      "(valid @ 28): L: 0.684512; A: 0.723684; R: 0.723684; P: 0.743603; F1: 0.724119\n",
      "(train @ 29): L: 0.489926; A: 0.760965; R: 0.760965; P: 0.768205; F1: 0.757831\n",
      "(valid @ 29): L: 0.688388; A: 0.714912; R: 0.714912; P: 0.741340; F1: 0.715288\n",
      "(train @ 30): L: 0.446818; A: 0.802632; R: 0.802632; P: 0.809771; F1: 0.800881\n",
      "(valid @ 30): L: 0.585566; A: 0.763158; R: 0.763158; P: 0.772995; F1: 0.762167\n",
      "Best val Metric 0.762167 @ 30\n",
      "\n",
      "models are saved @ ./predictions/211209064755/flvl_2_3_5_6_8_9_pretrained_model_c1.pt\n",
      "Saving predictions @ ./predictions/211209064755/flvl_train_2_3_5_6_8_9_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209064755/flvl_valid_1_4_7_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209064755/flvl_public_test_trained_on_2_3_5_6_8_9_r21d_rgb.csv\n",
      "Average of Best Metrics on Each Valid Set: 0.738880, 211209064755\n",
      "Saved test results @ ./predictions/211209064755/flvl_public_test_agg_r21d_rgb.csv\n"
     ]
    }
   ],
   "source": [
    "# View 2,3\n",
    "from main_2Views import Config, run_kfold\n",
    "\n",
    "# define architecture and run k-fold training\n",
    "cfg = Config()\n",
    "cfg.assign_variable('task', 'flvl')\n",
    "cfg.assign_variable('output_dim', 3)\n",
    "cfg.assign_variable('model_type', 'GRU')\n",
    "cfg.assign_variable('bi_dir', False)\n",
    "cfg.assign_variable('device', 'cuda:0')\n",
    "cfg.assign_variable('data_root', './r21d_rgb_features')\n",
    "cfg.assign_variable('drop_p', 0.0) # results will be irreproducible\n",
    "cfg.assign_variable('batch_size', 64)\n",
    "cfg.assign_variable('input_dim', 512)\n",
    "cfg.assign_variable('hidden_dim', 512)\n",
    "cfg.assign_variable('n_layers', 3)\n",
    "cfg.assign_variable('num_epochs', 30)\n",
    "cfg.assign_variable('seed', 1337)\n",
    "\n",
    "run_kfold(cfg,[2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### D. Stacked-feature test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': [1, 2, 4, 5, 7, 8], 'valid': [3, 6, 9], 'public_test': [10, 11, 12], 'private_test': [13, 14, 15]}\n",
      "<dataset.RGBDataset object at 0x7f3d487d8d90>\n",
      "(train @ 1): L: 1.031029; A: 0.432018; R: 0.432018; P: 0.398449; F1: 0.413469\n",
      "(valid @ 1): L: 0.983870; A: 0.478070; R: 0.478070; P: 0.435605; F1: 0.415544\n",
      "(train @ 2): L: 0.978386; A: 0.451754; R: 0.451754; P: 0.381721; F1: 0.413223\n",
      "(valid @ 2): L: 0.980247; A: 0.469298; R: 0.469298; P: 0.426429; F1: 0.404527\n",
      "(train @ 3): L: 0.936884; A: 0.495614; R: 0.495614; P: 0.538042; F1: 0.405092\n",
      "(valid @ 3): L: 0.955418; A: 0.438596; R: 0.438596; P: 0.339972; F1: 0.285799\n",
      "(train @ 4): L: 0.875304; A: 0.482456; R: 0.482456; P: 0.551210; F1: 0.445279\n",
      "(valid @ 4): L: 0.928145; A: 0.535088; R: 0.535088; P: 0.608397; F1: 0.511510\n",
      "(train @ 5): L: 0.820493; A: 0.570175; R: 0.570175; P: 0.601696; F1: 0.568738\n",
      "(valid @ 5): L: 0.953912; A: 0.565789; R: 0.565789; P: 0.570515; F1: 0.541860\n",
      "(train @ 6): L: 0.832228; A: 0.581140; R: 0.581140; P: 0.584712; F1: 0.581242\n",
      "(valid @ 6): L: 0.842928; A: 0.609649; R: 0.609649; P: 0.611892; F1: 0.591617\n",
      "(train @ 7): L: 0.733737; A: 0.629386; R: 0.629386; P: 0.676192; F1: 0.615710\n",
      "(valid @ 7): L: 0.760637; A: 0.640351; R: 0.640351; P: 0.643169; F1: 0.636972\n",
      "(train @ 8): L: 0.689809; A: 0.692982; R: 0.692982; P: 0.716233; F1: 0.691558\n",
      "(valid @ 8): L: 0.738441; A: 0.675439; R: 0.675439; P: 0.675188; F1: 0.672545\n",
      "(train @ 9): L: 0.665934; A: 0.673246; R: 0.673246; P: 0.700212; F1: 0.667913\n",
      "(valid @ 9): L: 0.730538; A: 0.657895; R: 0.657895; P: 0.658364; F1: 0.653153\n",
      "(train @ 10): L: 0.616592; A: 0.719298; R: 0.719298; P: 0.755576; F1: 0.711259\n",
      "(valid @ 10): L: 0.743299; A: 0.618421; R: 0.618421; P: 0.624064; F1: 0.602692\n",
      "(train @ 11): L: 0.649613; A: 0.664474; R: 0.664474; P: 0.710322; F1: 0.651771\n",
      "(valid @ 11): L: 0.878222; A: 0.609649; R: 0.609649; P: 0.639987; F1: 0.609560\n",
      "(train @ 12): L: 0.680808; A: 0.682018; R: 0.682018; P: 0.687873; F1: 0.680981\n",
      "(valid @ 12): L: 0.851178; A: 0.552632; R: 0.552632; P: 0.643675; F1: 0.490046\n",
      "(train @ 13): L: 0.642636; A: 0.671053; R: 0.671053; P: 0.675764; F1: 0.670534\n",
      "(valid @ 13): L: 0.846548; A: 0.578947; R: 0.578947; P: 0.678057; F1: 0.555353\n",
      "(train @ 14): L: 0.650433; A: 0.638158; R: 0.638158; P: 0.645388; F1: 0.635950\n",
      "(valid @ 14): L: 0.657179; A: 0.657895; R: 0.657895; P: 0.673893; F1: 0.645441\n",
      "(train @ 15): L: 0.648765; A: 0.675439; R: 0.675439; P: 0.718723; F1: 0.652749\n",
      "(valid @ 15): L: 0.668733; A: 0.622807; R: 0.622807; P: 0.656248; F1: 0.601848\n",
      "(train @ 16): L: 0.587083; A: 0.723684; R: 0.723684; P: 0.725602; F1: 0.720834\n",
      "(valid @ 16): L: 0.674631; A: 0.644737; R: 0.644737; P: 0.655230; F1: 0.634427\n",
      "(train @ 17): L: 0.521526; A: 0.758772; R: 0.758772; P: 0.765323; F1: 0.757713\n",
      "(valid @ 17): L: 0.673811; A: 0.640351; R: 0.640351; P: 0.651553; F1: 0.630279\n",
      "(train @ 18): L: 0.504475; A: 0.798246; R: 0.798246; P: 0.798380; F1: 0.797517\n",
      "(valid @ 18): L: 0.673192; A: 0.697368; R: 0.697368; P: 0.717452; F1: 0.697854\n",
      "(train @ 19): L: 0.462031; A: 0.826754; R: 0.826754; P: 0.827117; F1: 0.826885\n",
      "(valid @ 19): L: 0.624831; A: 0.710526; R: 0.710526; P: 0.715220; F1: 0.709781\n",
      "(train @ 20): L: 0.468214; A: 0.800439; R: 0.800439; P: 0.801042; F1: 0.800597\n",
      "(valid @ 20): L: 0.688819; A: 0.640351; R: 0.640351; P: 0.662439; F1: 0.628162\n",
      "(train @ 21): L: 0.560596; A: 0.728070; R: 0.728070; P: 0.729840; F1: 0.727962\n",
      "(valid @ 21): L: 0.680811; A: 0.622807; R: 0.622807; P: 0.674075; F1: 0.595452\n",
      "(train @ 22): L: 0.458103; A: 0.780702; R: 0.780702; P: 0.787531; F1: 0.779521\n",
      "(valid @ 22): L: 0.754082; A: 0.640351; R: 0.640351; P: 0.697994; F1: 0.621987\n",
      "(train @ 23): L: 0.450356; A: 0.782895; R: 0.782895; P: 0.789816; F1: 0.781252\n",
      "(valid @ 23): L: 0.632324; A: 0.719298; R: 0.719298; P: 0.728302; F1: 0.719765\n",
      "(train @ 24): L: 0.375427; A: 0.844298; R: 0.844298; P: 0.846995; F1: 0.843738\n",
      "(valid @ 24): L: 0.611039; A: 0.728070; R: 0.728070; P: 0.735788; F1: 0.728156\n",
      "(train @ 25): L: 0.396090; A: 0.848684; R: 0.848684; P: 0.854954; F1: 0.848652\n",
      "(valid @ 25): L: 0.812372; A: 0.679825; R: 0.679825; P: 0.769852; F1: 0.688857\n",
      "(train @ 26): L: 0.350957; A: 0.857456; R: 0.857456; P: 0.860398; F1: 0.857700\n",
      "(valid @ 26): L: 0.825366; A: 0.657895; R: 0.657895; P: 0.732769; F1: 0.664957\n",
      "(train @ 27): L: 0.348857; A: 0.853070; R: 0.853070; P: 0.853055; F1: 0.853058\n",
      "(valid @ 27): L: 0.839795; A: 0.649123; R: 0.649123; P: 0.729637; F1: 0.652027\n",
      "(train @ 28): L: 0.275257; A: 0.890351; R: 0.890351; P: 0.890272; F1: 0.890279\n",
      "(valid @ 28): L: 0.905436; A: 0.649123; R: 0.649123; P: 0.729275; F1: 0.652629\n",
      "(train @ 29): L: 0.326693; A: 0.853070; R: 0.853070; P: 0.855135; F1: 0.853413\n",
      "(valid @ 29): L: 1.077979; A: 0.578947; R: 0.578947; P: 0.697448; F1: 0.547779\n",
      "(train @ 30): L: 0.417890; A: 0.811404; R: 0.811404; P: 0.811265; F1: 0.811305\n",
      "(valid @ 30): L: 0.893550; A: 0.649123; R: 0.649123; P: 0.732166; F1: 0.652784\n",
      "Best val Metric 0.728156 @ 24\n",
      "\n",
      "models are saved @ ./predictions/211209071503/flvl_1_2_4_5_7_8_pretrained_model_c1.pt\n",
      "Saving predictions @ ./predictions/211209071503/flvl_train_1_2_4_5_7_8_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209071503/flvl_valid_3_6_9_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209071503/flvl_public_test_trained_on_1_2_4_5_7_8_r21d_rgb.csv\n",
      "{'train': [1, 3, 4, 6, 7, 9], 'valid': [2, 5, 8], 'public_test': [10, 11, 12], 'private_test': [13, 14, 15]}\n",
      "<dataset.RGBDataset object at 0x7f3d487d8700>\n",
      "(train @ 1): L: 1.036322; A: 0.447368; R: 0.447368; P: 0.405550; F1: 0.420659\n",
      "(valid @ 1): L: 0.987635; A: 0.456140; R: 0.456140; P: 0.418129; F1: 0.373206\n",
      "(train @ 2): L: 0.993845; A: 0.458333; R: 0.458333; P: 0.384792; F1: 0.416433\n",
      "(valid @ 2): L: 0.989998; A: 0.478070; R: 0.478070; P: 0.435605; F1: 0.415544\n",
      "(train @ 3): L: 0.948641; A: 0.456140; R: 0.456140; P: 0.403459; F1: 0.399468\n",
      "(valid @ 3): L: 0.934240; A: 0.456140; R: 0.456140; P: 0.388653; F1: 0.323468\n",
      "(train @ 4): L: 0.892199; A: 0.528509; R: 0.528509; P: 0.578276; F1: 0.506968\n",
      "(valid @ 4): L: 0.879452; A: 0.574561; R: 0.574561; P: 0.641098; F1: 0.560248\n",
      "(train @ 5): L: 0.879618; A: 0.489035; R: 0.489035; P: 0.518673; F1: 0.486410\n",
      "(valid @ 5): L: 0.862485; A: 0.627193; R: 0.627193; P: 0.634660; F1: 0.621469\n",
      "(train @ 6): L: 0.845213; A: 0.572368; R: 0.572368; P: 0.650046; F1: 0.553285\n",
      "(valid @ 6): L: 0.800412; A: 0.631579; R: 0.631579; P: 0.664681; F1: 0.612895\n",
      "(train @ 7): L: 0.754171; A: 0.664474; R: 0.664474; P: 0.674094; F1: 0.662587\n",
      "(valid @ 7): L: 0.772437; A: 0.622807; R: 0.622807; P: 0.672403; F1: 0.618956\n",
      "(train @ 8): L: 0.755582; A: 0.598684; R: 0.598684; P: 0.614717; F1: 0.594467\n",
      "(valid @ 8): L: 0.701888; A: 0.671053; R: 0.671053; P: 0.712955; F1: 0.652295\n",
      "(train @ 9): L: 0.721046; A: 0.620614; R: 0.620614; P: 0.643103; F1: 0.611039\n",
      "(valid @ 9): L: 0.669352; A: 0.728070; R: 0.728070; P: 0.734744; F1: 0.729226\n",
      "(train @ 10): L: 0.698284; A: 0.640351; R: 0.640351; P: 0.644187; F1: 0.634914\n",
      "(valid @ 10): L: 0.663481; A: 0.631579; R: 0.631579; P: 0.640018; F1: 0.617459\n",
      "(train @ 11): L: 0.722071; A: 0.585526; R: 0.585526; P: 0.625324; F1: 0.540855\n",
      "(valid @ 11): L: 0.679460; A: 0.719298; R: 0.719298; P: 0.755499; F1: 0.712172\n",
      "(train @ 12): L: 0.704330; A: 0.671053; R: 0.671053; P: 0.671125; F1: 0.669530\n",
      "(valid @ 12): L: 0.692664; A: 0.618421; R: 0.618421; P: 0.687907; F1: 0.569213\n",
      "(train @ 13): L: 0.698514; A: 0.625000; R: 0.625000; P: 0.631344; F1: 0.622045\n",
      "(valid @ 13): L: 0.678508; A: 0.684211; R: 0.684211; P: 0.717934; F1: 0.677691\n",
      "(train @ 14): L: 0.693347; A: 0.668860; R: 0.668860; P: 0.689448; F1: 0.667244\n",
      "(valid @ 14): L: 0.627719; A: 0.719298; R: 0.719298; P: 0.745426; F1: 0.716908\n",
      "(train @ 15): L: 0.636398; A: 0.662281; R: 0.662281; P: 0.690783; F1: 0.646194\n",
      "(valid @ 15): L: 0.608236; A: 0.776316; R: 0.776316; P: 0.783366; F1: 0.777148\n",
      "(train @ 16): L: 0.616418; A: 0.701754; R: 0.701754; P: 0.728401; F1: 0.693644\n",
      "(valid @ 16): L: 0.636682; A: 0.618421; R: 0.618421; P: 0.631717; F1: 0.607018\n",
      "(train @ 17): L: 0.600929; A: 0.697368; R: 0.697368; P: 0.702871; F1: 0.695664\n",
      "(valid @ 17): L: 0.594891; A: 0.763158; R: 0.763158; P: 0.775477; F1: 0.764034\n",
      "(train @ 18): L: 0.548837; A: 0.776316; R: 0.776316; P: 0.775255; F1: 0.775543\n",
      "(valid @ 18): L: 0.576184; A: 0.750000; R: 0.750000; P: 0.754231; F1: 0.750797\n",
      "(train @ 19): L: 0.519758; A: 0.769737; R: 0.769737; P: 0.769067; F1: 0.769366\n",
      "(valid @ 19): L: 0.608832; A: 0.728070; R: 0.728070; P: 0.741276; F1: 0.728928\n",
      "(train @ 20): L: 0.526351; A: 0.750000; R: 0.750000; P: 0.749814; F1: 0.749443\n",
      "(valid @ 20): L: 0.778107; A: 0.679825; R: 0.679825; P: 0.710580; F1: 0.670862\n",
      "(train @ 21): L: 0.651547; A: 0.677632; R: 0.677632; P: 0.679246; F1: 0.676903\n",
      "(valid @ 21): L: 0.579021; A: 0.763158; R: 0.763158; P: 0.780849; F1: 0.762772\n",
      "(train @ 22): L: 0.538391; A: 0.743421; R: 0.743421; P: 0.758441; F1: 0.740300\n",
      "(valid @ 22): L: 0.529440; A: 0.767544; R: 0.767544; P: 0.778049; F1: 0.768119\n",
      "(train @ 23): L: 0.551980; A: 0.745614; R: 0.745614; P: 0.749970; F1: 0.744090\n",
      "(valid @ 23): L: 0.608993; A: 0.710526; R: 0.710526; P: 0.769424; F1: 0.705817\n",
      "(train @ 24): L: 0.515832; A: 0.756579; R: 0.756579; P: 0.758912; F1: 0.755997\n",
      "(valid @ 24): L: 0.592249; A: 0.723684; R: 0.723684; P: 0.739276; F1: 0.724671\n",
      "(train @ 25): L: 0.474864; A: 0.789474; R: 0.789474; P: 0.799379; F1: 0.788443\n",
      "(valid @ 25): L: 0.550942; A: 0.745614; R: 0.745614; P: 0.756328; F1: 0.743123\n",
      "(train @ 26): L: 0.459387; A: 0.813596; R: 0.813596; P: 0.813772; F1: 0.813648\n",
      "(valid @ 26): L: 0.537018; A: 0.728070; R: 0.728070; P: 0.746798; F1: 0.725336\n",
      "(train @ 27): L: 0.416946; A: 0.822368; R: 0.822368; P: 0.823728; F1: 0.822147\n",
      "(valid @ 27): L: 0.531344; A: 0.736842; R: 0.736842; P: 0.789389; F1: 0.731378\n",
      "(train @ 28): L: 0.366614; A: 0.839912; R: 0.839912; P: 0.840002; F1: 0.839134\n",
      "(valid @ 28): L: 0.615320; A: 0.692982; R: 0.692982; P: 0.740014; F1: 0.680057\n",
      "(train @ 29): L: 0.397731; A: 0.820175; R: 0.820175; P: 0.825334; F1: 0.821357\n",
      "(valid @ 29): L: 0.561947; A: 0.714912; R: 0.714912; P: 0.791213; F1: 0.702632\n",
      "(train @ 30): L: 0.418207; A: 0.793860; R: 0.793860; P: 0.795569; F1: 0.793372\n",
      "(valid @ 30): L: 0.507380; A: 0.758772; R: 0.758772; P: 0.759027; F1: 0.758662\n",
      "Best val Metric 0.777148 @ 15\n",
      "\n",
      "models are saved @ ./predictions/211209071503/flvl_1_3_4_6_7_9_pretrained_model_c1.pt\n",
      "Saving predictions @ ./predictions/211209071503/flvl_train_1_3_4_6_7_9_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209071503/flvl_valid_2_5_8_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209071503/flvl_public_test_trained_on_1_3_4_6_7_9_r21d_rgb.csv\n",
      "{'train': [2, 3, 5, 6, 8, 9], 'valid': [1, 4, 7], 'public_test': [10, 11, 12], 'private_test': [13, 14, 15]}\n",
      "<dataset.RGBDataset object at 0x7f3d54051670>\n",
      "(train @ 1): L: 1.035939; A: 0.416667; R: 0.416667; P: 0.377956; F1: 0.392014\n",
      "(valid @ 1): L: 0.984700; A: 0.473684; R: 0.473684; P: 0.434531; F1: 0.405480\n",
      "(train @ 2): L: 0.985726; A: 0.460526; R: 0.460526; P: 0.388689; F1: 0.421053\n",
      "(valid @ 2): L: 0.981921; A: 0.473684; R: 0.473684; P: 0.438516; F1: 0.400409\n",
      "(train @ 3): L: 0.938624; A: 0.471491; R: 0.471491; P: 0.592558; F1: 0.404762\n",
      "(valid @ 3): L: 0.945942; A: 0.442982; R: 0.442982; P: 0.339972; F1: 0.292731\n",
      "(train @ 4): L: 0.879401; A: 0.517544; R: 0.517544; P: 0.521241; F1: 0.511203\n",
      "(valid @ 4): L: 0.883753; A: 0.530702; R: 0.530702; P: 0.628092; F1: 0.495725\n",
      "(train @ 5): L: 0.856969; A: 0.510965; R: 0.510965; P: 0.515992; F1: 0.512154\n",
      "(valid @ 5): L: 0.811874; A: 0.635965; R: 0.635965; P: 0.672711; F1: 0.630733\n",
      "(train @ 6): L: 0.788597; A: 0.603070; R: 0.603070; P: 0.653489; F1: 0.587496\n",
      "(valid @ 6): L: 0.799954; A: 0.618421; R: 0.618421; P: 0.705654; F1: 0.577248\n",
      "(train @ 7): L: 0.722944; A: 0.653509; R: 0.653509; P: 0.657731; F1: 0.652639\n",
      "(valid @ 7): L: 0.836017; A: 0.618421; R: 0.618421; P: 0.678145; F1: 0.610335\n",
      "(train @ 8): L: 0.733797; A: 0.574561; R: 0.574561; P: 0.573332; F1: 0.572450\n",
      "(valid @ 8): L: 0.774555; A: 0.697368; R: 0.697368; P: 0.719298; F1: 0.698088\n",
      "(train @ 9): L: 0.702319; A: 0.649123; R: 0.649123; P: 0.675654; F1: 0.642790\n",
      "(valid @ 9): L: 0.731581; A: 0.710526; R: 0.710526; P: 0.767232; F1: 0.705426\n",
      "(train @ 10): L: 0.658759; A: 0.684211; R: 0.684211; P: 0.704034; F1: 0.675567\n",
      "(valid @ 10): L: 0.699468; A: 0.635965; R: 0.635965; P: 0.678678; F1: 0.628831\n",
      "(train @ 11): L: 0.682181; A: 0.594298; R: 0.594298; P: 0.627987; F1: 0.573097\n",
      "(valid @ 11): L: 0.659266; A: 0.692982; R: 0.692982; P: 0.781396; F1: 0.674608\n",
      "(train @ 12): L: 0.676422; A: 0.712719; R: 0.712719; P: 0.720985; F1: 0.711364\n",
      "(valid @ 12): L: 0.695333; A: 0.618421; R: 0.618421; P: 0.701027; F1: 0.585869\n",
      "(train @ 13): L: 0.684547; A: 0.618421; R: 0.618421; P: 0.616485; F1: 0.617147\n",
      "(valid @ 13): L: 0.711472; A: 0.605263; R: 0.605263; P: 0.655864; F1: 0.588577\n",
      "(train @ 14): L: 0.778196; A: 0.627193; R: 0.627193; P: 0.655000; F1: 0.620673\n",
      "(valid @ 14): L: 0.801117; A: 0.684211; R: 0.684211; P: 0.716958; F1: 0.674415\n",
      "(train @ 15): L: 0.662619; A: 0.660088; R: 0.660088; P: 0.703789; F1: 0.642179\n",
      "(valid @ 15): L: 0.732131; A: 0.671053; R: 0.671053; P: 0.696757; F1: 0.670935\n",
      "(train @ 16): L: 0.603494; A: 0.745614; R: 0.745614; P: 0.758959; F1: 0.743060\n",
      "(valid @ 16): L: 0.680914; A: 0.649123; R: 0.649123; P: 0.673561; F1: 0.639566\n",
      "(train @ 17): L: 0.590088; A: 0.706140; R: 0.706140; P: 0.709441; F1: 0.703818\n",
      "(valid @ 17): L: 0.665417; A: 0.719298; R: 0.719298; P: 0.738868; F1: 0.720818\n",
      "(train @ 18): L: 0.541877; A: 0.782895; R: 0.782895; P: 0.788055; F1: 0.783218\n",
      "(valid @ 18): L: 0.622006; A: 0.741228; R: 0.741228; P: 0.754234; F1: 0.741750\n",
      "(train @ 19): L: 0.509038; A: 0.778509; R: 0.778509; P: 0.778319; F1: 0.777934\n",
      "(valid @ 19): L: 0.689127; A: 0.719298; R: 0.719298; P: 0.738638; F1: 0.718777\n",
      "(train @ 20): L: 0.505810; A: 0.780702; R: 0.780702; P: 0.781937; F1: 0.779880\n",
      "(valid @ 20): L: 0.798260; A: 0.675439; R: 0.675439; P: 0.706277; F1: 0.665414\n",
      "(train @ 21): L: 0.593187; A: 0.710526; R: 0.710526; P: 0.710415; F1: 0.710238\n",
      "(valid @ 21): L: 0.748969; A: 0.675439; R: 0.675439; P: 0.719687; F1: 0.661824\n",
      "(train @ 22): L: 0.525885; A: 0.743421; R: 0.743421; P: 0.761943; F1: 0.741127\n",
      "(valid @ 22): L: 0.552712; A: 0.745614; R: 0.745614; P: 0.754363; F1: 0.746601\n",
      "(train @ 23): L: 0.584855; A: 0.723684; R: 0.723684; P: 0.725883; F1: 0.723082\n",
      "(valid @ 23): L: 0.781736; A: 0.635965; R: 0.635965; P: 0.726529; F1: 0.620386\n",
      "(train @ 24): L: 0.621025; A: 0.699561; R: 0.699561; P: 0.703457; F1: 0.699391\n",
      "(valid @ 24): L: 0.854249; A: 0.605263; R: 0.605263; P: 0.652709; F1: 0.581109\n",
      "(train @ 25): L: 0.525768; A: 0.756579; R: 0.756579; P: 0.768730; F1: 0.755057\n",
      "(valid @ 25): L: 0.672160; A: 0.692982; R: 0.692982; P: 0.735559; F1: 0.684118\n",
      "(train @ 26): L: 0.484865; A: 0.769737; R: 0.769737; P: 0.768905; F1: 0.768412\n",
      "(valid @ 26): L: 0.700740; A: 0.719298; R: 0.719298; P: 0.750708; F1: 0.717095\n",
      "(train @ 27): L: 0.473043; A: 0.785088; R: 0.785088; P: 0.794075; F1: 0.784346\n",
      "(valid @ 27): L: 0.739810; A: 0.723684; R: 0.723684; P: 0.744888; F1: 0.720093\n",
      "(train @ 28): L: 0.403741; A: 0.831140; R: 0.831140; P: 0.830832; F1: 0.830593\n",
      "(valid @ 28): L: 0.569502; A: 0.750000; R: 0.750000; P: 0.784720; F1: 0.749045\n",
      "(train @ 29): L: 0.465756; A: 0.769737; R: 0.769737; P: 0.776209; F1: 0.771648\n",
      "(valid @ 29): L: 0.587986; A: 0.723684; R: 0.723684; P: 0.756442; F1: 0.713263\n",
      "(train @ 30): L: 0.458731; A: 0.765351; R: 0.765351; P: 0.767208; F1: 0.764005\n",
      "(valid @ 30): L: 0.538710; A: 0.745614; R: 0.745614; P: 0.759395; F1: 0.742040\n",
      "Best val Metric 0.749045 @ 28\n",
      "\n",
      "models are saved @ ./predictions/211209071503/flvl_2_3_5_6_8_9_pretrained_model_c1.pt\n",
      "Saving predictions @ ./predictions/211209071503/flvl_train_2_3_5_6_8_9_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209071503/flvl_valid_1_4_7_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209071503/flvl_public_test_trained_on_2_3_5_6_8_9_r21d_rgb.csv\n",
      "Average of Best Metrics on Each Valid Set: 0.751449, 211209071503\n",
      "Saved test results @ ./predictions/211209071503/flvl_public_test_agg_r21d_rgb.csv\n"
     ]
    }
   ],
   "source": [
    "# Combined\n",
    "from main_stacked_feature import Config, run_kfold\n",
    "\n",
    "# define architecture and run k-fold training\n",
    "cfg = Config()\n",
    "cfg.assign_variable('task', 'flvl')\n",
    "cfg.assign_variable('output_dim', 3)\n",
    "cfg.assign_variable('model_type', 'GRU')\n",
    "cfg.assign_variable('bi_dir', False)\n",
    "cfg.assign_variable('device', 'cuda:0')\n",
    "cfg.assign_variable('data_root', './r21d_rgb_features')\n",
    "cfg.assign_variable('drop_p', 0.0) # results will be irreproducible\n",
    "cfg.assign_variable('batch_size', 64)\n",
    "cfg.assign_variable('input_dim', 512*4)\n",
    "cfg.assign_variable('hidden_dim', 512)\n",
    "cfg.assign_variable('n_layers', 3)\n",
    "cfg.assign_variable('num_epochs', 30)\n",
    "cfg.assign_variable('seed', 1337)\n",
    "\n",
    "run_kfold(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### E. Experiments with Single View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': [1, 2, 4, 5, 7, 8], 'valid': [3, 6, 9], 'public_test': [10, 11, 12], 'private_test': [13, 14, 15]}\n",
      "(train @ 1): L: 0.999714; A: 0.464912; R: 0.464912; P: 0.467617; F1: 0.462918\n",
      "(valid @ 1): L: 1.016992; A: 0.451754; R: 0.451754; P: 0.522825; F1: 0.388362\n",
      "(train @ 2): L: 0.935492; A: 0.517544; R: 0.517544; P: 0.623366; F1: 0.472986\n",
      "(valid @ 2): L: 0.997290; A: 0.486842; R: 0.486842; P: 0.402961; F1: 0.438573\n",
      "(train @ 3): L: 0.934536; A: 0.554825; R: 0.554825; P: 0.566989; F1: 0.537352\n",
      "(valid @ 3): L: 1.008171; A: 0.478070; R: 0.478070; P: 0.588479; F1: 0.421609\n",
      "(train @ 4): L: 0.911960; A: 0.464912; R: 0.464912; P: 0.554558; F1: 0.429154\n",
      "(valid @ 4): L: 0.926567; A: 0.578947; R: 0.578947; P: 0.646100; F1: 0.567046\n",
      "(train @ 5): L: 0.885413; A: 0.546053; R: 0.546053; P: 0.726009; F1: 0.490996\n",
      "(valid @ 5): L: 0.919153; A: 0.495614; R: 0.495614; P: 0.584881; F1: 0.391504\n",
      "(train @ 6): L: 0.935843; A: 0.486842; R: 0.486842; P: 0.531850; F1: 0.445827\n",
      "(valid @ 6): L: 0.956341; A: 0.530702; R: 0.530702; P: 0.614974; F1: 0.492467\n",
      "(train @ 7): L: 0.934333; A: 0.548246; R: 0.548246; P: 0.542810; F1: 0.488264\n",
      "(valid @ 7): L: 0.890673; A: 0.644737; R: 0.644737; P: 0.649289; F1: 0.642632\n",
      "(train @ 8): L: 0.819363; A: 0.561404; R: 0.561404; P: 0.581689; F1: 0.533768\n",
      "(valid @ 8): L: 0.910769; A: 0.631579; R: 0.631579; P: 0.633562; F1: 0.623248\n",
      "(train @ 9): L: 0.781999; A: 0.631579; R: 0.631579; P: 0.697000; F1: 0.616936\n",
      "(valid @ 9): L: 0.832781; A: 0.570175; R: 0.570175; P: 0.612716; F1: 0.572829\n",
      "(train @ 10): L: 0.753119; A: 0.605263; R: 0.605263; P: 0.628720; F1: 0.600296\n",
      "(valid @ 10): L: 0.784039; A: 0.618421; R: 0.618421; P: 0.623008; F1: 0.604307\n",
      "(train @ 11): L: 0.693722; A: 0.657895; R: 0.657895; P: 0.688527; F1: 0.652674\n",
      "(valid @ 11): L: 0.755935; A: 0.644737; R: 0.644737; P: 0.643267; F1: 0.640451\n",
      "(train @ 12): L: 0.682272; A: 0.668860; R: 0.668860; P: 0.676054; F1: 0.667049\n",
      "(valid @ 12): L: 0.847746; A: 0.583333; R: 0.583333; P: 0.585049; F1: 0.561454\n",
      "(train @ 13): L: 0.768513; A: 0.616228; R: 0.616228; P: 0.706847; F1: 0.586988\n",
      "(valid @ 13): L: 0.709734; A: 0.644737; R: 0.644737; P: 0.642551; F1: 0.642193\n",
      "(train @ 14): L: 0.671458; A: 0.699561; R: 0.699561; P: 0.711774; F1: 0.697997\n",
      "(valid @ 14): L: 0.699360; A: 0.649123; R: 0.649123; P: 0.647262; F1: 0.644826\n",
      "(train @ 15): L: 0.666000; A: 0.690789; R: 0.690789; P: 0.717474; F1: 0.691231\n",
      "(valid @ 15): L: 0.774895; A: 0.600877; R: 0.600877; P: 0.605798; F1: 0.581810\n",
      "(train @ 16): L: 0.707266; A: 0.638158; R: 0.638158; P: 0.668861; F1: 0.629490\n",
      "(valid @ 16): L: 0.715045; A: 0.627193; R: 0.627193; P: 0.669459; F1: 0.600591\n",
      "(train @ 17): L: 0.630878; A: 0.697368; R: 0.697368; P: 0.700785; F1: 0.695694\n",
      "(valid @ 17): L: 0.781603; A: 0.596491; R: 0.596491; P: 0.600963; F1: 0.576282\n",
      "(train @ 18): L: 0.695861; A: 0.673246; R: 0.673246; P: 0.734551; F1: 0.655471\n",
      "(valid @ 18): L: 0.682860; A: 0.653509; R: 0.653509; P: 0.687247; F1: 0.639301\n",
      "(train @ 19): L: 0.619974; A: 0.739035; R: 0.739035; P: 0.750224; F1: 0.735714\n",
      "(valid @ 19): L: 0.659498; A: 0.692982; R: 0.692982; P: 0.702093; F1: 0.690355\n",
      "(train @ 20): L: 0.601098; A: 0.741228; R: 0.741228; P: 0.787165; F1: 0.738059\n",
      "(valid @ 20): L: 0.721050; A: 0.675439; R: 0.675439; P: 0.682300; F1: 0.672818\n",
      "(train @ 21): L: 0.608762; A: 0.697368; R: 0.697368; P: 0.698477; F1: 0.697024\n",
      "(valid @ 21): L: 0.759917; A: 0.614035; R: 0.614035; P: 0.632765; F1: 0.593537\n",
      "(train @ 22): L: 0.668509; A: 0.677632; R: 0.677632; P: 0.721804; F1: 0.663228\n",
      "(valid @ 22): L: 0.688104; A: 0.614035; R: 0.614035; P: 0.652155; F1: 0.585612\n",
      "(train @ 23): L: 0.596147; A: 0.719298; R: 0.719298; P: 0.725318; F1: 0.719096\n",
      "(valid @ 23): L: 0.639456; A: 0.662281; R: 0.662281; P: 0.671183; F1: 0.658249\n",
      "(train @ 24): L: 0.585167; A: 0.730263; R: 0.730263; P: 0.758347; F1: 0.732046\n",
      "(valid @ 24): L: 0.676061; A: 0.666667; R: 0.666667; P: 0.682394; F1: 0.665872\n",
      "(train @ 25): L: 0.552370; A: 0.739035; R: 0.739035; P: 0.752187; F1: 0.737469\n",
      "(valid @ 25): L: 0.645240; A: 0.657895; R: 0.657895; P: 0.670147; F1: 0.656772\n",
      "(train @ 26): L: 0.510675; A: 0.767544; R: 0.767544; P: 0.784469; F1: 0.764975\n",
      "(valid @ 26): L: 0.669093; A: 0.657895; R: 0.657895; P: 0.683401; F1: 0.652632\n",
      "(train @ 27): L: 0.493407; A: 0.782895; R: 0.782895; P: 0.794772; F1: 0.782507\n",
      "(valid @ 27): L: 0.705659; A: 0.653509; R: 0.653509; P: 0.706916; F1: 0.651140\n",
      "(train @ 28): L: 0.497601; A: 0.763158; R: 0.763158; P: 0.775894; F1: 0.762966\n",
      "(valid @ 28): L: 0.749573; A: 0.644737; R: 0.644737; P: 0.686713; F1: 0.643976\n",
      "(train @ 29): L: 0.475377; A: 0.778509; R: 0.778509; P: 0.792676; F1: 0.777584\n",
      "(valid @ 29): L: 0.742112; A: 0.657895; R: 0.657895; P: 0.712680; F1: 0.659367\n",
      "(train @ 30): L: 0.471280; A: 0.793860; R: 0.793860; P: 0.798832; F1: 0.793836\n",
      "(valid @ 30): L: 0.783562; A: 0.640351; R: 0.640351; P: 0.708917; F1: 0.636372\n",
      "Best val Metric 0.690355 @ 19\n",
      "\n",
      "models are saved @ ./predictions/211209073813/flvl_1_2_4_5_7_8_pretrained_model_c1.pt\n",
      "Saving predictions @ ./predictions/211209073813/flvl_train_1_2_4_5_7_8_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209073813/flvl_valid_3_6_9_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209073813/flvl_public_test_trained_on_1_2_4_5_7_8_r21d_rgb.csv\n",
      "{'train': [1, 3, 4, 6, 7, 9], 'valid': [2, 5, 8], 'public_test': [10, 11, 12], 'private_test': [13, 14, 15]}\n",
      "(train @ 1): L: 1.009672; A: 0.458333; R: 0.458333; P: 0.457177; F1: 0.456320\n",
      "(valid @ 1): L: 0.981326; A: 0.421053; R: 0.421053; P: 0.180451; F1: 0.252632\n",
      "(train @ 2): L: 0.954229; A: 0.526316; R: 0.526316; P: 0.619069; F1: 0.481510\n",
      "(valid @ 2): L: 0.984835; A: 0.535088; R: 0.535088; P: 0.439622; F1: 0.473327\n",
      "(train @ 3): L: 0.936461; A: 0.515351; R: 0.515351; P: 0.541594; F1: 0.498520\n",
      "(valid @ 3): L: 0.878259; A: 0.596491; R: 0.596491; P: 0.666382; F1: 0.579289\n",
      "(train @ 4): L: 0.960610; A: 0.475877; R: 0.475877; P: 0.565302; F1: 0.448793\n",
      "(valid @ 4): L: 0.887192; A: 0.574561; R: 0.574561; P: 0.640357; F1: 0.537274\n",
      "(train @ 5): L: 0.906645; A: 0.550439; R: 0.550439; P: 0.687273; F1: 0.501284\n",
      "(valid @ 5): L: 0.957451; A: 0.451754; R: 0.451754; P: 0.386261; F1: 0.314731\n",
      "(train @ 6): L: 0.893564; A: 0.506579; R: 0.506579; P: 0.554981; F1: 0.451681\n",
      "(valid @ 6): L: 0.834077; A: 0.583333; R: 0.583333; P: 0.651998; F1: 0.570926\n",
      "(train @ 7): L: 0.869211; A: 0.526316; R: 0.526316; P: 0.642296; F1: 0.474186\n",
      "(valid @ 7): L: 0.818429; A: 0.570175; R: 0.570175; P: 0.705263; F1: 0.493863\n",
      "(train @ 8): L: 0.827443; A: 0.495614; R: 0.495614; P: 0.520582; F1: 0.411191\n",
      "(valid @ 8): L: 0.761195; A: 0.666667; R: 0.666667; P: 0.668442; F1: 0.661468\n",
      "(train @ 9): L: 0.792175; A: 0.585526; R: 0.585526; P: 0.686057; F1: 0.549083\n",
      "(valid @ 9): L: 0.788868; A: 0.640351; R: 0.640351; P: 0.674895; F1: 0.636901\n",
      "(train @ 10): L: 0.754116; A: 0.572368; R: 0.572368; P: 0.573090; F1: 0.569897\n",
      "(valid @ 10): L: 0.709252; A: 0.701754; R: 0.701754; P: 0.718757; F1: 0.703779\n",
      "(train @ 11): L: 0.726495; A: 0.625000; R: 0.625000; P: 0.700603; F1: 0.596142\n",
      "(valid @ 11): L: 0.753782; A: 0.657895; R: 0.657895; P: 0.684915; F1: 0.654911\n",
      "(train @ 12): L: 0.738006; A: 0.589912; R: 0.589912; P: 0.594975; F1: 0.583184\n",
      "(valid @ 12): L: 0.684634; A: 0.706140; R: 0.706140; P: 0.704761; F1: 0.704440\n",
      "(train @ 13): L: 0.761973; A: 0.598684; R: 0.598684; P: 0.689834; F1: 0.560247\n",
      "(valid @ 13): L: 0.674377; A: 0.719298; R: 0.719298; P: 0.732441; F1: 0.722315\n",
      "(train @ 14): L: 0.725036; A: 0.600877; R: 0.600877; P: 0.605743; F1: 0.595831\n",
      "(valid @ 14): L: 0.694933; A: 0.653509; R: 0.653509; P: 0.687006; F1: 0.643010\n",
      "(train @ 15): L: 0.694242; A: 0.649123; R: 0.649123; P: 0.671692; F1: 0.640903\n",
      "(valid @ 15): L: 0.635735; A: 0.688596; R: 0.688596; P: 0.706317; F1: 0.690614\n",
      "(train @ 16): L: 0.701482; A: 0.657895; R: 0.657895; P: 0.691247; F1: 0.648410\n",
      "(valid @ 16): L: 0.633064; A: 0.723684; R: 0.723684; P: 0.734235; F1: 0.721983\n",
      "(train @ 17): L: 0.672759; A: 0.664474; R: 0.664474; P: 0.667137; F1: 0.662155\n",
      "(valid @ 17): L: 0.634662; A: 0.666667; R: 0.666667; P: 0.675386; F1: 0.661327\n",
      "(train @ 18): L: 0.734138; A: 0.600877; R: 0.600877; P: 0.649523; F1: 0.572479\n",
      "(valid @ 18): L: 0.635689; A: 0.723684; R: 0.723684; P: 0.735589; F1: 0.722746\n",
      "(train @ 19): L: 0.665115; A: 0.697368; R: 0.697368; P: 0.699043; F1: 0.696194\n",
      "(valid @ 19): L: 0.663356; A: 0.684211; R: 0.684211; P: 0.711639; F1: 0.679933\n",
      "(train @ 20): L: 0.648461; A: 0.690789; R: 0.690789; P: 0.718682; F1: 0.683875\n",
      "(valid @ 20): L: 0.601753; A: 0.745614; R: 0.745614; P: 0.760682; F1: 0.746703\n",
      "(train @ 21): L: 0.639836; A: 0.714912; R: 0.714912; P: 0.717824; F1: 0.715240\n",
      "(valid @ 21): L: 0.681505; A: 0.618421; R: 0.618421; P: 0.634494; F1: 0.595867\n",
      "(train @ 22): L: 0.732242; A: 0.644737; R: 0.644737; P: 0.677387; F1: 0.633959\n",
      "(valid @ 22): L: 0.638791; A: 0.679825; R: 0.679825; P: 0.697777; F1: 0.681661\n",
      "(train @ 23): L: 0.668832; A: 0.682018; R: 0.682018; P: 0.687597; F1: 0.678819\n",
      "(valid @ 23): L: 0.657907; A: 0.710526; R: 0.710526; P: 0.738917; F1: 0.712517\n",
      "(train @ 24): L: 0.631723; A: 0.708333; R: 0.708333; P: 0.735600; F1: 0.704762\n",
      "(valid @ 24): L: 0.588939; A: 0.714912; R: 0.714912; P: 0.729026; F1: 0.718054\n",
      "(train @ 25): L: 0.637360; A: 0.666667; R: 0.666667; P: 0.681631; F1: 0.661627\n",
      "(valid @ 25): L: 0.568439; A: 0.741228; R: 0.741228; P: 0.760415; F1: 0.744057\n",
      "(train @ 26): L: 0.591143; A: 0.732456; R: 0.732456; P: 0.739666; F1: 0.730191\n",
      "(valid @ 26): L: 0.577031; A: 0.750000; R: 0.750000; P: 0.778253; F1: 0.751785\n",
      "(train @ 27): L: 0.574094; A: 0.736842; R: 0.736842; P: 0.745214; F1: 0.735929\n",
      "(valid @ 27): L: 0.619902; A: 0.697368; R: 0.697368; P: 0.799620; F1: 0.681277\n",
      "(train @ 28): L: 0.621536; A: 0.690789; R: 0.690789; P: 0.707941; F1: 0.684430\n",
      "(valid @ 28): L: 0.548261; A: 0.750000; R: 0.750000; P: 0.767404; F1: 0.751401\n",
      "(train @ 29): L: 0.586863; A: 0.725877; R: 0.725877; P: 0.728015; F1: 0.725519\n",
      "(valid @ 29): L: 0.567791; A: 0.723684; R: 0.723684; P: 0.788907; F1: 0.712317\n",
      "(train @ 30): L: 0.592605; A: 0.679825; R: 0.679825; P: 0.680003; F1: 0.679729\n",
      "(valid @ 30): L: 0.588363; A: 0.701754; R: 0.701754; P: 0.742331; F1: 0.690531\n",
      "Best val Metric 0.751785 @ 26\n",
      "\n",
      "models are saved @ ./predictions/211209073813/flvl_1_3_4_6_7_9_pretrained_model_c1.pt\n",
      "Saving predictions @ ./predictions/211209073813/flvl_train_1_3_4_6_7_9_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209073813/flvl_valid_2_5_8_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209073813/flvl_public_test_trained_on_1_3_4_6_7_9_r21d_rgb.csv\n",
      "{'train': [2, 3, 5, 6, 8, 9], 'valid': [1, 4, 7], 'public_test': [10, 11, 12], 'private_test': [13, 14, 15]}\n",
      "(train @ 1): L: 1.012419; A: 0.438596; R: 0.438596; P: 0.440795; F1: 0.436164\n",
      "(valid @ 1): L: 0.994608; A: 0.425439; R: 0.425439; P: 0.337544; F1: 0.260379\n",
      "(train @ 2): L: 0.967480; A: 0.482456; R: 0.482456; P: 0.577199; F1: 0.438364\n",
      "(valid @ 2): L: 0.999502; A: 0.552632; R: 0.552632; P: 0.460161; F1: 0.492251\n",
      "(train @ 3): L: 0.948415; A: 0.475877; R: 0.475877; P: 0.559360; F1: 0.441527\n",
      "(valid @ 3): L: 0.942346; A: 0.592105; R: 0.592105; P: 0.643693; F1: 0.583402\n",
      "(train @ 4): L: 1.003746; A: 0.451754; R: 0.451754; P: 0.486212; F1: 0.427219\n",
      "(valid @ 4): L: 0.943254; A: 0.557018; R: 0.557018; P: 0.622180; F1: 0.511394\n",
      "(train @ 5): L: 0.913821; A: 0.541667; R: 0.541667; P: 0.640220; F1: 0.503076\n",
      "(valid @ 5): L: 0.908214; A: 0.578947; R: 0.578947; P: 0.641440; F1: 0.554570\n",
      "(train @ 6): L: 0.867960; A: 0.500000; R: 0.500000; P: 0.570897; F1: 0.436609\n",
      "(valid @ 6): L: 0.851320; A: 0.609649; R: 0.609649; P: 0.660412; F1: 0.605921\n",
      "(train @ 7): L: 0.885991; A: 0.530702; R: 0.530702; P: 0.637973; F1: 0.482008\n",
      "(valid @ 7): L: 0.836194; A: 0.635965; R: 0.635965; P: 0.666971; F1: 0.637184\n",
      "(train @ 8): L: 0.839286; A: 0.469298; R: 0.469298; P: 0.483493; F1: 0.404092\n",
      "(valid @ 8): L: 0.797851; A: 0.635965; R: 0.635965; P: 0.649262; F1: 0.636156\n",
      "(train @ 9): L: 0.798670; A: 0.565789; R: 0.565789; P: 0.676228; F1: 0.518545\n",
      "(valid @ 9): L: 0.815396; A: 0.627193; R: 0.627193; P: 0.674096; F1: 0.624102\n",
      "(train @ 10): L: 0.744545; A: 0.609649; R: 0.609649; P: 0.620322; F1: 0.602930\n",
      "(valid @ 10): L: 0.782730; A: 0.649123; R: 0.649123; P: 0.675002; F1: 0.652511\n",
      "(train @ 11): L: 0.722856; A: 0.629386; R: 0.629386; P: 0.665367; F1: 0.612646\n",
      "(valid @ 11): L: 0.853810; A: 0.631579; R: 0.631579; P: 0.668645; F1: 0.631285\n",
      "(train @ 12): L: 0.736896; A: 0.611842; R: 0.611842; P: 0.611079; F1: 0.610296\n",
      "(valid @ 12): L: 0.724807; A: 0.675439; R: 0.675439; P: 0.692620; F1: 0.678821\n",
      "(train @ 13): L: 0.713063; A: 0.616228; R: 0.616228; P: 0.678598; F1: 0.588879\n",
      "(valid @ 13): L: 0.737926; A: 0.666667; R: 0.666667; P: 0.691919; F1: 0.669710\n",
      "(train @ 14): L: 0.683474; A: 0.640351; R: 0.640351; P: 0.639024; F1: 0.635927\n",
      "(valid @ 14): L: 0.755225; A: 0.675439; R: 0.675439; P: 0.697954; F1: 0.678492\n",
      "(train @ 15): L: 0.683095; A: 0.640351; R: 0.640351; P: 0.681830; F1: 0.620836\n",
      "(valid @ 15): L: 0.755162; A: 0.679825; R: 0.679825; P: 0.706178; F1: 0.681901\n",
      "(train @ 16): L: 0.676548; A: 0.655702; R: 0.655702; P: 0.683020; F1: 0.649160\n",
      "(valid @ 16): L: 0.686678; A: 0.697368; R: 0.697368; P: 0.717674; F1: 0.700123\n",
      "(train @ 17): L: 0.676182; A: 0.660088; R: 0.660088; P: 0.663819; F1: 0.657256\n",
      "(valid @ 17): L: 0.698865; A: 0.675439; R: 0.675439; P: 0.706401; F1: 0.674539\n",
      "(train @ 18): L: 0.725472; A: 0.607456; R: 0.607456; P: 0.626030; F1: 0.593060\n",
      "(valid @ 18): L: 0.696666; A: 0.714912; R: 0.714912; P: 0.733740; F1: 0.717145\n",
      "(train @ 19): L: 0.664237; A: 0.675439; R: 0.675439; P: 0.677399; F1: 0.672108\n",
      "(valid @ 19): L: 0.749061; A: 0.701754; R: 0.701754; P: 0.724839; F1: 0.701992\n",
      "(train @ 20): L: 0.644155; A: 0.675439; R: 0.675439; P: 0.694974; F1: 0.667569\n",
      "(valid @ 20): L: 0.694949; A: 0.723684; R: 0.723684; P: 0.741325; F1: 0.724773\n",
      "(train @ 21): L: 0.632835; A: 0.671053; R: 0.671053; P: 0.676662; F1: 0.671679\n",
      "(valid @ 21): L: 0.697097; A: 0.614035; R: 0.614035; P: 0.670717; F1: 0.581601\n",
      "(train @ 22): L: 0.704726; A: 0.629386; R: 0.629386; P: 0.684546; F1: 0.604216\n",
      "(valid @ 22): L: 0.718311; A: 0.675439; R: 0.675439; P: 0.697539; F1: 0.676792\n",
      "(train @ 23): L: 0.655699; A: 0.668860; R: 0.668860; P: 0.688167; F1: 0.659197\n",
      "(valid @ 23): L: 0.736529; A: 0.701754; R: 0.701754; P: 0.747964; F1: 0.699250\n",
      "(train @ 24): L: 0.625226; A: 0.662281; R: 0.662281; P: 0.703546; F1: 0.653719\n",
      "(valid @ 24): L: 0.737995; A: 0.627193; R: 0.627193; P: 0.657787; F1: 0.622168\n",
      "(train @ 25): L: 0.651317; A: 0.653509; R: 0.653509; P: 0.667041; F1: 0.646836\n",
      "(valid @ 25): L: 0.655937; A: 0.736842; R: 0.736842; P: 0.761489; F1: 0.736745\n",
      "(train @ 26): L: 0.591793; A: 0.710526; R: 0.710526; P: 0.716704; F1: 0.708753\n",
      "(valid @ 26): L: 0.637169; A: 0.741228; R: 0.741228; P: 0.771146; F1: 0.740716\n",
      "(train @ 27): L: 0.565487; A: 0.732456; R: 0.732456; P: 0.747566; F1: 0.731802\n",
      "(valid @ 27): L: 0.627530; A: 0.684211; R: 0.684211; P: 0.743441; F1: 0.673546\n",
      "(train @ 28): L: 0.603884; A: 0.710526; R: 0.710526; P: 0.726194; F1: 0.706059\n",
      "(valid @ 28): L: 0.616889; A: 0.728070; R: 0.728070; P: 0.767274; F1: 0.726495\n",
      "(train @ 29): L: 0.548097; A: 0.739035; R: 0.739035; P: 0.741612; F1: 0.738089\n",
      "(valid @ 29): L: 0.617355; A: 0.719298; R: 0.719298; P: 0.795472; F1: 0.708225\n",
      "(train @ 30): L: 0.581758; A: 0.688596; R: 0.688596; P: 0.692049; F1: 0.687908\n",
      "(valid @ 30): L: 0.666993; A: 0.666667; R: 0.666667; P: 0.685621; F1: 0.656425\n",
      "Best val Metric 0.740716 @ 26\n",
      "\n",
      "models are saved @ ./predictions/211209073813/flvl_2_3_5_6_8_9_pretrained_model_c1.pt\n",
      "Saving predictions @ ./predictions/211209073813/flvl_train_2_3_5_6_8_9_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209073813/flvl_valid_1_4_7_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209073813/flvl_public_test_trained_on_2_3_5_6_8_9_r21d_rgb.csv\n",
      "Average of Best Metrics on Each Valid Set: 0.727619, 211209073813\n",
      "Saved test results @ ./predictions/211209073813/flvl_public_test_agg_r21d_rgb.csv\n"
     ]
    }
   ],
   "source": [
    "# View 1\n",
    "from main_1View import Config, run_kfold\n",
    "\n",
    "# define architecture and run k-fold training\n",
    "cfg = Config()\n",
    "cfg.assign_variable('task', 'flvl')\n",
    "cfg.assign_variable('output_dim', 3)\n",
    "cfg.assign_variable('model_type', 'GRU')\n",
    "cfg.assign_variable('bi_dir', False)\n",
    "cfg.assign_variable('device', 'cuda:0')\n",
    "cfg.assign_variable('data_root', './r21d_rgb_features')\n",
    "cfg.assign_variable('drop_p', 0.0) # results will be irreproducible\n",
    "cfg.assign_variable('batch_size', 64)\n",
    "cfg.assign_variable('input_dim', 512)\n",
    "cfg.assign_variable('hidden_dim', 512)\n",
    "cfg.assign_variable('n_layers', 3)\n",
    "cfg.assign_variable('num_epochs', 30)\n",
    "cfg.assign_variable('seed', 1337)\n",
    "\n",
    "run_kfold(cfg,[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': [1, 2, 4, 5, 7, 8], 'valid': [3, 6, 9], 'public_test': [10, 11, 12], 'private_test': [13, 14, 15]}\n",
      "(train @ 1): L: 1.005212; A: 0.453947; R: 0.453947; P: 0.459158; F1: 0.451558\n",
      "(valid @ 1): L: 1.023810; A: 0.473684; R: 0.473684; P: 0.581555; F1: 0.404932\n",
      "(train @ 2): L: 0.940467; A: 0.517544; R: 0.517544; P: 0.603719; F1: 0.481963\n",
      "(valid @ 2): L: 1.002155; A: 0.482456; R: 0.482456; P: 0.404444; F1: 0.439887\n",
      "(train @ 3): L: 0.933872; A: 0.557018; R: 0.557018; P: 0.580306; F1: 0.536431\n",
      "(valid @ 3): L: 1.017276; A: 0.500000; R: 0.500000; P: 0.598963; F1: 0.458602\n",
      "(train @ 4): L: 0.916012; A: 0.473684; R: 0.473684; P: 0.565865; F1: 0.436487\n",
      "(valid @ 4): L: 0.940686; A: 0.574561; R: 0.574561; P: 0.646926; F1: 0.559949\n",
      "(train @ 5): L: 0.891855; A: 0.537281; R: 0.537281; P: 0.722997; F1: 0.476468\n",
      "(valid @ 5): L: 0.926526; A: 0.495614; R: 0.495614; P: 0.604701; F1: 0.391514\n",
      "(train @ 6): L: 0.930269; A: 0.500000; R: 0.500000; P: 0.550232; F1: 0.458159\n",
      "(valid @ 6): L: 0.946708; A: 0.548246; R: 0.548246; P: 0.630060; F1: 0.513385\n",
      "(train @ 7): L: 0.919605; A: 0.543860; R: 0.543860; P: 0.539945; F1: 0.482958\n",
      "(valid @ 7): L: 0.889076; A: 0.622807; R: 0.622807; P: 0.656688; F1: 0.621730\n",
      "(train @ 8): L: 0.815717; A: 0.567982; R: 0.567982; P: 0.567688; F1: 0.554250\n",
      "(valid @ 8): L: 0.926120; A: 0.614035; R: 0.614035; P: 0.622787; F1: 0.609844\n",
      "(train @ 9): L: 0.782462; A: 0.625000; R: 0.625000; P: 0.677897; F1: 0.614955\n",
      "(valid @ 9): L: 0.864477; A: 0.583333; R: 0.583333; P: 0.643828; F1: 0.573620\n",
      "(train @ 10): L: 0.765383; A: 0.587719; R: 0.587719; P: 0.626546; F1: 0.580984\n",
      "(valid @ 10): L: 0.810933; A: 0.614035; R: 0.614035; P: 0.617544; F1: 0.598333\n",
      "(train @ 11): L: 0.716148; A: 0.640351; R: 0.640351; P: 0.648511; F1: 0.639221\n",
      "(valid @ 11): L: 0.764822; A: 0.649123; R: 0.649123; P: 0.653620; F1: 0.649581\n",
      "(train @ 12): L: 0.680789; A: 0.677632; R: 0.677632; P: 0.693239; F1: 0.674182\n",
      "(valid @ 12): L: 0.800061; A: 0.609649; R: 0.609649; P: 0.611892; F1: 0.591617\n",
      "(train @ 13): L: 0.721468; A: 0.625000; R: 0.625000; P: 0.660184; F1: 0.608945\n",
      "(valid @ 13): L: 0.724697; A: 0.653509; R: 0.653509; P: 0.653514; F1: 0.648048\n",
      "(train @ 14): L: 0.680308; A: 0.682018; R: 0.682018; P: 0.712038; F1: 0.676216\n",
      "(valid @ 14): L: 0.713434; A: 0.666667; R: 0.666667; P: 0.665788; F1: 0.662936\n",
      "(train @ 15): L: 0.653242; A: 0.675439; R: 0.675439; P: 0.691387; F1: 0.670996\n",
      "(valid @ 15): L: 0.727171; A: 0.644737; R: 0.644737; P: 0.646983; F1: 0.637010\n",
      "(train @ 16): L: 0.664779; A: 0.677632; R: 0.677632; P: 0.712308; F1: 0.669769\n",
      "(valid @ 16): L: 0.710847; A: 0.640351; R: 0.640351; P: 0.643307; F1: 0.635767\n",
      "(train @ 17): L: 0.625166; A: 0.723684; R: 0.723684; P: 0.730054; F1: 0.722556\n",
      "(valid @ 17): L: 0.754439; A: 0.605263; R: 0.605263; P: 0.608533; F1: 0.587813\n",
      "(train @ 18): L: 0.685305; A: 0.682018; R: 0.682018; P: 0.748295; F1: 0.663637\n",
      "(valid @ 18): L: 0.697836; A: 0.671053; R: 0.671053; P: 0.689805; F1: 0.663856\n",
      "(train @ 19): L: 0.622167; A: 0.686404; R: 0.686404; P: 0.710584; F1: 0.678256\n",
      "(valid @ 19): L: 0.687219; A: 0.666667; R: 0.666667; P: 0.674130; F1: 0.662511\n",
      "(train @ 20): L: 0.603954; A: 0.732456; R: 0.732456; P: 0.774250; F1: 0.728051\n",
      "(valid @ 20): L: 0.703486; A: 0.671053; R: 0.671053; P: 0.674765; F1: 0.668743\n",
      "(train @ 21): L: 0.586591; A: 0.734649; R: 0.734649; P: 0.736904; F1: 0.734510\n",
      "(valid @ 21): L: 0.842795; A: 0.574561; R: 0.574561; P: 0.590119; F1: 0.551425\n",
      "(train @ 22): L: 0.709445; A: 0.644737; R: 0.644737; P: 0.679071; F1: 0.633331\n",
      "(valid @ 22): L: 0.686038; A: 0.657895; R: 0.657895; P: 0.672946; F1: 0.651097\n",
      "(train @ 23): L: 0.616703; A: 0.699561; R: 0.699561; P: 0.712583; F1: 0.698683\n",
      "(valid @ 23): L: 0.694505; A: 0.653509; R: 0.653509; P: 0.663547; F1: 0.646758\n",
      "(train @ 24): L: 0.653331; A: 0.682018; R: 0.682018; P: 0.700186; F1: 0.682887\n",
      "(valid @ 24): L: 0.672502; A: 0.666667; R: 0.666667; P: 0.674514; F1: 0.662021\n",
      "(train @ 25): L: 0.564227; A: 0.717105; R: 0.717105; P: 0.722924; F1: 0.716950\n",
      "(valid @ 25): L: 0.703662; A: 0.640351; R: 0.640351; P: 0.660951; F1: 0.633368\n",
      "(train @ 26): L: 0.538003; A: 0.750000; R: 0.750000; P: 0.764271; F1: 0.746779\n",
      "(valid @ 26): L: 0.660689; A: 0.662281; R: 0.662281; P: 0.672442; F1: 0.661834\n",
      "(train @ 27): L: 0.523875; A: 0.758772; R: 0.758772; P: 0.778229; F1: 0.756578\n",
      "(valid @ 27): L: 0.732952; A: 0.640351; R: 0.640351; P: 0.709602; F1: 0.633888\n",
      "(train @ 28): L: 0.574068; A: 0.717105; R: 0.717105; P: 0.734477; F1: 0.713320\n",
      "(valid @ 28): L: 0.689985; A: 0.644737; R: 0.644737; P: 0.644173; F1: 0.641163\n",
      "(train @ 29): L: 0.538209; A: 0.736842; R: 0.736842; P: 0.749220; F1: 0.734869\n",
      "(valid @ 29): L: 0.692912; A: 0.644737; R: 0.644737; P: 0.657474; F1: 0.643582\n",
      "(train @ 30): L: 0.501153; A: 0.765351; R: 0.765351; P: 0.766964; F1: 0.765071\n",
      "(valid @ 30): L: 0.682163; A: 0.649123; R: 0.649123; P: 0.671908; F1: 0.647092\n",
      "Best val Metric 0.668743 @ 20\n",
      "\n",
      "models are saved @ ./predictions/211209080119/flvl_1_2_4_5_7_8_pretrained_model_c1.pt\n",
      "Saving predictions @ ./predictions/211209080119/flvl_train_1_2_4_5_7_8_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209080119/flvl_valid_3_6_9_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209080119/flvl_public_test_trained_on_1_2_4_5_7_8_r21d_rgb.csv\n",
      "{'train': [1, 3, 4, 6, 7, 9], 'valid': [2, 5, 8], 'public_test': [10, 11, 12], 'private_test': [13, 14, 15]}\n",
      "(train @ 1): L: 1.015457; A: 0.471491; R: 0.471491; P: 0.469649; F1: 0.469542\n",
      "(valid @ 1): L: 0.986938; A: 0.434211; R: 0.434211; P: 0.257490; F1: 0.287707\n",
      "(train @ 2): L: 0.959506; A: 0.532895; R: 0.532895; P: 0.487802; F1: 0.484579\n",
      "(valid @ 2): L: 0.997703; A: 0.535088; R: 0.535088; P: 0.440580; F1: 0.478052\n",
      "(train @ 3): L: 0.951851; A: 0.489035; R: 0.489035; P: 0.570194; F1: 0.456165\n",
      "(valid @ 3): L: 0.909024; A: 0.565789; R: 0.565789; P: 0.650517; F1: 0.538222\n",
      "(train @ 4): L: 0.970764; A: 0.471491; R: 0.471491; P: 0.569888; F1: 0.431176\n",
      "(valid @ 4): L: 0.918645; A: 0.543860; R: 0.543860; P: 0.458906; F1: 0.497747\n",
      "(train @ 5): L: 0.921533; A: 0.535088; R: 0.535088; P: 0.686433; F1: 0.477700\n",
      "(valid @ 5): L: 0.944880; A: 0.478070; R: 0.478070; P: 0.435162; F1: 0.360573\n",
      "(train @ 6): L: 0.901588; A: 0.480263; R: 0.480263; P: 0.510427; F1: 0.404038\n",
      "(valid @ 6): L: 0.857058; A: 0.578947; R: 0.578947; P: 0.656735; F1: 0.557863\n",
      "(train @ 7): L: 0.892836; A: 0.517544; R: 0.517544; P: 0.645544; F1: 0.461168\n",
      "(valid @ 7): L: 0.816394; A: 0.622807; R: 0.622807; P: 0.649958; F1: 0.618119\n",
      "(train @ 8): L: 0.843539; A: 0.460526; R: 0.460526; P: 0.459777; F1: 0.394427\n",
      "(valid @ 8): L: 0.776498; A: 0.697368; R: 0.697368; P: 0.696079; F1: 0.694572\n",
      "(train @ 9): L: 0.796141; A: 0.581140; R: 0.581140; P: 0.672880; F1: 0.550795\n",
      "(valid @ 9): L: 0.787934; A: 0.657895; R: 0.657895; P: 0.692591; F1: 0.654666\n",
      "(train @ 10): L: 0.759265; A: 0.587719; R: 0.587719; P: 0.593604; F1: 0.585931\n",
      "(valid @ 10): L: 0.701293; A: 0.710526; R: 0.710526; P: 0.717874; F1: 0.712586\n",
      "(train @ 11): L: 0.732073; A: 0.629386; R: 0.629386; P: 0.688435; F1: 0.610012\n",
      "(valid @ 11): L: 0.750950; A: 0.640351; R: 0.640351; P: 0.670565; F1: 0.633546\n",
      "(train @ 12): L: 0.738135; A: 0.576754; R: 0.576754; P: 0.575341; F1: 0.572490\n",
      "(valid @ 12): L: 0.687927; A: 0.697368; R: 0.697368; P: 0.696562; F1: 0.694237\n",
      "(train @ 13): L: 0.757470; A: 0.589912; R: 0.589912; P: 0.663236; F1: 0.550505\n",
      "(valid @ 13): L: 0.668033; A: 0.714912; R: 0.714912; P: 0.721680; F1: 0.716984\n",
      "(train @ 14): L: 0.735240; A: 0.596491; R: 0.596491; P: 0.600002; F1: 0.591062\n",
      "(valid @ 14): L: 0.680005; A: 0.706140; R: 0.706140; P: 0.730038; F1: 0.703942\n",
      "(train @ 15): L: 0.701811; A: 0.638158; R: 0.638158; P: 0.676194; F1: 0.624091\n",
      "(valid @ 15): L: 0.631572; A: 0.728070; R: 0.728070; P: 0.730463; F1: 0.728944\n",
      "(train @ 16): L: 0.707843; A: 0.649123; R: 0.649123; P: 0.671902; F1: 0.641914\n",
      "(valid @ 16): L: 0.640673; A: 0.719298; R: 0.719298; P: 0.731140; F1: 0.717093\n",
      "(train @ 17): L: 0.676354; A: 0.675439; R: 0.675439; P: 0.683478; F1: 0.672820\n",
      "(valid @ 17): L: 0.638563; A: 0.671053; R: 0.671053; P: 0.679391; F1: 0.666216\n",
      "(train @ 18): L: 0.733285; A: 0.596491; R: 0.596491; P: 0.625349; F1: 0.579297\n",
      "(valid @ 18): L: 0.643816; A: 0.701754; R: 0.701754; P: 0.711582; F1: 0.698919\n",
      "(train @ 19): L: 0.677329; A: 0.668860; R: 0.668860; P: 0.669777; F1: 0.666443\n",
      "(valid @ 19): L: 0.660096; A: 0.706140; R: 0.706140; P: 0.724138; F1: 0.706191\n",
      "(train @ 20): L: 0.652752; A: 0.708333; R: 0.708333; P: 0.725797; F1: 0.705463\n",
      "(valid @ 20): L: 0.597424; A: 0.736842; R: 0.736842; P: 0.749391; F1: 0.738342\n",
      "(train @ 21): L: 0.641180; A: 0.719298; R: 0.719298; P: 0.724520; F1: 0.719047\n",
      "(valid @ 21): L: 0.730943; A: 0.600877; R: 0.600877; P: 0.661319; F1: 0.562791\n",
      "(train @ 22): L: 0.738510; A: 0.635965; R: 0.635965; P: 0.660936; F1: 0.626042\n",
      "(valid @ 22): L: 0.649119; A: 0.728070; R: 0.728070; P: 0.742626; F1: 0.730494\n",
      "(train @ 23): L: 0.661184; A: 0.688596; R: 0.688596; P: 0.694334; F1: 0.686485\n",
      "(valid @ 23): L: 0.690741; A: 0.697368; R: 0.697368; P: 0.717432; F1: 0.699199\n",
      "(train @ 24): L: 0.661712; A: 0.692982; R: 0.692982; P: 0.699748; F1: 0.692542\n",
      "(valid @ 24): L: 0.575007; A: 0.741228; R: 0.741228; P: 0.754517; F1: 0.744250\n",
      "(train @ 25): L: 0.634746; A: 0.695175; R: 0.695175; P: 0.707529; F1: 0.691776\n",
      "(valid @ 25): L: 0.554902; A: 0.763158; R: 0.763158; P: 0.771430; F1: 0.764670\n",
      "(train @ 26): L: 0.598908; A: 0.697368; R: 0.697368; P: 0.703208; F1: 0.696334\n",
      "(valid @ 26): L: 0.577090; A: 0.741228; R: 0.741228; P: 0.785254; F1: 0.740751\n",
      "(train @ 27): L: 0.584070; A: 0.717105; R: 0.717105; P: 0.721942; F1: 0.716410\n",
      "(valid @ 27): L: 0.650449; A: 0.662281; R: 0.662281; P: 0.741123; F1: 0.638260\n",
      "(train @ 28): L: 0.658538; A: 0.668860; R: 0.668860; P: 0.682417; F1: 0.662588\n",
      "(valid @ 28): L: 0.603883; A: 0.706140; R: 0.706140; P: 0.712916; F1: 0.707858\n",
      "(train @ 29): L: 0.604879; A: 0.712719; R: 0.712719; P: 0.717560; F1: 0.711388\n",
      "(valid @ 29): L: 0.563985; A: 0.728070; R: 0.728070; P: 0.776739; F1: 0.723457\n",
      "(train @ 30): L: 0.582026; A: 0.706140; R: 0.706140; P: 0.707284; F1: 0.704275\n",
      "(valid @ 30): L: 0.564222; A: 0.728070; R: 0.728070; P: 0.749457; F1: 0.725949\n",
      "Best val Metric 0.764670 @ 25\n",
      "\n",
      "models are saved @ ./predictions/211209080119/flvl_1_3_4_6_7_9_pretrained_model_c1.pt\n",
      "Saving predictions @ ./predictions/211209080119/flvl_train_1_3_4_6_7_9_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209080119/flvl_valid_2_5_8_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209080119/flvl_public_test_trained_on_1_3_4_6_7_9_r21d_rgb.csv\n",
      "{'train': [2, 3, 5, 6, 8, 9], 'valid': [1, 4, 7], 'public_test': [10, 11, 12], 'private_test': [13, 14, 15]}\n",
      "(train @ 1): L: 1.014981; A: 0.451754; R: 0.451754; P: 0.455918; F1: 0.449337\n",
      "(valid @ 1): L: 0.991166; A: 0.434211; R: 0.434211; P: 0.341627; F1: 0.280121\n",
      "(train @ 2): L: 0.973112; A: 0.478070; R: 0.478070; P: 0.417187; F1: 0.430684\n",
      "(valid @ 2): L: 1.001869; A: 0.535088; R: 0.535088; P: 0.446375; F1: 0.477081\n",
      "(train @ 3): L: 0.955498; A: 0.493421; R: 0.493421; P: 0.572631; F1: 0.457229\n",
      "(valid @ 3): L: 0.957922; A: 0.570175; R: 0.570175; P: 0.610427; F1: 0.560871\n",
      "(train @ 4): L: 1.013732; A: 0.473684; R: 0.473684; P: 0.487767; F1: 0.450170\n",
      "(valid @ 4): L: 0.959993; A: 0.526316; R: 0.526316; P: 0.438356; F1: 0.468870\n",
      "(train @ 5): L: 0.913757; A: 0.541667; R: 0.541667; P: 0.631928; F1: 0.505741\n",
      "(valid @ 5): L: 0.901266; A: 0.587719; R: 0.587719; P: 0.652661; F1: 0.566774\n",
      "(train @ 6): L: 0.874033; A: 0.482456; R: 0.482456; P: 0.531939; F1: 0.415422\n",
      "(valid @ 6): L: 0.863714; A: 0.592105; R: 0.592105; P: 0.655881; F1: 0.581642\n",
      "(train @ 7): L: 0.893688; A: 0.513158; R: 0.513158; P: 0.625445; F1: 0.463905\n",
      "(valid @ 7): L: 0.843713; A: 0.596491; R: 0.596491; P: 0.650649; F1: 0.590503\n",
      "(train @ 8): L: 0.856075; A: 0.447368; R: 0.447368; P: 0.455085; F1: 0.374873\n",
      "(valid @ 8): L: 0.787803; A: 0.644737; R: 0.644737; P: 0.652159; F1: 0.643249\n",
      "(train @ 9): L: 0.798130; A: 0.565789; R: 0.565789; P: 0.669618; F1: 0.523443\n",
      "(valid @ 9): L: 0.815317; A: 0.600877; R: 0.600877; P: 0.659400; F1: 0.593008\n",
      "(train @ 10): L: 0.754958; A: 0.585526; R: 0.585526; P: 0.591269; F1: 0.583903\n",
      "(valid @ 10): L: 0.754689; A: 0.657895; R: 0.657895; P: 0.681589; F1: 0.661561\n",
      "(train @ 11): L: 0.732982; A: 0.629386; R: 0.629386; P: 0.674583; F1: 0.611784\n",
      "(valid @ 11): L: 0.842641; A: 0.622807; R: 0.622807; P: 0.667775; F1: 0.618343\n",
      "(train @ 12): L: 0.748592; A: 0.603070; R: 0.603070; P: 0.602362; F1: 0.601679\n",
      "(valid @ 12): L: 0.716845; A: 0.662281; R: 0.662281; P: 0.679963; F1: 0.666424\n",
      "(train @ 13): L: 0.722704; A: 0.616228; R: 0.616228; P: 0.676075; F1: 0.589160\n",
      "(valid @ 13): L: 0.734676; A: 0.662281; R: 0.662281; P: 0.689703; F1: 0.664960\n",
      "(train @ 14): L: 0.708207; A: 0.631579; R: 0.631579; P: 0.633291; F1: 0.626545\n",
      "(valid @ 14): L: 0.771877; A: 0.675439; R: 0.675439; P: 0.699029; F1: 0.677782\n",
      "(train @ 15): L: 0.697000; A: 0.622807; R: 0.622807; P: 0.652136; F1: 0.603693\n",
      "(valid @ 15): L: 0.765839; A: 0.692982; R: 0.692982; P: 0.716108; F1: 0.694830\n",
      "(train @ 16): L: 0.690101; A: 0.655702; R: 0.655702; P: 0.675106; F1: 0.649797\n",
      "(valid @ 16): L: 0.713318; A: 0.679825; R: 0.679825; P: 0.704720; F1: 0.682345\n",
      "(train @ 17): L: 0.671121; A: 0.657895; R: 0.657895; P: 0.673132; F1: 0.652498\n",
      "(valid @ 17): L: 0.719829; A: 0.679825; R: 0.679825; P: 0.711646; F1: 0.680028\n",
      "(train @ 18): L: 0.715672; A: 0.605263; R: 0.605263; P: 0.629086; F1: 0.587853\n",
      "(valid @ 18): L: 0.734996; A: 0.692982; R: 0.692982; P: 0.714059; F1: 0.695391\n",
      "(train @ 19): L: 0.666714; A: 0.657895; R: 0.657895; P: 0.659155; F1: 0.655289\n",
      "(valid @ 19): L: 0.759508; A: 0.697368; R: 0.697368; P: 0.721989; F1: 0.696825\n",
      "(train @ 20): L: 0.654562; A: 0.677632; R: 0.677632; P: 0.704196; F1: 0.671088\n",
      "(valid @ 20): L: 0.726850; A: 0.710526; R: 0.710526; P: 0.730349; F1: 0.710282\n",
      "(train @ 21): L: 0.633629; A: 0.684211; R: 0.684211; P: 0.684482; F1: 0.684331\n",
      "(valid @ 21): L: 0.709823; A: 0.631579; R: 0.631579; P: 0.709748; F1: 0.610258\n",
      "(train @ 22): L: 0.691689; A: 0.638158; R: 0.638158; P: 0.671023; F1: 0.619767\n",
      "(valid @ 22): L: 0.728259; A: 0.657895; R: 0.657895; P: 0.679672; F1: 0.658148\n",
      "(train @ 23): L: 0.653668; A: 0.701754; R: 0.701754; P: 0.709441; F1: 0.698231\n",
      "(valid @ 23): L: 0.760963; A: 0.692982; R: 0.692982; P: 0.735976; F1: 0.691269\n",
      "(train @ 24): L: 0.631989; A: 0.688596; R: 0.688596; P: 0.709703; F1: 0.685993\n",
      "(valid @ 24): L: 0.746808; A: 0.657895; R: 0.657895; P: 0.684562; F1: 0.657002\n",
      "(train @ 25): L: 0.649435; A: 0.664474; R: 0.664474; P: 0.684529; F1: 0.656810\n",
      "(valid @ 25): L: 0.680419; A: 0.701754; R: 0.701754; P: 0.721760; F1: 0.703558\n",
      "(train @ 26): L: 0.610633; A: 0.703947; R: 0.703947; P: 0.708898; F1: 0.701933\n",
      "(valid @ 26): L: 0.698714; A: 0.719298; R: 0.719298; P: 0.751839; F1: 0.719776\n",
      "(train @ 27): L: 0.584066; A: 0.730263; R: 0.730263; P: 0.746522; F1: 0.729163\n",
      "(valid @ 27): L: 0.612866; A: 0.706140; R: 0.706140; P: 0.767908; F1: 0.702129\n",
      "(train @ 28): L: 0.655263; A: 0.699561; R: 0.699561; P: 0.716054; F1: 0.697259\n",
      "(valid @ 28): L: 0.719820; A: 0.697368; R: 0.697368; P: 0.730080; F1: 0.698622\n",
      "(train @ 29): L: 0.576741; A: 0.719298; R: 0.719298; P: 0.721483; F1: 0.718484\n",
      "(valid @ 29): L: 0.624411; A: 0.714912; R: 0.714912; P: 0.765069; F1: 0.713320\n",
      "(train @ 30): L: 0.560182; A: 0.725877; R: 0.725877; P: 0.726669; F1: 0.726205\n",
      "(valid @ 30): L: 0.617579; A: 0.714912; R: 0.714912; P: 0.765391; F1: 0.709568\n",
      "Best val Metric 0.719776 @ 26\n",
      "\n",
      "models are saved @ ./predictions/211209080119/flvl_2_3_5_6_8_9_pretrained_model_c1.pt\n",
      "Saving predictions @ ./predictions/211209080119/flvl_train_2_3_5_6_8_9_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209080119/flvl_valid_1_4_7_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209080119/flvl_public_test_trained_on_2_3_5_6_8_9_r21d_rgb.csv\n",
      "Average of Best Metrics on Each Valid Set: 0.717730, 211209080119\n",
      "Saved test results @ ./predictions/211209080119/flvl_public_test_agg_r21d_rgb.csv\n"
     ]
    }
   ],
   "source": [
    "# View 2\n",
    "\n",
    "# define architecture and run k-fold training\n",
    "from main_1View import Config, run_kfold\n",
    "cfg = Config()\n",
    "cfg.assign_variable('task', 'flvl')\n",
    "cfg.assign_variable('output_dim', 3)\n",
    "cfg.assign_variable('model_type', 'GRU')\n",
    "cfg.assign_variable('bi_dir', False)\n",
    "cfg.assign_variable('device', 'cuda:0')\n",
    "cfg.assign_variable('data_root', './r21d_rgb_features')\n",
    "cfg.assign_variable('drop_p', 0.0) # results will be irreproducible\n",
    "cfg.assign_variable('batch_size', 64)\n",
    "cfg.assign_variable('input_dim', 512)\n",
    "cfg.assign_variable('hidden_dim', 512)\n",
    "cfg.assign_variable('n_layers', 3)\n",
    "cfg.assign_variable('num_epochs', 30)\n",
    "cfg.assign_variable('seed', 1337)\n",
    "\n",
    "run_kfold(cfg,[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': [1, 2, 4, 5, 7, 8], 'valid': [3, 6, 9], 'public_test': [10, 11, 12], 'private_test': [13, 14, 15]}\n",
      "(train @ 1): L: 1.008560; A: 0.436404; R: 0.436404; P: 0.439429; F1: 0.434960\n",
      "(valid @ 1): L: 1.016719; A: 0.469298; R: 0.469298; P: 0.553780; F1: 0.405823\n",
      "(train @ 2): L: 0.941856; A: 0.513158; R: 0.513158; P: 0.551200; F1: 0.480558\n",
      "(valid @ 2): L: 1.000721; A: 0.491228; R: 0.491228; P: 0.410678; F1: 0.446946\n",
      "(train @ 3): L: 0.927354; A: 0.539474; R: 0.539474; P: 0.480288; F1: 0.495639\n",
      "(valid @ 3): L: 1.036299; A: 0.495614; R: 0.495614; P: 0.597845; F1: 0.450747\n",
      "(train @ 4): L: 0.927739; A: 0.460526; R: 0.460526; P: 0.547034; F1: 0.424386\n",
      "(valid @ 4): L: 0.929101; A: 0.530702; R: 0.530702; P: 0.626603; F1: 0.495733\n",
      "(train @ 5): L: 0.890512; A: 0.524123; R: 0.524123; P: 0.685870; F1: 0.445792\n",
      "(valid @ 5): L: 0.930254; A: 0.491228; R: 0.491228; P: 0.332662; F1: 0.359903\n",
      "(train @ 6): L: 0.927757; A: 0.497807; R: 0.497807; P: 0.548620; F1: 0.459325\n",
      "(valid @ 6): L: 0.971121; A: 0.530702; R: 0.530702; P: 0.462760; F1: 0.486178\n",
      "(train @ 7): L: 0.933219; A: 0.543860; R: 0.543860; P: 0.552349; F1: 0.479348\n",
      "(valid @ 7): L: 0.922924; A: 0.622807; R: 0.622807; P: 0.620618; F1: 0.621058\n",
      "(train @ 8): L: 0.827756; A: 0.563596; R: 0.563596; P: 0.573616; F1: 0.536912\n",
      "(valid @ 8): L: 0.952460; A: 0.614035; R: 0.614035; P: 0.618485; F1: 0.598997\n",
      "(train @ 9): L: 0.807882; A: 0.609649; R: 0.609649; P: 0.690375; F1: 0.583976\n",
      "(valid @ 9): L: 0.864371; A: 0.587719; R: 0.587719; P: 0.635684; F1: 0.580645\n",
      "(train @ 10): L: 0.775506; A: 0.627193; R: 0.627193; P: 0.645005; F1: 0.626615\n",
      "(valid @ 10): L: 0.810816; A: 0.614035; R: 0.614035; P: 0.618997; F1: 0.597208\n",
      "(train @ 11): L: 0.715072; A: 0.657895; R: 0.657895; P: 0.699684; F1: 0.652647\n",
      "(valid @ 11): L: 0.762435; A: 0.644737; R: 0.644737; P: 0.642444; F1: 0.641988\n",
      "(train @ 12): L: 0.685116; A: 0.697368; R: 0.697368; P: 0.703744; F1: 0.696470\n",
      "(valid @ 12): L: 0.817717; A: 0.587719; R: 0.587719; P: 0.588694; F1: 0.565479\n",
      "(train @ 13): L: 0.750961; A: 0.625000; R: 0.625000; P: 0.700557; F1: 0.601324\n",
      "(valid @ 13): L: 0.707168; A: 0.649123; R: 0.649123; P: 0.649476; F1: 0.644015\n",
      "(train @ 14): L: 0.685669; A: 0.686404; R: 0.686404; P: 0.708167; F1: 0.687464\n",
      "(valid @ 14): L: 0.704206; A: 0.662281; R: 0.662281; P: 0.671260; F1: 0.659307\n",
      "(train @ 15): L: 0.653529; A: 0.701754; R: 0.701754; P: 0.737999; F1: 0.697382\n",
      "(valid @ 15): L: 0.703651; A: 0.644737; R: 0.644737; P: 0.645813; F1: 0.639256\n",
      "(train @ 16): L: 0.656098; A: 0.697368; R: 0.697368; P: 0.712387; F1: 0.695908\n",
      "(valid @ 16): L: 0.682410; A: 0.657895; R: 0.657895; P: 0.655177; F1: 0.655721\n",
      "(train @ 17): L: 0.613323; A: 0.712719; R: 0.712719; P: 0.723105; F1: 0.713111\n",
      "(valid @ 17): L: 0.723181; A: 0.622807; R: 0.622807; P: 0.627939; F1: 0.610547\n",
      "(train @ 18): L: 0.667299; A: 0.682018; R: 0.682018; P: 0.736692; F1: 0.669607\n",
      "(valid @ 18): L: 0.672268; A: 0.666667; R: 0.666667; P: 0.714541; F1: 0.648864\n",
      "(train @ 19): L: 0.607785; A: 0.730263; R: 0.730263; P: 0.749323; F1: 0.725523\n",
      "(valid @ 19): L: 0.652190; A: 0.679825; R: 0.679825; P: 0.679929; F1: 0.678777\n",
      "(train @ 20): L: 0.584984; A: 0.741228; R: 0.741228; P: 0.764095; F1: 0.739654\n",
      "(valid @ 20): L: 0.675489; A: 0.671053; R: 0.671053; P: 0.690539; F1: 0.665544\n",
      "(train @ 21): L: 0.561908; A: 0.745614; R: 0.745614; P: 0.749337; F1: 0.746750\n",
      "(valid @ 21): L: 0.876262; A: 0.574561; R: 0.574561; P: 0.595022; F1: 0.543021\n",
      "(train @ 22): L: 0.708582; A: 0.653509; R: 0.653509; P: 0.673797; F1: 0.646833\n",
      "(valid @ 22): L: 0.637595; A: 0.688596; R: 0.688596; P: 0.702018; F1: 0.686148\n",
      "(train @ 23): L: 0.593654; A: 0.730263; R: 0.730263; P: 0.742575; F1: 0.731128\n",
      "(valid @ 23): L: 0.660900; A: 0.671053; R: 0.671053; P: 0.688725; F1: 0.666634\n",
      "(train @ 24): L: 0.614471; A: 0.712719; R: 0.712719; P: 0.727500; F1: 0.713439\n",
      "(valid @ 24): L: 0.675468; A: 0.635965; R: 0.635965; P: 0.646270; F1: 0.632417\n",
      "(train @ 25): L: 0.531158; A: 0.743421; R: 0.743421; P: 0.756958; F1: 0.743185\n",
      "(valid @ 25): L: 0.654086; A: 0.635965; R: 0.635965; P: 0.646572; F1: 0.630167\n",
      "(train @ 26): L: 0.507971; A: 0.765351; R: 0.765351; P: 0.770524; F1: 0.764779\n",
      "(valid @ 26): L: 0.619816; A: 0.671053; R: 0.671053; P: 0.676824; F1: 0.669971\n",
      "(train @ 27): L: 0.506838; A: 0.745614; R: 0.745614; P: 0.758061; F1: 0.744369\n",
      "(valid @ 27): L: 0.664286; A: 0.644737; R: 0.644737; P: 0.674678; F1: 0.640869\n",
      "(train @ 28): L: 0.520411; A: 0.747807; R: 0.747807; P: 0.752097; F1: 0.746960\n",
      "(valid @ 28): L: 0.679997; A: 0.644737; R: 0.644737; P: 0.657615; F1: 0.642164\n",
      "(train @ 29): L: 0.509894; A: 0.763158; R: 0.763158; P: 0.771725; F1: 0.761811\n",
      "(valid @ 29): L: 0.690447; A: 0.653509; R: 0.653509; P: 0.672052; F1: 0.651284\n",
      "(train @ 30): L: 0.481658; A: 0.787281; R: 0.787281; P: 0.791401; F1: 0.786995\n",
      "(valid @ 30): L: 0.628348; A: 0.679825; R: 0.679825; P: 0.709767; F1: 0.674861\n",
      "Best val Metric 0.686148 @ 22\n",
      "\n",
      "models are saved @ ./predictions/211209082414/flvl_1_2_4_5_7_8_pretrained_model_c1.pt\n",
      "Saving predictions @ ./predictions/211209082414/flvl_train_1_2_4_5_7_8_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209082414/flvl_valid_3_6_9_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209082414/flvl_public_test_trained_on_1_2_4_5_7_8_r21d_rgb.csv\n",
      "{'train': [1, 3, 4, 6, 7, 9], 'valid': [2, 5, 8], 'public_test': [10, 11, 12], 'private_test': [13, 14, 15]}\n",
      "(train @ 1): L: 1.016431; A: 0.458333; R: 0.458333; P: 0.455532; F1: 0.456106\n",
      "(valid @ 1): L: 0.994166; A: 0.421053; R: 0.421053; P: 0.181260; F1: 0.253424\n",
      "(train @ 2): L: 0.957871; A: 0.515351; R: 0.515351; P: 0.452908; F1: 0.468900\n",
      "(valid @ 2): L: 0.994469; A: 0.565789; R: 0.565789; P: 0.467518; F1: 0.501316\n",
      "(train @ 3): L: 0.943867; A: 0.508772; R: 0.508772; P: 0.589730; F1: 0.475159\n",
      "(valid @ 3): L: 0.898100; A: 0.574561; R: 0.574561; P: 0.649574; F1: 0.556503\n",
      "(train @ 4): L: 0.982075; A: 0.458333; R: 0.458333; P: 0.485061; F1: 0.430295\n",
      "(valid @ 4): L: 0.915139; A: 0.570175; R: 0.570175; P: 0.484087; F1: 0.522979\n",
      "(train @ 5): L: 0.932467; A: 0.541667; R: 0.541667; P: 0.683817; F1: 0.487498\n",
      "(valid @ 5): L: 0.926791; A: 0.421053; R: 0.421053; P: 0.177285; F1: 0.249513\n",
      "(train @ 6): L: 0.890594; A: 0.486842; R: 0.486842; P: 0.572261; F1: 0.423490\n",
      "(valid @ 6): L: 0.861816; A: 0.552632; R: 0.552632; P: 0.642234; F1: 0.523138\n",
      "(train @ 7): L: 0.890298; A: 0.524123; R: 0.524123; P: 0.625434; F1: 0.477484\n",
      "(valid @ 7): L: 0.879244; A: 0.486842; R: 0.486842; P: 0.347665; F1: 0.354504\n",
      "(train @ 8): L: 0.853414; A: 0.489035; R: 0.489035; P: 0.516666; F1: 0.430938\n",
      "(valid @ 8): L: 0.800786; A: 0.635965; R: 0.635965; P: 0.653662; F1: 0.625533\n",
      "(train @ 9): L: 0.828188; A: 0.583333; R: 0.583333; P: 0.689176; F1: 0.541684\n",
      "(valid @ 9): L: 0.831821; A: 0.618421; R: 0.618421; P: 0.661797; F1: 0.601509\n",
      "(train @ 10): L: 0.783989; A: 0.600877; R: 0.600877; P: 0.601283; F1: 0.600571\n",
      "(valid @ 10): L: 0.728987; A: 0.662281; R: 0.662281; P: 0.684682; F1: 0.659602\n",
      "(train @ 11): L: 0.756944; A: 0.609649; R: 0.609649; P: 0.690427; F1: 0.580646\n",
      "(valid @ 11): L: 0.804067; A: 0.600877; R: 0.600877; P: 0.672450; F1: 0.568656\n",
      "(train @ 12): L: 0.756846; A: 0.561404; R: 0.561404; P: 0.582938; F1: 0.543608\n",
      "(valid @ 12): L: 0.695612; A: 0.662281; R: 0.662281; P: 0.668984; F1: 0.655695\n",
      "(train @ 13): L: 0.794165; A: 0.592105; R: 0.592105; P: 0.687540; F1: 0.537881\n",
      "(valid @ 13): L: 0.682876; A: 0.688596; R: 0.688596; P: 0.714483; F1: 0.684972\n",
      "(train @ 14): L: 0.737371; A: 0.611842; R: 0.611842; P: 0.649075; F1: 0.585295\n",
      "(valid @ 14): L: 0.673780; A: 0.710526; R: 0.710526; P: 0.724862; F1: 0.713014\n",
      "(train @ 15): L: 0.708975; A: 0.620614; R: 0.620614; P: 0.678590; F1: 0.599648\n",
      "(valid @ 15): L: 0.622941; A: 0.728070; R: 0.728070; P: 0.735014; F1: 0.730204\n",
      "(train @ 16): L: 0.712884; A: 0.653509; R: 0.653509; P: 0.670559; F1: 0.648763\n",
      "(valid @ 16): L: 0.630315; A: 0.732456; R: 0.732456; P: 0.740758; F1: 0.733021\n",
      "(train @ 17): L: 0.679622; A: 0.673246; R: 0.673246; P: 0.687446; F1: 0.671931\n",
      "(valid @ 17): L: 0.626027; A: 0.723684; R: 0.723684; P: 0.732806; F1: 0.723726\n",
      "(train @ 18): L: 0.710475; A: 0.616228; R: 0.616228; P: 0.651617; F1: 0.598991\n",
      "(valid @ 18): L: 0.648545; A: 0.697368; R: 0.697368; P: 0.717584; F1: 0.692836\n",
      "(train @ 19): L: 0.663241; A: 0.688596; R: 0.688596; P: 0.696627; F1: 0.687772\n",
      "(valid @ 19): L: 0.636968; A: 0.719298; R: 0.719298; P: 0.734513; F1: 0.720796\n",
      "(train @ 20): L: 0.637354; A: 0.710526; R: 0.710526; P: 0.729109; F1: 0.707324\n",
      "(valid @ 20): L: 0.609723; A: 0.719298; R: 0.719298; P: 0.738941; F1: 0.718258\n",
      "(train @ 21): L: 0.655825; A: 0.706140; R: 0.706140; P: 0.719993; F1: 0.707310\n",
      "(valid @ 21): L: 0.754913; A: 0.618421; R: 0.618421; P: 0.642518; F1: 0.602573\n",
      "(train @ 22): L: 0.776196; A: 0.644737; R: 0.644737; P: 0.677370; F1: 0.634695\n",
      "(valid @ 22): L: 0.756825; A: 0.583333; R: 0.583333; P: 0.647048; F1: 0.561103\n",
      "(train @ 23): L: 0.702176; A: 0.657895; R: 0.657895; P: 0.669241; F1: 0.658245\n",
      "(valid @ 23): L: 0.655507; A: 0.710526; R: 0.710526; P: 0.754218; F1: 0.710605\n",
      "(train @ 24): L: 0.689512; A: 0.657895; R: 0.657895; P: 0.670839; F1: 0.659775\n",
      "(valid @ 24): L: 0.571746; A: 0.745614; R: 0.745614; P: 0.753940; F1: 0.743812\n",
      "(train @ 25): L: 0.674227; A: 0.651316; R: 0.651316; P: 0.694115; F1: 0.636734\n",
      "(valid @ 25): L: 0.675452; A: 0.671053; R: 0.671053; P: 0.699729; F1: 0.661175\n",
      "(train @ 26): L: 0.651234; A: 0.682018; R: 0.682018; P: 0.697910; F1: 0.677560\n",
      "(valid @ 26): L: 0.628686; A: 0.719298; R: 0.719298; P: 0.807441; F1: 0.709836\n",
      "(train @ 27): L: 0.631463; A: 0.703947; R: 0.703947; P: 0.725659; F1: 0.700450\n",
      "(valid @ 27): L: 0.575162; A: 0.736842; R: 0.736842; P: 0.750392; F1: 0.739568\n",
      "(train @ 28): L: 0.601448; A: 0.697368; R: 0.697368; P: 0.716474; F1: 0.692458\n",
      "(valid @ 28): L: 0.603810; A: 0.736842; R: 0.736842; P: 0.755483; F1: 0.739479\n",
      "(train @ 29): L: 0.572956; A: 0.743421; R: 0.743421; P: 0.745702; F1: 0.743115\n",
      "(valid @ 29): L: 0.524456; A: 0.771930; R: 0.771930; P: 0.797059; F1: 0.769741\n",
      "(train @ 30): L: 0.566899; A: 0.712719; R: 0.712719; P: 0.722376; F1: 0.710354\n",
      "(valid @ 30): L: 0.526671; A: 0.767544; R: 0.767544; P: 0.769872; F1: 0.767407\n",
      "Best val Metric 0.769741 @ 29\n",
      "\n",
      "models are saved @ ./predictions/211209082414/flvl_1_3_4_6_7_9_pretrained_model_c1.pt\n",
      "Saving predictions @ ./predictions/211209082414/flvl_train_1_3_4_6_7_9_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209082414/flvl_valid_2_5_8_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209082414/flvl_public_test_trained_on_1_3_4_6_7_9_r21d_rgb.csv\n",
      "{'train': [2, 3, 5, 6, 8, 9], 'valid': [1, 4, 7], 'public_test': [10, 11, 12], 'private_test': [13, 14, 15]}\n",
      "(train @ 1): L: 1.018179; A: 0.425439; R: 0.425439; P: 0.427892; F1: 0.423609\n",
      "(valid @ 1): L: 0.988142; A: 0.421053; R: 0.421053; P: 0.184571; F1: 0.256642\n",
      "(train @ 2): L: 0.964944; A: 0.475877; R: 0.475877; P: 0.414127; F1: 0.429936\n",
      "(valid @ 2): L: 1.000649; A: 0.592105; R: 0.592105; P: 0.500142; F1: 0.529924\n",
      "(train @ 3): L: 0.947924; A: 0.504386; R: 0.504386; P: 0.582006; F1: 0.467367\n",
      "(valid @ 3): L: 0.951218; A: 0.565789; R: 0.565789; P: 0.641426; F1: 0.548789\n",
      "(train @ 4): L: 1.015275; A: 0.456140; R: 0.456140; P: 0.470565; F1: 0.428958\n",
      "(valid @ 4): L: 0.956742; A: 0.574561; R: 0.574561; P: 0.482843; F1: 0.517861\n",
      "(train @ 5): L: 0.916999; A: 0.535088; R: 0.535088; P: 0.623211; F1: 0.499397\n",
      "(valid @ 5): L: 0.905212; A: 0.552632; R: 0.552632; P: 0.638541; F1: 0.510312\n",
      "(train @ 6): L: 0.865748; A: 0.510965; R: 0.510965; P: 0.566238; F1: 0.444212\n",
      "(valid @ 6): L: 0.874210; A: 0.565789; R: 0.565789; P: 0.650481; F1: 0.541818\n",
      "(train @ 7): L: 0.885671; A: 0.517544; R: 0.517544; P: 0.598788; F1: 0.481956\n",
      "(valid @ 7): L: 0.912553; A: 0.486842; R: 0.486842; P: 0.347665; F1: 0.354504\n",
      "(train @ 8): L: 0.855591; A: 0.508772; R: 0.508772; P: 0.532134; F1: 0.475415\n",
      "(valid @ 8): L: 0.818423; A: 0.635965; R: 0.635965; P: 0.652347; F1: 0.626356\n",
      "(train @ 9): L: 0.821493; A: 0.574561; R: 0.574561; P: 0.684120; F1: 0.536593\n",
      "(valid @ 9): L: 0.859133; A: 0.622807; R: 0.622807; P: 0.696423; F1: 0.600639\n",
      "(train @ 10): L: 0.759958; A: 0.589912; R: 0.589912; P: 0.590127; F1: 0.588289\n",
      "(valid @ 10): L: 0.777383; A: 0.622807; R: 0.622807; P: 0.664160; F1: 0.622095\n",
      "(train @ 11): L: 0.738568; A: 0.611842; R: 0.611842; P: 0.661673; F1: 0.590418\n",
      "(valid @ 11): L: 0.881534; A: 0.631579; R: 0.631579; P: 0.691135; F1: 0.608847\n",
      "(train @ 12): L: 0.752451; A: 0.574561; R: 0.574561; P: 0.583853; F1: 0.566697\n",
      "(valid @ 12): L: 0.740110; A: 0.649123; R: 0.649123; P: 0.676023; F1: 0.643393\n",
      "(train @ 13): L: 0.768945; A: 0.594298; R: 0.594298; P: 0.702886; F1: 0.535860\n",
      "(valid @ 13): L: 0.764317; A: 0.684211; R: 0.684211; P: 0.776809; F1: 0.670066\n",
      "(train @ 14): L: 0.698566; A: 0.622807; R: 0.622807; P: 0.678847; F1: 0.594198\n",
      "(valid @ 14): L: 0.747397; A: 0.644737; R: 0.644737; P: 0.673954; F1: 0.647777\n",
      "(train @ 15): L: 0.694664; A: 0.631579; R: 0.631579; P: 0.688648; F1: 0.603665\n",
      "(valid @ 15): L: 0.719748; A: 0.710526; R: 0.710526; P: 0.732354; F1: 0.709290\n",
      "(train @ 16): L: 0.673438; A: 0.668860; R: 0.668860; P: 0.676221; F1: 0.667246\n",
      "(valid @ 16): L: 0.663583; A: 0.692982; R: 0.692982; P: 0.709480; F1: 0.694779\n",
      "(train @ 17): L: 0.675607; A: 0.677632; R: 0.677632; P: 0.709059; F1: 0.667281\n",
      "(valid @ 17): L: 0.691593; A: 0.684211; R: 0.684211; P: 0.706282; F1: 0.687207\n",
      "(train @ 18): L: 0.703223; A: 0.609649; R: 0.609649; P: 0.626774; F1: 0.597282\n",
      "(valid @ 18): L: 0.728044; A: 0.688596; R: 0.688596; P: 0.711077; F1: 0.688886\n",
      "(train @ 19): L: 0.643624; A: 0.695175; R: 0.695175; P: 0.699522; F1: 0.691207\n",
      "(valid @ 19): L: 0.741241; A: 0.675439; R: 0.675439; P: 0.703011; F1: 0.672553\n",
      "(train @ 20): L: 0.614170; A: 0.708333; R: 0.708333; P: 0.728482; F1: 0.701677\n",
      "(valid @ 20): L: 0.722684; A: 0.666667; R: 0.666667; P: 0.706349; F1: 0.654642\n",
      "(train @ 21): L: 0.595662; A: 0.697368; R: 0.697368; P: 0.698436; F1: 0.697622\n",
      "(valid @ 21): L: 0.715869; A: 0.605263; R: 0.605263; P: 0.671787; F1: 0.575555\n",
      "(train @ 22): L: 0.684434; A: 0.642544; R: 0.642544; P: 0.648571; F1: 0.635216\n",
      "(valid @ 22): L: 0.696748; A: 0.684211; R: 0.684211; P: 0.776901; F1: 0.667921\n",
      "(train @ 23): L: 0.614674; A: 0.721491; R: 0.721491; P: 0.723772; F1: 0.720155\n",
      "(valid @ 23): L: 0.729941; A: 0.692982; R: 0.692982; P: 0.718758; F1: 0.693247\n",
      "(train @ 24): L: 0.608507; A: 0.706140; R: 0.706140; P: 0.719192; F1: 0.706804\n",
      "(valid @ 24): L: 0.720508; A: 0.697368; R: 0.697368; P: 0.721122; F1: 0.695656\n",
      "(train @ 25): L: 0.606809; A: 0.688596; R: 0.688596; P: 0.720153; F1: 0.681454\n",
      "(valid @ 25): L: 0.695255; A: 0.684211; R: 0.684211; P: 0.808831; F1: 0.664181\n",
      "(train @ 26): L: 0.586162; A: 0.706140; R: 0.706140; P: 0.707161; F1: 0.703751\n",
      "(valid @ 26): L: 0.670426; A: 0.679825; R: 0.679825; P: 0.719786; F1: 0.680265\n",
      "(train @ 27): L: 0.559559; A: 0.732456; R: 0.732456; P: 0.733927; F1: 0.732477\n",
      "(valid @ 27): L: 0.575652; A: 0.728070; R: 0.728070; P: 0.745174; F1: 0.729926\n",
      "(train @ 28): L: 0.627559; A: 0.706140; R: 0.706140; P: 0.726991; F1: 0.700002\n",
      "(valid @ 28): L: 0.786234; A: 0.653509; R: 0.653509; P: 0.699192; F1: 0.642120\n",
      "(train @ 29): L: 0.544897; A: 0.741228; R: 0.741228; P: 0.744477; F1: 0.740633\n",
      "(valid @ 29): L: 0.601554; A: 0.732456; R: 0.732456; P: 0.758662; F1: 0.733222\n",
      "(train @ 30): L: 0.503967; A: 0.745614; R: 0.745614; P: 0.749625; F1: 0.746923\n",
      "(valid @ 30): L: 0.556159; A: 0.745614; R: 0.745614; P: 0.761424; F1: 0.745351\n",
      "Best val Metric 0.745351 @ 30\n",
      "\n",
      "models are saved @ ./predictions/211209082414/flvl_2_3_5_6_8_9_pretrained_model_c1.pt\n",
      "Saving predictions @ ./predictions/211209082414/flvl_train_2_3_5_6_8_9_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209082414/flvl_valid_1_4_7_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209082414/flvl_public_test_trained_on_2_3_5_6_8_9_r21d_rgb.csv\n",
      "Average of Best Metrics on Each Valid Set: 0.733746, 211209082414\n",
      "Saved test results @ ./predictions/211209082414/flvl_public_test_agg_r21d_rgb.csv\n"
     ]
    }
   ],
   "source": [
    "# View 3\n",
    "\n",
    "# define architecture and run k-fold training\n",
    "from main_1View import Config, run_kfold\n",
    "cfg = Config()\n",
    "cfg.assign_variable('task', 'flvl')\n",
    "cfg.assign_variable('output_dim', 3)\n",
    "cfg.assign_variable('model_type', 'GRU')\n",
    "cfg.assign_variable('bi_dir', False)\n",
    "cfg.assign_variable('device', 'cuda:0')\n",
    "cfg.assign_variable('data_root', './r21d_rgb_features')\n",
    "cfg.assign_variable('drop_p', 0.0) # results will be irreproducible\n",
    "cfg.assign_variable('batch_size', 64)\n",
    "cfg.assign_variable('input_dim', 512)\n",
    "cfg.assign_variable('hidden_dim', 512)\n",
    "cfg.assign_variable('n_layers', 3)\n",
    "cfg.assign_variable('num_epochs', 30)\n",
    "cfg.assign_variable('seed', 1337)\n",
    "\n",
    "run_kfold(cfg,[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': [1, 2, 4, 5, 7, 8], 'valid': [3, 6, 9], 'public_test': [10, 11, 12], 'private_test': [13, 14, 15]}\n",
      "(train @ 1): L: 1.005956; A: 0.473684; R: 0.473684; P: 0.475322; F1: 0.472010\n",
      "(valid @ 1): L: 1.010160; A: 0.447368; R: 0.447368; P: 0.363400; F1: 0.383215\n",
      "(train @ 2): L: 0.933834; A: 0.543860; R: 0.543860; P: 0.494393; F1: 0.496599\n",
      "(valid @ 2): L: 0.989012; A: 0.478070; R: 0.478070; P: 0.399108; F1: 0.434515\n",
      "(train @ 3): L: 0.925284; A: 0.561404; R: 0.561404; P: 0.665973; F1: 0.521673\n",
      "(valid @ 3): L: 1.002057; A: 0.478070; R: 0.478070; P: 0.588479; F1: 0.421609\n",
      "(train @ 4): L: 0.913704; A: 0.475877; R: 0.475877; P: 0.563371; F1: 0.442224\n",
      "(valid @ 4): L: 0.933286; A: 0.539474; R: 0.539474; P: 0.627303; F1: 0.512526\n",
      "(train @ 5): L: 0.900342; A: 0.532895; R: 0.532895; P: 0.714877; F1: 0.462144\n",
      "(valid @ 5): L: 0.947665; A: 0.469298; R: 0.469298; P: 0.344167; F1: 0.332190\n",
      "(train @ 6): L: 0.946262; A: 0.502193; R: 0.502193; P: 0.551471; F1: 0.469260\n",
      "(valid @ 6): L: 0.975515; A: 0.530702; R: 0.530702; P: 0.473934; F1: 0.481593\n",
      "(train @ 7): L: 0.936570; A: 0.548246; R: 0.548246; P: 0.569801; F1: 0.480969\n",
      "(valid @ 7): L: 0.920469; A: 0.574561; R: 0.574561; P: 0.623728; F1: 0.572704\n",
      "(train @ 8): L: 0.829719; A: 0.592105; R: 0.592105; P: 0.598126; F1: 0.576725\n",
      "(valid @ 8): L: 1.002638; A: 0.552632; R: 0.552632; P: 0.570138; F1: 0.544515\n",
      "(train @ 9): L: 0.832783; A: 0.578947; R: 0.578947; P: 0.664636; F1: 0.558006\n",
      "(valid @ 9): L: 0.902466; A: 0.539474; R: 0.539474; P: 0.613623; F1: 0.510747\n",
      "(train @ 10): L: 0.797504; A: 0.567982; R: 0.567982; P: 0.600568; F1: 0.564097\n",
      "(valid @ 10): L: 0.856671; A: 0.596491; R: 0.596491; P: 0.601064; F1: 0.578919\n",
      "(train @ 11): L: 0.754889; A: 0.644737; R: 0.644737; P: 0.673684; F1: 0.638923\n",
      "(valid @ 11): L: 0.800774; A: 0.635965; R: 0.635965; P: 0.646713; F1: 0.638969\n",
      "(train @ 12): L: 0.704474; A: 0.675439; R: 0.675439; P: 0.688788; F1: 0.673715\n",
      "(valid @ 12): L: 0.829833; A: 0.609649; R: 0.609649; P: 0.614394; F1: 0.594721\n",
      "(train @ 13): L: 0.761532; A: 0.633772; R: 0.633772; P: 0.671187; F1: 0.623331\n",
      "(valid @ 13): L: 0.734735; A: 0.635965; R: 0.635965; P: 0.639058; F1: 0.628329\n",
      "(train @ 14): L: 0.722109; A: 0.666667; R: 0.666667; P: 0.698402; F1: 0.661205\n",
      "(valid @ 14): L: 0.728839; A: 0.662281; R: 0.662281; P: 0.662775; F1: 0.661430\n",
      "(train @ 15): L: 0.672170; A: 0.730263; R: 0.730263; P: 0.750839; F1: 0.728888\n",
      "(valid @ 15): L: 0.741824; A: 0.640351; R: 0.640351; P: 0.650631; F1: 0.631295\n",
      "(train @ 16): L: 0.710368; A: 0.662281; R: 0.662281; P: 0.682550; F1: 0.657440\n",
      "(valid @ 16): L: 0.716941; A: 0.622807; R: 0.622807; P: 0.651490; F1: 0.604868\n",
      "(train @ 17): L: 0.635991; A: 0.714912; R: 0.714912; P: 0.727531; F1: 0.714598\n",
      "(valid @ 17): L: 0.717399; A: 0.644737; R: 0.644737; P: 0.655442; F1: 0.634558\n",
      "(train @ 18): L: 0.674505; A: 0.684211; R: 0.684211; P: 0.734973; F1: 0.672662\n",
      "(valid @ 18): L: 0.719472; A: 0.587719; R: 0.587719; P: 0.626781; F1: 0.555044\n",
      "(train @ 19): L: 0.604666; A: 0.750000; R: 0.750000; P: 0.751437; F1: 0.748398\n",
      "(valid @ 19): L: 0.670730; A: 0.684211; R: 0.684211; P: 0.687165; F1: 0.683593\n",
      "(train @ 20): L: 0.569320; A: 0.756579; R: 0.756579; P: 0.782195; F1: 0.754581\n",
      "(valid @ 20): L: 0.691345; A: 0.701754; R: 0.701754; P: 0.709606; F1: 0.700645\n",
      "(train @ 21): L: 0.559341; A: 0.745614; R: 0.745614; P: 0.747621; F1: 0.744854\n",
      "(valid @ 21): L: 0.906881; A: 0.565789; R: 0.565789; P: 0.677245; F1: 0.519803\n",
      "(train @ 22): L: 0.721861; A: 0.642544; R: 0.642544; P: 0.654012; F1: 0.638395\n",
      "(valid @ 22): L: 0.649572; A: 0.666667; R: 0.666667; P: 0.669896; F1: 0.664664\n",
      "(train @ 23): L: 0.592640; A: 0.728070; R: 0.728070; P: 0.756396; F1: 0.724467\n",
      "(valid @ 23): L: 0.703456; A: 0.622807; R: 0.622807; P: 0.641060; F1: 0.610131\n",
      "(train @ 24): L: 0.587381; A: 0.695175; R: 0.695175; P: 0.707856; F1: 0.693855\n",
      "(valid @ 24): L: 0.691097; A: 0.631579; R: 0.631579; P: 0.647703; F1: 0.621038\n",
      "(train @ 25): L: 0.552278; A: 0.732456; R: 0.732456; P: 0.752063; F1: 0.731368\n",
      "(valid @ 25): L: 0.679573; A: 0.697368; R: 0.697368; P: 0.711144; F1: 0.695887\n",
      "(train @ 26): L: 0.478819; A: 0.789474; R: 0.789474; P: 0.791748; F1: 0.788841\n",
      "(valid @ 26): L: 0.668571; A: 0.688596; R: 0.688596; P: 0.695539; F1: 0.687293\n",
      "(train @ 27): L: 0.483354; A: 0.771930; R: 0.771930; P: 0.775599; F1: 0.770860\n",
      "(valid @ 27): L: 0.779237; A: 0.644737; R: 0.644737; P: 0.697662; F1: 0.636682\n",
      "(train @ 28): L: 0.515150; A: 0.758772; R: 0.758772; P: 0.766247; F1: 0.757465\n",
      "(valid @ 28): L: 0.717519; A: 0.662281; R: 0.662281; P: 0.679225; F1: 0.659359\n",
      "(train @ 29): L: 0.441380; A: 0.807018; R: 0.807018; P: 0.812356; F1: 0.805661\n",
      "(valid @ 29): L: 0.749059; A: 0.644737; R: 0.644737; P: 0.671630; F1: 0.644557\n",
      "(train @ 30): L: 0.409369; A: 0.839912; R: 0.839912; P: 0.840051; F1: 0.839640\n",
      "(valid @ 30): L: 0.713090; A: 0.666667; R: 0.666667; P: 0.688389; F1: 0.666137\n",
      "Best val Metric 0.700645 @ 20\n",
      "\n",
      "models are saved @ ./predictions/211209084739/flvl_1_2_4_5_7_8_pretrained_model_c1.pt\n",
      "Saving predictions @ ./predictions/211209084739/flvl_train_1_2_4_5_7_8_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209084739/flvl_valid_3_6_9_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209084739/flvl_public_test_trained_on_1_2_4_5_7_8_r21d_rgb.csv\n",
      "{'train': [1, 3, 4, 6, 7, 9], 'valid': [2, 5, 8], 'public_test': [10, 11, 12], 'private_test': [13, 14, 15]}\n",
      "(train @ 1): L: 1.014273; A: 0.467105; R: 0.467105; P: 0.469543; F1: 0.465155\n",
      "(valid @ 1): L: 0.973306; A: 0.500000; R: 0.500000; P: 0.560183; F1: 0.409694\n",
      "(train @ 2): L: 0.961360; A: 0.530702; R: 0.530702; P: 0.643077; F1: 0.484664\n",
      "(valid @ 2): L: 0.984405; A: 0.552632; R: 0.552632; P: 0.462768; F1: 0.503383\n",
      "(train @ 3): L: 0.948157; A: 0.506579; R: 0.506579; P: 0.552272; F1: 0.487823\n",
      "(valid @ 3): L: 0.899890; A: 0.578947; R: 0.578947; P: 0.656735; F1: 0.557863\n",
      "(train @ 4): L: 0.978234; A: 0.464912; R: 0.464912; P: 0.485099; F1: 0.426741\n",
      "(valid @ 4): L: 0.922744; A: 0.578947; R: 0.578947; P: 0.493783; F1: 0.531517\n",
      "(train @ 5): L: 0.930473; A: 0.532895; R: 0.532895; P: 0.702090; F1: 0.466209\n",
      "(valid @ 5): L: 0.943066; A: 0.421053; R: 0.421053; P: 0.177285; F1: 0.249513\n",
      "(train @ 6): L: 0.899135; A: 0.469298; R: 0.469298; P: 0.561056; F1: 0.387689\n",
      "(valid @ 6): L: 0.866308; A: 0.552632; R: 0.552632; P: 0.648429; F1: 0.512358\n",
      "(train @ 7): L: 0.892695; A: 0.521930; R: 0.521930; P: 0.629192; F1: 0.473816\n",
      "(valid @ 7): L: 0.878432; A: 0.500000; R: 0.500000; P: 0.561823; F1: 0.374940\n",
      "(train @ 8): L: 0.854909; A: 0.495614; R: 0.495614; P: 0.528984; F1: 0.426469\n",
      "(valid @ 8): L: 0.820610; A: 0.627193; R: 0.627193; P: 0.635127; F1: 0.615174\n",
      "(train @ 9): L: 0.841400; A: 0.574561; R: 0.574561; P: 0.678894; F1: 0.537158\n",
      "(valid @ 9): L: 0.853578; A: 0.592105; R: 0.592105; P: 0.638897; F1: 0.568976\n",
      "(train @ 10): L: 0.785774; A: 0.581140; R: 0.581140; P: 0.583766; F1: 0.581794\n",
      "(valid @ 10): L: 0.743311; A: 0.666667; R: 0.666667; P: 0.668963; F1: 0.662835\n",
      "(train @ 11): L: 0.775199; A: 0.607456; R: 0.607456; P: 0.686871; F1: 0.583125\n",
      "(valid @ 11): L: 0.843598; A: 0.578947; R: 0.578947; P: 0.619781; F1: 0.553705\n",
      "(train @ 12): L: 0.770721; A: 0.592105; R: 0.592105; P: 0.590600; F1: 0.587690\n",
      "(valid @ 12): L: 0.729231; A: 0.662281; R: 0.662281; P: 0.664593; F1: 0.657362\n",
      "(train @ 13): L: 0.792822; A: 0.587719; R: 0.587719; P: 0.670412; F1: 0.551128\n",
      "(valid @ 13): L: 0.724519; A: 0.701754; R: 0.701754; P: 0.721536; F1: 0.702596\n",
      "(train @ 14): L: 0.733576; A: 0.651316; R: 0.651316; P: 0.672792; F1: 0.646171\n",
      "(valid @ 14): L: 0.700102; A: 0.710526; R: 0.710526; P: 0.722121; F1: 0.710777\n",
      "(train @ 15): L: 0.731339; A: 0.629386; R: 0.629386; P: 0.683394; F1: 0.617471\n",
      "(valid @ 15): L: 0.669817; A: 0.697368; R: 0.697368; P: 0.700252; F1: 0.695078\n",
      "(train @ 16): L: 0.767975; A: 0.616228; R: 0.616228; P: 0.658788; F1: 0.606622\n",
      "(valid @ 16): L: 0.747672; A: 0.622807; R: 0.622807; P: 0.660857; F1: 0.603452\n",
      "(train @ 17): L: 0.705529; A: 0.635965; R: 0.635965; P: 0.639902; F1: 0.633479\n",
      "(valid @ 17): L: 0.689310; A: 0.609649; R: 0.609649; P: 0.620547; F1: 0.598147\n",
      "(train @ 18): L: 0.756132; A: 0.592105; R: 0.592105; P: 0.626772; F1: 0.570286\n",
      "(valid @ 18): L: 0.701398; A: 0.631579; R: 0.631579; P: 0.657892; F1: 0.614779\n",
      "(train @ 19): L: 0.678689; A: 0.697368; R: 0.697368; P: 0.700473; F1: 0.695337\n",
      "(valid @ 19): L: 0.637116; A: 0.758772; R: 0.758772; P: 0.768157; F1: 0.758654\n",
      "(train @ 20): L: 0.649035; A: 0.688596; R: 0.688596; P: 0.707033; F1: 0.684337\n",
      "(valid @ 20): L: 0.600946; A: 0.750000; R: 0.750000; P: 0.754038; F1: 0.750728\n",
      "(train @ 21): L: 0.638851; A: 0.697368; R: 0.697368; P: 0.704196; F1: 0.695705\n",
      "(valid @ 21): L: 0.718070; A: 0.618421; R: 0.618421; P: 0.693812; F1: 0.580935\n",
      "(train @ 22): L: 0.710321; A: 0.653509; R: 0.653509; P: 0.674097; F1: 0.643165\n",
      "(valid @ 22): L: 0.639211; A: 0.701754; R: 0.701754; P: 0.715250; F1: 0.702592\n",
      "(train @ 23): L: 0.640289; A: 0.697368; R: 0.697368; P: 0.715433; F1: 0.694543\n",
      "(valid @ 23): L: 0.684302; A: 0.714912; R: 0.714912; P: 0.742093; F1: 0.716017\n",
      "(train @ 24): L: 0.614184; A: 0.717105; R: 0.717105; P: 0.731113; F1: 0.716014\n",
      "(valid @ 24): L: 0.645311; A: 0.723684; R: 0.723684; P: 0.753236; F1: 0.724539\n",
      "(train @ 25): L: 0.616364; A: 0.699561; R: 0.699561; P: 0.718844; F1: 0.692981\n",
      "(valid @ 25): L: 0.679705; A: 0.671053; R: 0.671053; P: 0.692543; F1: 0.664707\n",
      "(train @ 26): L: 0.604807; A: 0.717105; R: 0.717105; P: 0.719450; F1: 0.714780\n",
      "(valid @ 26): L: 0.651088; A: 0.719298; R: 0.719298; P: 0.778291; F1: 0.715755\n",
      "(train @ 27): L: 0.540390; A: 0.763158; R: 0.763158; P: 0.770626; F1: 0.761054\n",
      "(valid @ 27): L: 0.598124; A: 0.714912; R: 0.714912; P: 0.780205; F1: 0.702669\n",
      "(train @ 28): L: 0.558923; A: 0.743421; R: 0.743421; P: 0.765359; F1: 0.739624\n",
      "(valid @ 28): L: 0.536919; A: 0.758772; R: 0.758772; P: 0.767087; F1: 0.756413\n",
      "(train @ 29): L: 0.556073; A: 0.732456; R: 0.732456; P: 0.739903; F1: 0.731215\n",
      "(valid @ 29): L: 0.611624; A: 0.679825; R: 0.679825; P: 0.757452; F1: 0.656270\n",
      "(train @ 30): L: 0.545921; A: 0.752193; R: 0.752193; P: 0.756613; F1: 0.751340\n",
      "(valid @ 30): L: 0.648781; A: 0.653509; R: 0.653509; P: 0.724173; F1: 0.622159\n",
      "Best val Metric 0.758654 @ 19\n",
      "\n",
      "models are saved @ ./predictions/211209084739/flvl_1_3_4_6_7_9_pretrained_model_c1.pt\n",
      "Saving predictions @ ./predictions/211209084739/flvl_train_1_3_4_6_7_9_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209084739/flvl_valid_2_5_8_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209084739/flvl_public_test_trained_on_1_3_4_6_7_9_r21d_rgb.csv\n",
      "{'train': [2, 3, 5, 6, 8, 9], 'valid': [1, 4, 7], 'public_test': [10, 11, 12], 'private_test': [13, 14, 15]}\n",
      "(train @ 1): L: 1.011791; A: 0.449561; R: 0.449561; P: 0.455730; F1: 0.445592\n",
      "(valid @ 1): L: 0.971525; A: 0.548246; R: 0.548246; P: 0.466993; F1: 0.479648\n",
      "(train @ 2): L: 0.964494; A: 0.486842; R: 0.486842; P: 0.434345; F1: 0.440239\n",
      "(valid @ 2): L: 0.990433; A: 0.552632; R: 0.552632; P: 0.463872; F1: 0.504224\n",
      "(train @ 3): L: 0.949482; A: 0.504386; R: 0.504386; P: 0.528187; F1: 0.472587\n",
      "(valid @ 3): L: 0.946000; A: 0.587719; R: 0.587719; P: 0.623875; F1: 0.580673\n",
      "(train @ 4): L: 1.017020; A: 0.449561; R: 0.449561; P: 0.456561; F1: 0.427933\n",
      "(valid @ 4): L: 0.966963; A: 0.583333; R: 0.583333; P: 0.491691; F1: 0.526046\n",
      "(train @ 5): L: 0.922002; A: 0.543860; R: 0.543860; P: 0.622388; F1: 0.514092\n",
      "(valid @ 5): L: 0.918641; A: 0.570175; R: 0.570175; P: 0.636766; F1: 0.533234\n",
      "(train @ 6): L: 0.877097; A: 0.486842; R: 0.486842; P: 0.557600; F1: 0.422545\n",
      "(valid @ 6): L: 0.869609; A: 0.583333; R: 0.583333; P: 0.654307; F1: 0.569024\n",
      "(train @ 7): L: 0.896276; A: 0.504386; R: 0.504386; P: 0.609937; F1: 0.454662\n",
      "(valid @ 7): L: 0.881714; A: 0.491228; R: 0.491228; P: 0.559990; F1: 0.364802\n",
      "(train @ 8): L: 0.865793; A: 0.484649; R: 0.484649; P: 0.567726; F1: 0.410118\n",
      "(valid @ 8): L: 0.829404; A: 0.627193; R: 0.627193; P: 0.640057; F1: 0.619608\n",
      "(train @ 9): L: 0.832116; A: 0.559211; R: 0.559211; P: 0.678548; F1: 0.514164\n",
      "(valid @ 9): L: 0.831848; A: 0.622807; R: 0.622807; P: 0.663749; F1: 0.620036\n",
      "(train @ 10): L: 0.770621; A: 0.616228; R: 0.616228; P: 0.618149; F1: 0.616040\n",
      "(valid @ 10): L: 0.751679; A: 0.666667; R: 0.666667; P: 0.689916; F1: 0.668640\n",
      "(train @ 11): L: 0.755363; A: 0.603070; R: 0.603070; P: 0.663639; F1: 0.575256\n",
      "(valid @ 11): L: 0.878115; A: 0.627193; R: 0.627193; P: 0.671656; F1: 0.614991\n",
      "(train @ 12): L: 0.774709; A: 0.554825; R: 0.554825; P: 0.554264; F1: 0.542446\n",
      "(valid @ 12): L: 0.736440; A: 0.662281; R: 0.662281; P: 0.665000; F1: 0.662283\n",
      "(train @ 13): L: 0.738058; A: 0.603070; R: 0.603070; P: 0.668761; F1: 0.569965\n",
      "(valid @ 13): L: 0.719334; A: 0.662281; R: 0.662281; P: 0.681057; F1: 0.665523\n",
      "(train @ 14): L: 0.733241; A: 0.600877; R: 0.600877; P: 0.609140; F1: 0.594495\n",
      "(valid @ 14): L: 0.793171; A: 0.653509; R: 0.653509; P: 0.683671; F1: 0.649769\n",
      "(train @ 15): L: 0.684126; A: 0.638158; R: 0.638158; P: 0.679715; F1: 0.620658\n",
      "(valid @ 15): L: 0.707101; A: 0.675439; R: 0.675439; P: 0.696790; F1: 0.677346\n",
      "(train @ 16): L: 0.677051; A: 0.677632; R: 0.677632; P: 0.690145; F1: 0.672589\n",
      "(valid @ 16): L: 0.671424; A: 0.710526; R: 0.710526; P: 0.719853; F1: 0.711783\n",
      "(train @ 17): L: 0.656374; A: 0.684211; R: 0.684211; P: 0.697771; F1: 0.680772\n",
      "(valid @ 17): L: 0.678039; A: 0.692982; R: 0.692982; P: 0.718640; F1: 0.693510\n",
      "(train @ 18): L: 0.715160; A: 0.581140; R: 0.581140; P: 0.588939; F1: 0.573176\n",
      "(valid @ 18): L: 0.688114; A: 0.675439; R: 0.675439; P: 0.690162; F1: 0.677422\n",
      "(train @ 19): L: 0.661073; A: 0.686404; R: 0.686404; P: 0.695891; F1: 0.682497\n",
      "(valid @ 19): L: 0.768034; A: 0.657895; R: 0.657895; P: 0.687000; F1: 0.655705\n",
      "(train @ 20): L: 0.608817; A: 0.725877; R: 0.725877; P: 0.743746; F1: 0.720198\n",
      "(valid @ 20): L: 0.710533; A: 0.671053; R: 0.671053; P: 0.681444; F1: 0.668041\n",
      "(train @ 21): L: 0.579423; A: 0.717105; R: 0.717105; P: 0.716723; F1: 0.716070\n",
      "(valid @ 21): L: 0.709592; A: 0.631579; R: 0.631579; P: 0.683953; F1: 0.616134\n",
      "(train @ 22): L: 0.679030; A: 0.642544; R: 0.642544; P: 0.657443; F1: 0.633338\n",
      "(valid @ 22): L: 0.731240; A: 0.640351; R: 0.640351; P: 0.653458; F1: 0.635657\n",
      "(train @ 23): L: 0.582496; A: 0.725877; R: 0.725877; P: 0.731272; F1: 0.723585\n",
      "(valid @ 23): L: 0.788161; A: 0.640351; R: 0.640351; P: 0.671915; F1: 0.636843\n",
      "(train @ 24): L: 0.568803; A: 0.721491; R: 0.721491; P: 0.728609; F1: 0.720109\n",
      "(valid @ 24): L: 0.832879; A: 0.635965; R: 0.635965; P: 0.670319; F1: 0.627834\n",
      "(train @ 25): L: 0.577176; A: 0.706140; R: 0.706140; P: 0.724601; F1: 0.698595\n",
      "(valid @ 25): L: 0.895493; A: 0.583333; R: 0.583333; P: 0.626399; F1: 0.566967\n",
      "(train @ 26): L: 0.574567; A: 0.708333; R: 0.708333; P: 0.708529; F1: 0.706344\n",
      "(valid @ 26): L: 0.719008; A: 0.688596; R: 0.688596; P: 0.724693; F1: 0.685520\n",
      "(train @ 27): L: 0.502349; A: 0.774123; R: 0.774123; P: 0.778764; F1: 0.773914\n",
      "(valid @ 27): L: 0.637075; A: 0.697368; R: 0.697368; P: 0.701927; F1: 0.697207\n",
      "(train @ 28): L: 0.540091; A: 0.760965; R: 0.760965; P: 0.770622; F1: 0.758913\n",
      "(valid @ 28): L: 0.705339; A: 0.653509; R: 0.653509; P: 0.653999; F1: 0.652642\n",
      "(train @ 29): L: 0.491226; A: 0.769737; R: 0.769737; P: 0.771614; F1: 0.768071\n",
      "(valid @ 29): L: 0.679970; A: 0.688596; R: 0.688596; P: 0.728223; F1: 0.681232\n",
      "(train @ 30): L: 0.533453; A: 0.745614; R: 0.745614; P: 0.747227; F1: 0.746178\n",
      "(valid @ 30): L: 0.663221; A: 0.684211; R: 0.684211; P: 0.684851; F1: 0.684279\n",
      "Best val Metric 0.711783 @ 16\n",
      "\n",
      "models are saved @ ./predictions/211209084739/flvl_2_3_5_6_8_9_pretrained_model_c1.pt\n",
      "Saving predictions @ ./predictions/211209084739/flvl_train_2_3_5_6_8_9_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209084739/flvl_valid_1_4_7_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209084739/flvl_public_test_trained_on_2_3_5_6_8_9_r21d_rgb.csv\n",
      "Average of Best Metrics on Each Valid Set: 0.723694, 211209084739\n",
      "Saved test results @ ./predictions/211209084739/flvl_public_test_agg_r21d_rgb.csv\n"
     ]
    }
   ],
   "source": [
    "# View 4\n",
    "\n",
    "# define architecture and run k-fold training\n",
    "from main_1View import Config, run_kfold\n",
    "cfg = Config()\n",
    "cfg.assign_variable('task', 'flvl')\n",
    "cfg.assign_variable('output_dim', 3)\n",
    "cfg.assign_variable('model_type', 'GRU')\n",
    "cfg.assign_variable('bi_dir', False)\n",
    "cfg.assign_variable('device', 'cuda:0')\n",
    "cfg.assign_variable('data_root', './r21d_rgb_features')\n",
    "cfg.assign_variable('drop_p', 0.0) # results will be irreproducible\n",
    "cfg.assign_variable('batch_size', 64)\n",
    "cfg.assign_variable('input_dim', 512)\n",
    "cfg.assign_variable('hidden_dim', 512)\n",
    "cfg.assign_variable('n_layers', 3)\n",
    "cfg.assign_variable('num_epochs', 30)\n",
    "cfg.assign_variable('seed', 1337)\n",
    "\n",
    "run_kfold(cfg,[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tests with 5 containers ( container 2,3,4,5 and 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### A. Benchmark, uses all 4 views with 4 GRU units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': [2, 3, 5], 'valid': [4, 6], 'public_test': [10, 11, 12], 'private_test': [13, 14, 15]}\n",
      "(train @ 1): L: 1.874717; A: 0.420635; R: 0.420635; P: 0.431424; F1: 0.393776\n",
      "(valid @ 1): L: 1.342260; A: 0.428571; R: 0.428571; P: 0.183673; F1: 0.257143\n",
      "(train @ 2): L: 0.952471; A: 0.583333; R: 0.583333; P: 0.593610; F1: 0.578037\n",
      "(valid @ 2): L: 1.199909; A: 0.494048; R: 0.494048; P: 0.567507; F1: 0.394745\n",
      "(train @ 3): L: 0.867329; A: 0.555556; R: 0.555556; P: 0.639436; F1: 0.530081\n",
      "(valid @ 3): L: 1.010221; A: 0.422619; R: 0.422619; P: 0.677992; F1: 0.322202\n",
      "(train @ 4): L: 0.732311; A: 0.638889; R: 0.638889; P: 0.649580; F1: 0.619331\n",
      "(valid @ 4): L: 0.959502; A: 0.601190; R: 0.601190; P: 0.693404; F1: 0.578213\n",
      "(train @ 5): L: 0.763405; A: 0.599206; R: 0.599206; P: 0.668233; F1: 0.580467\n",
      "(valid @ 5): L: 1.064238; A: 0.535714; R: 0.535714; P: 0.645287; F1: 0.535033\n",
      "(train @ 6): L: 0.623321; A: 0.722222; R: 0.722222; P: 0.736613; F1: 0.719252\n",
      "(valid @ 6): L: 0.891119; A: 0.494048; R: 0.494048; P: 0.505510; F1: 0.491705\n",
      "(train @ 7): L: 0.564645; A: 0.726190; R: 0.726190; P: 0.744361; F1: 0.726730\n",
      "(valid @ 7): L: 1.078671; A: 0.559524; R: 0.559524; P: 0.667446; F1: 0.559961\n",
      "(train @ 8): L: 0.568008; A: 0.722222; R: 0.722222; P: 0.730175; F1: 0.719652\n",
      "(valid @ 8): L: 0.892289; A: 0.494048; R: 0.494048; P: 0.483482; F1: 0.479334\n",
      "(train @ 9): L: 0.584647; A: 0.702381; R: 0.702381; P: 0.718894; F1: 0.700403\n",
      "(valid @ 9): L: 1.632263; A: 0.505952; R: 0.505952; P: 0.663672; F1: 0.492220\n",
      "(train @ 10): L: 0.781477; A: 0.646825; R: 0.646825; P: 0.681472; F1: 0.639333\n",
      "(valid @ 10): L: 1.101401; A: 0.577381; R: 0.577381; P: 0.686444; F1: 0.570942\n",
      "(train @ 11): L: 0.513326; A: 0.769841; R: 0.769841; P: 0.792125; F1: 0.764937\n",
      "(valid @ 11): L: 0.732837; A: 0.583333; R: 0.583333; P: 0.584794; F1: 0.582284\n",
      "(train @ 12): L: 0.496523; A: 0.769841; R: 0.769841; P: 0.779563; F1: 0.770801\n",
      "(valid @ 12): L: 1.114039; A: 0.583333; R: 0.583333; P: 0.707928; F1: 0.581886\n",
      "(train @ 13): L: 0.468345; A: 0.801587; R: 0.801587; P: 0.801903; F1: 0.801519\n",
      "(valid @ 13): L: 0.740969; A: 0.630952; R: 0.630952; P: 0.628030; F1: 0.628645\n",
      "(train @ 14): L: 0.441273; A: 0.757937; R: 0.757937; P: 0.764903; F1: 0.757918\n",
      "(valid @ 14): L: 0.843964; A: 0.684524; R: 0.684524; P: 0.698086; F1: 0.679223\n",
      "(train @ 15): L: 0.383161; A: 0.841270; R: 0.841270; P: 0.840882; F1: 0.840860\n",
      "(valid @ 15): L: 1.028045; A: 0.648810; R: 0.648810; P: 0.692552; F1: 0.648902\n",
      "(train @ 16): L: 0.455465; A: 0.757937; R: 0.757937; P: 0.760267; F1: 0.758767\n",
      "(valid @ 16): L: 0.988313; A: 0.678571; R: 0.678571; P: 0.727684; F1: 0.661494\n",
      "(train @ 17): L: 0.343555; A: 0.853175; R: 0.853175; P: 0.860333; F1: 0.852259\n",
      "(valid @ 17): L: 0.802675; A: 0.648810; R: 0.648810; P: 0.642314; F1: 0.642353\n",
      "(train @ 18): L: 0.403725; A: 0.817460; R: 0.817460; P: 0.818818; F1: 0.817210\n",
      "(valid @ 18): L: 1.019656; A: 0.666667; R: 0.666667; P: 0.705329; F1: 0.661002\n",
      "(train @ 19): L: 0.385478; A: 0.857143; R: 0.857143; P: 0.865306; F1: 0.856144\n",
      "(valid @ 19): L: 0.898229; A: 0.690476; R: 0.690476; P: 0.710012; F1: 0.683642\n",
      "(train @ 20): L: 0.546306; A: 0.702381; R: 0.702381; P: 0.723615; F1: 0.693767\n",
      "(valid @ 20): L: 0.819751; A: 0.678571; R: 0.678571; P: 0.689229; F1: 0.671522\n",
      "(train @ 21): L: 0.512431; A: 0.730159; R: 0.730159; P: 0.784059; F1: 0.712137\n",
      "(valid @ 21): L: 0.813969; A: 0.678571; R: 0.678571; P: 0.692176; F1: 0.670759\n",
      "(train @ 22): L: 0.441109; A: 0.793651; R: 0.793651; P: 0.860714; F1: 0.780956\n",
      "(valid @ 22): L: 0.720109; A: 0.702381; R: 0.702381; P: 0.699610; F1: 0.698189\n",
      "(train @ 23): L: 0.470701; A: 0.777778; R: 0.777778; P: 0.824424; F1: 0.767186\n",
      "(valid @ 23): L: 0.830377; A: 0.714286; R: 0.714286; P: 0.770000; F1: 0.698307\n",
      "(train @ 24): L: 0.323499; A: 0.845238; R: 0.845238; P: 0.863492; F1: 0.842782\n",
      "(valid @ 24): L: 0.908231; A: 0.690476; R: 0.690476; P: 0.752171; F1: 0.674939\n",
      "(train @ 25): L: 0.319459; A: 0.861111; R: 0.861111; P: 0.861136; F1: 0.861108\n",
      "(valid @ 25): L: 0.695396; A: 0.684524; R: 0.684524; P: 0.680518; F1: 0.679489\n",
      "(train @ 26): L: 0.302398; A: 0.888889; R: 0.888889; P: 0.900162; F1: 0.887928\n",
      "(valid @ 26): L: 0.982842; A: 0.684524; R: 0.684524; P: 0.748385; F1: 0.667481\n",
      "(train @ 27): L: 0.453353; A: 0.781746; R: 0.781746; P: 0.836261; F1: 0.769905\n",
      "(valid @ 27): L: 0.700868; A: 0.738095; R: 0.738095; P: 0.749206; F1: 0.733938\n",
      "(train @ 28): L: 0.540498; A: 0.750000; R: 0.750000; P: 0.803157; F1: 0.735097\n",
      "(valid @ 28): L: 0.682489; A: 0.732143; R: 0.732143; P: 0.740988; F1: 0.726311\n",
      "(train @ 29): L: 0.469377; A: 0.801587; R: 0.801587; P: 0.834107; F1: 0.795250\n",
      "(valid @ 29): L: 1.104261; A: 0.601190; R: 0.601190; P: 0.705605; F1: 0.555946\n",
      "(train @ 30): L: 0.458696; A: 0.742063; R: 0.742063; P: 0.743852; F1: 0.741393\n",
      "(valid @ 30): L: 0.646984; A: 0.660714; R: 0.660714; P: 0.688341; F1: 0.639412\n",
      "Best val Metric 0.733938 @ 27\n",
      "\n",
      "models are saved @ ./predictions/211209091051/flvl_2_3_5_pretrained_model_c1.pt\n",
      "Saving predictions @ ./predictions/211209091051/flvl_train_2_3_5_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209091051/flvl_valid_4_6_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209091051/flvl_public_test_trained_on_2_3_5_r21d_rgb.csv\n",
      "{'train': [2, 4, 5], 'valid': [3, 6], 'public_test': [10, 11, 12], 'private_test': [13, 14, 15]}\n",
      "(train @ 1): L: 1.790031; A: 0.440476; R: 0.440476; P: 0.426189; F1: 0.412726\n",
      "(valid @ 1): L: 1.125827; A: 0.464286; R: 0.464286; P: 0.333333; F1: 0.320879\n",
      "(train @ 2): L: 0.916659; A: 0.507937; R: 0.507937; P: 0.512114; F1: 0.495375\n",
      "(valid @ 2): L: 1.267182; A: 0.494048; R: 0.494048; P: 0.591918; F1: 0.389102\n",
      "(train @ 3): L: 0.856561; A: 0.619048; R: 0.619048; P: 0.723806; F1: 0.595790\n",
      "(valid @ 3): L: 0.962912; A: 0.458333; R: 0.458333; P: 0.269424; F1: 0.337823\n",
      "(train @ 4): L: 0.755681; A: 0.638889; R: 0.638889; P: 0.680641; F1: 0.624104\n",
      "(valid @ 4): L: 1.060744; A: 0.523810; R: 0.523810; P: 0.655321; F1: 0.486933\n",
      "(train @ 5): L: 0.658083; A: 0.710317; R: 0.710317; P: 0.771074; F1: 0.706455\n",
      "(valid @ 5): L: 0.953629; A: 0.571429; R: 0.571429; P: 0.653295; F1: 0.581994\n",
      "(train @ 6): L: 0.625804; A: 0.718254; R: 0.718254; P: 0.725247; F1: 0.720183\n",
      "(valid @ 6): L: 0.863937; A: 0.577381; R: 0.577381; P: 0.605511; F1: 0.579939\n",
      "(train @ 7): L: 0.635843; A: 0.666667; R: 0.666667; P: 0.692418; F1: 0.670026\n",
      "(valid @ 7): L: 1.056441; A: 0.565476; R: 0.565476; P: 0.693262; F1: 0.561684\n",
      "(train @ 8): L: 0.557274; A: 0.730159; R: 0.730159; P: 0.735225; F1: 0.728906\n",
      "(valid @ 8): L: 0.860140; A: 0.571429; R: 0.571429; P: 0.591311; F1: 0.566717\n",
      "(train @ 9): L: 0.609797; A: 0.757937; R: 0.757937; P: 0.774346; F1: 0.759604\n",
      "(valid @ 9): L: 1.618641; A: 0.470238; R: 0.470238; P: 0.651934; F1: 0.449915\n",
      "(train @ 10): L: 0.627306; A: 0.670635; R: 0.670635; P: 0.677320; F1: 0.667530\n",
      "(valid @ 10): L: 0.933870; A: 0.613095; R: 0.613095; P: 0.679445; F1: 0.613723\n",
      "(train @ 11): L: 0.514117; A: 0.738095; R: 0.738095; P: 0.740735; F1: 0.739176\n",
      "(valid @ 11): L: 0.810540; A: 0.636905; R: 0.636905; P: 0.688451; F1: 0.629409\n",
      "(train @ 12): L: 0.432664; A: 0.861111; R: 0.861111; P: 0.861136; F1: 0.861108\n",
      "(valid @ 12): L: 0.823719; A: 0.642857; R: 0.642857; P: 0.671650; F1: 0.640376\n",
      "(train @ 13): L: 0.438245; A: 0.825397; R: 0.825397; P: 0.829309; F1: 0.826517\n",
      "(valid @ 13): L: 0.992688; A: 0.607143; R: 0.607143; P: 0.713691; F1: 0.587387\n",
      "(train @ 14): L: 0.576274; A: 0.686508; R: 0.686508; P: 0.685642; F1: 0.685525\n",
      "(valid @ 14): L: 0.849448; A: 0.654762; R: 0.654762; P: 0.692747; F1: 0.645028\n",
      "(train @ 15): L: 0.546065; A: 0.690476; R: 0.690476; P: 0.694971; F1: 0.691188\n",
      "(valid @ 15): L: 0.826963; A: 0.660714; R: 0.660714; P: 0.734167; F1: 0.631068\n",
      "(train @ 16): L: 0.549628; A: 0.734127; R: 0.734127; P: 0.753230; F1: 0.729049\n",
      "(valid @ 16): L: 0.793302; A: 0.660714; R: 0.660714; P: 0.708521; F1: 0.647499\n",
      "(train @ 17): L: 0.420008; A: 0.805556; R: 0.805556; P: 0.854742; F1: 0.796733\n",
      "(valid @ 17): L: 0.805096; A: 0.666667; R: 0.666667; P: 0.750002; F1: 0.636353\n",
      "(train @ 18): L: 0.334307; A: 0.892857; R: 0.892857; P: 0.893548; F1: 0.892800\n",
      "(valid @ 18): L: 0.654827; A: 0.684524; R: 0.684524; P: 0.681290; F1: 0.679069\n",
      "(train @ 19): L: 0.333285; A: 0.896825; R: 0.896825; P: 0.906122; F1: 0.896104\n",
      "(valid @ 19): L: 1.131997; A: 0.625000; R: 0.625000; P: 0.723397; F1: 0.576251\n",
      "(train @ 20): L: 0.510279; A: 0.801587; R: 0.801587; P: 0.842597; F1: 0.793791\n",
      "(valid @ 20): L: 0.618087; A: 0.708333; R: 0.708333; P: 0.710372; F1: 0.709276\n",
      "(train @ 21): L: 0.476374; A: 0.753968; R: 0.753968; P: 0.790296; F1: 0.743317\n",
      "(valid @ 21): L: 1.113422; A: 0.619048; R: 0.619048; P: 0.718082; F1: 0.574812\n",
      "(train @ 22): L: 0.508627; A: 0.761905; R: 0.761905; P: 0.801903; F1: 0.751272\n",
      "(valid @ 22): L: 0.621515; A: 0.696429; R: 0.696429; P: 0.698908; F1: 0.690107\n",
      "(train @ 23): L: 0.610123; A: 0.690476; R: 0.690476; P: 0.714168; F1: 0.677076\n",
      "(valid @ 23): L: 0.994607; A: 0.630952; R: 0.630952; P: 0.738161; F1: 0.567879\n",
      "(train @ 24): L: 0.454958; A: 0.801587; R: 0.801587; P: 0.847392; F1: 0.792998\n",
      "(valid @ 24): L: 0.549809; A: 0.732143; R: 0.732143; P: 0.736510; F1: 0.727861\n",
      "(train @ 25): L: 0.346887; A: 0.821429; R: 0.821429; P: 0.843876; F1: 0.817673\n",
      "(valid @ 25): L: 0.593416; A: 0.708333; R: 0.708333; P: 0.724176; F1: 0.698060\n",
      "(train @ 26): L: 0.323009; A: 0.849206; R: 0.849206; P: 0.852679; F1: 0.848739\n",
      "(valid @ 26): L: 0.611894; A: 0.732143; R: 0.732143; P: 0.783495; F1: 0.716257\n",
      "(train @ 27): L: 0.324752; A: 0.837302; R: 0.837302; P: 0.844057; F1: 0.836288\n",
      "(valid @ 27): L: 0.618887; A: 0.732143; R: 0.732143; P: 0.786453; F1: 0.714740\n",
      "(train @ 28): L: 0.329481; A: 0.861111; R: 0.861111; P: 0.870363; F1: 0.860028\n",
      "(valid @ 28): L: 0.574582; A: 0.750000; R: 0.750000; P: 0.776472; F1: 0.741864\n",
      "(train @ 29): L: 0.276207; A: 0.876984; R: 0.876984; P: 0.878273; F1: 0.876855\n",
      "(valid @ 29): L: 0.950461; A: 0.666667; R: 0.666667; P: 0.755263; F1: 0.627560\n",
      "(train @ 30): L: 0.391649; A: 0.785714; R: 0.785714; P: 0.789377; F1: 0.784810\n",
      "(valid @ 30): L: 0.513645; A: 0.761905; R: 0.761905; P: 0.781520; F1: 0.756215\n",
      "Best val Metric 0.756215 @ 30\n",
      "\n",
      "models are saved @ ./predictions/211209091051/flvl_2_4_5_pretrained_model_c1.pt\n",
      "Saving predictions @ ./predictions/211209091051/flvl_train_2_4_5_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209091051/flvl_valid_3_6_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209091051/flvl_public_test_trained_on_2_4_5_r21d_rgb.csv\n",
      "{'train': [3, 4, 6], 'valid': [2, 5], 'public_test': [10, 11, 12], 'private_test': [13, 14, 15]}\n",
      "(train @ 1): L: 1.720781; A: 0.424603; R: 0.424603; P: 0.366822; F1: 0.393547\n",
      "(valid @ 1): L: 1.599510; A: 0.428571; R: 0.428571; P: 0.183673; F1: 0.257143\n",
      "(train @ 2): L: 0.999915; A: 0.563492; R: 0.563492; P: 0.576723; F1: 0.549104\n",
      "(valid @ 2): L: 0.923590; A: 0.565476; R: 0.565476; P: 0.624216; F1: 0.495133\n",
      "(train @ 3): L: 0.999094; A: 0.503968; R: 0.503968; P: 0.444758; F1: 0.467802\n",
      "(valid @ 3): L: 0.985017; A: 0.470238; R: 0.470238; P: 0.336927; F1: 0.331678\n",
      "(train @ 4): L: 1.034779; A: 0.460317; R: 0.460317; P: 0.492030; F1: 0.439750\n",
      "(valid @ 4): L: 0.800856; A: 0.619048; R: 0.619048; P: 0.575218; F1: 0.572997\n",
      "(train @ 5): L: 0.842587; A: 0.543651; R: 0.543651; P: 0.565913; F1: 0.477448\n",
      "(valid @ 5): L: 0.836959; A: 0.428571; R: 0.428571; P: 0.207095; F1: 0.279250\n",
      "(train @ 6): L: 0.839038; A: 0.476190; R: 0.476190; P: 0.441300; F1: 0.424953\n",
      "(valid @ 6): L: 0.694947; A: 0.684524; R: 0.684524; P: 0.746716; F1: 0.663593\n",
      "(train @ 7): L: 0.794769; A: 0.638889; R: 0.638889; P: 0.665375; F1: 0.635062\n",
      "(valid @ 7): L: 0.663516; A: 0.672619; R: 0.672619; P: 0.738897; F1: 0.663173\n",
      "(train @ 8): L: 0.766677; A: 0.674603; R: 0.674603; P: 0.708411; F1: 0.664294\n",
      "(valid @ 8): L: 0.789632; A: 0.482143; R: 0.482143; P: 0.345865; F1: 0.353432\n",
      "(train @ 9): L: 0.729060; A: 0.630952; R: 0.630952; P: 0.661973; F1: 0.632768\n",
      "(valid @ 9): L: 0.601478; A: 0.702381; R: 0.702381; P: 0.797094; F1: 0.680954\n",
      "(train @ 10): L: 0.776585; A: 0.531746; R: 0.531746; P: 0.528185; F1: 0.522708\n",
      "(valid @ 10): L: 0.632732; A: 0.595238; R: 0.595238; P: 0.745803; F1: 0.507011\n",
      "(train @ 11): L: 0.639564; A: 0.626984; R: 0.626984; P: 0.643829; F1: 0.630223\n",
      "(valid @ 11): L: 0.681789; A: 0.738095; R: 0.738095; P: 0.769389; F1: 0.730295\n",
      "(train @ 12): L: 0.648468; A: 0.686508; R: 0.686508; P: 0.733231; F1: 0.681804\n",
      "(valid @ 12): L: 0.567059; A: 0.684524; R: 0.684524; P: 0.770000; F1: 0.646721\n",
      "(train @ 13): L: 0.591422; A: 0.702381; R: 0.702381; P: 0.719630; F1: 0.689277\n",
      "(valid @ 13): L: 0.538970; A: 0.779762; R: 0.779762; P: 0.797654; F1: 0.776439\n",
      "(train @ 14): L: 0.574111; A: 0.706349; R: 0.706349; P: 0.711673; F1: 0.707869\n",
      "(valid @ 14): L: 0.520424; A: 0.767857; R: 0.767857; P: 0.786626; F1: 0.764000\n",
      "(train @ 15): L: 0.509444; A: 0.757937; R: 0.757937; P: 0.769603; F1: 0.755583\n",
      "(valid @ 15): L: 0.463155; A: 0.803571; R: 0.803571; P: 0.836156; F1: 0.798989\n",
      "(train @ 16): L: 0.576539; A: 0.634921; R: 0.634921; P: 0.639675; F1: 0.621569\n",
      "(valid @ 16): L: 0.446182; A: 0.833333; R: 0.833333; P: 0.841753; F1: 0.833034\n",
      "(train @ 17): L: 0.484270; A: 0.742063; R: 0.742063; P: 0.815175; F1: 0.723091\n",
      "(valid @ 17): L: 0.450861; A: 0.803571; R: 0.803571; P: 0.816761; F1: 0.801634\n",
      "(train @ 18): L: 0.443432; A: 0.833333; R: 0.833333; P: 0.833423; F1: 0.833319\n",
      "(valid @ 18): L: 0.450599; A: 0.773810; R: 0.773810; P: 0.823626; F1: 0.762059\n",
      "(train @ 19): L: 0.457216; A: 0.793651; R: 0.793651; P: 0.794877; F1: 0.793367\n",
      "(valid @ 19): L: 0.379216; A: 0.880952; R: 0.880952; P: 0.884821; F1: 0.880584\n",
      "(train @ 20): L: 0.462046; A: 0.797619; R: 0.797619; P: 0.803366; F1: 0.796358\n",
      "(valid @ 20): L: 0.430726; A: 0.791667; R: 0.791667; P: 0.835646; F1: 0.782782\n",
      "(train @ 21): L: 0.411558; A: 0.829365; R: 0.829365; P: 0.831169; F1: 0.829068\n",
      "(valid @ 21): L: 0.366730; A: 0.863095; R: 0.863095; P: 0.867725; F1: 0.862558\n",
      "(train @ 22): L: 0.393281; A: 0.853175; R: 0.853175; P: 0.853392; F1: 0.853146\n",
      "(valid @ 22): L: 0.367808; A: 0.851190; R: 0.851190; P: 0.867708; F1: 0.849087\n",
      "(train @ 23): L: 0.375461; A: 0.845238; R: 0.845238; P: 0.850624; F1: 0.844488\n",
      "(valid @ 23): L: 0.330563; A: 0.880952; R: 0.880952; P: 0.883117; F1: 0.880745\n",
      "(train @ 24): L: 0.376767; A: 0.825397; R: 0.825397; P: 0.843697; F1: 0.822413\n",
      "(valid @ 24): L: 0.357193; A: 0.815476; R: 0.815476; P: 0.848933; F1: 0.809742\n",
      "(train @ 25): L: 0.359142; A: 0.817460; R: 0.817460; P: 0.852223; F1: 0.811630\n",
      "(valid @ 25): L: 0.547375; A: 0.714286; R: 0.714286; P: 0.812814; F1: 0.681817\n",
      "(train @ 26): L: 0.379975; A: 0.825397; R: 0.825397; P: 0.849839; F1: 0.821479\n",
      "(valid @ 26): L: 0.414952; A: 0.797619; R: 0.797619; P: 0.853297; F1: 0.787106\n",
      "(train @ 27): L: 0.349802; A: 0.841270; R: 0.841270; P: 0.852952; F1: 0.839606\n",
      "(valid @ 27): L: 0.383363; A: 0.827381; R: 0.827381; P: 0.869264; F1: 0.821091\n",
      "(train @ 28): L: 0.344832; A: 0.861111; R: 0.861111; P: 0.861136; F1: 0.861108\n",
      "(valid @ 28): L: 0.316531; A: 0.839286; R: 0.839286; P: 0.864191; F1: 0.835793\n",
      "(train @ 29): L: 0.393614; A: 0.809524; R: 0.809524; P: 0.819831; F1: 0.807527\n",
      "(valid @ 29): L: 0.319456; A: 0.833333; R: 0.833333; P: 0.866071; F1: 0.828571\n",
      "(train @ 30): L: 0.344972; A: 0.833333; R: 0.833333; P: 0.862146; F1: 0.829099\n",
      "(valid @ 30): L: 0.386369; A: 0.815476; R: 0.815476; P: 0.862735; F1: 0.807676\n",
      "Best val Metric 0.880745 @ 23\n",
      "\n",
      "models are saved @ ./predictions/211209091051/flvl_3_4_6_pretrained_model_c1.pt\n",
      "Saving predictions @ ./predictions/211209091051/flvl_train_3_4_6_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209091051/flvl_valid_2_5_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209091051/flvl_public_test_trained_on_3_4_6_r21d_rgb.csv\n",
      "Average of Best Metrics on Each Valid Set: 0.790299, 211209091051\n",
      "Saved test results @ ./predictions/211209091051/flvl_public_test_agg_r21d_rgb.csv\n"
     ]
    }
   ],
   "source": [
    "# Benchmark\n",
    "from main_NoOpaque import Config, run_kfold\n",
    "\n",
    "cfg = Config()\n",
    "cfg.assign_variable('task', 'flvl')\n",
    "cfg.assign_variable('output_dim', 3)\n",
    "cfg.assign_variable('model_type', 'GRU')\n",
    "cfg.assign_variable('bi_dir', False)\n",
    "cfg.assign_variable('device', 'cuda:0')\n",
    "cfg.assign_variable('data_root', './r21d_rgb_features')\n",
    "cfg.assign_variable('drop_p', 0.0) # results will be irreproducible\n",
    "cfg.assign_variable('batch_size', 64)\n",
    "cfg.assign_variable('input_dim', 512)\n",
    "cfg.assign_variable('hidden_dim', 512)\n",
    "cfg.assign_variable('n_layers', 3)\n",
    "cfg.assign_variable('num_epochs', 30)\n",
    "cfg.assign_variable('seed', 1337)\n",
    "\n",
    "run_kfold(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### B. Experiment with 3 Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': [2, 3, 5], 'valid': [4, 6], 'public_test': [10, 11, 12], 'private_test': [13, 14, 15]}\n",
      "(train @ 1): L: 1.530574; A: 0.416667; R: 0.416667; P: 0.335714; F1: 0.355357\n",
      "(valid @ 1): L: 0.939548; A: 0.500000; R: 0.500000; P: 0.430030; F1: 0.462319\n",
      "(train @ 2): L: 0.998559; A: 0.535714; R: 0.535714; P: 0.644065; F1: 0.497380\n",
      "(valid @ 2): L: 0.918618; A: 0.595238; R: 0.595238; P: 0.727373; F1: 0.562132\n",
      "(train @ 3): L: 0.873507; A: 0.464286; R: 0.464286; P: 0.370027; F1: 0.397980\n",
      "(valid @ 3): L: 0.895572; A: 0.565476; R: 0.565476; P: 0.560140; F1: 0.510365\n",
      "(train @ 4): L: 0.884331; A: 0.599206; R: 0.599206; P: 0.709513; F1: 0.570176\n",
      "(valid @ 4): L: 1.007370; A: 0.553571; R: 0.553571; P: 0.678458; F1: 0.553128\n",
      "(train @ 5): L: 0.740557; A: 0.662698; R: 0.662698; P: 0.679274; F1: 0.660154\n",
      "(valid @ 5): L: 0.856500; A: 0.559524; R: 0.559524; P: 0.621335; F1: 0.548050\n",
      "(train @ 6): L: 0.693994; A: 0.662698; R: 0.662698; P: 0.721745; F1: 0.625578\n",
      "(valid @ 6): L: 1.085029; A: 0.529762; R: 0.529762; P: 0.642723; F1: 0.512117\n",
      "(train @ 7): L: 0.749234; A: 0.670635; R: 0.670635; P: 0.747238; F1: 0.647796\n",
      "(valid @ 7): L: 0.848942; A: 0.547619; R: 0.547619; P: 0.547917; F1: 0.547441\n",
      "(train @ 8): L: 0.658944; A: 0.611111; R: 0.611111; P: 0.645902; F1: 0.570728\n",
      "(valid @ 8): L: 0.849490; A: 0.619048; R: 0.619048; P: 0.689333; F1: 0.604932\n",
      "(train @ 9): L: 0.618024; A: 0.674603; R: 0.674603; P: 0.777365; F1: 0.652769\n",
      "(valid @ 9): L: 0.985951; A: 0.571429; R: 0.571429; P: 0.664358; F1: 0.574189\n",
      "(train @ 10): L: 0.502668; A: 0.817460; R: 0.817460; P: 0.821768; F1: 0.818754\n",
      "(valid @ 10): L: 0.839815; A: 0.553571; R: 0.553571; P: 0.549081; F1: 0.547741\n",
      "(train @ 11): L: 0.614354; A: 0.678571; R: 0.678571; P: 0.685864; F1: 0.680339\n",
      "(valid @ 11): L: 1.128749; A: 0.541667; R: 0.541667; P: 0.656939; F1: 0.525193\n",
      "(train @ 12): L: 0.499744; A: 0.753968; R: 0.753968; P: 0.765731; F1: 0.755813\n",
      "(valid @ 12): L: 0.891495; A: 0.625000; R: 0.625000; P: 0.677245; F1: 0.620664\n",
      "(train @ 13): L: 0.466069; A: 0.781746; R: 0.781746; P: 0.785661; F1: 0.783168\n",
      "(valid @ 13): L: 1.228447; A: 0.529762; R: 0.529762; P: 0.658269; F1: 0.505062\n",
      "(train @ 14): L: 0.527129; A: 0.742063; R: 0.742063; P: 0.775303; F1: 0.733449\n",
      "(valid @ 14): L: 0.769400; A: 0.666667; R: 0.666667; P: 0.684950; F1: 0.662239\n",
      "(train @ 15): L: 0.430238; A: 0.837302; R: 0.837302; P: 0.853148; F1: 0.835153\n",
      "(valid @ 15): L: 0.684645; A: 0.714286; R: 0.714286; P: 0.723005; F1: 0.711417\n",
      "(train @ 16): L: 0.520312; A: 0.706349; R: 0.706349; P: 0.736250; F1: 0.692398\n",
      "(valid @ 16): L: 0.785684; A: 0.684524; R: 0.684524; P: 0.700055; F1: 0.679374\n",
      "(train @ 17): L: 0.437141; A: 0.797619; R: 0.797619; P: 0.835706; F1: 0.790055\n",
      "(valid @ 17): L: 0.797059; A: 0.684524; R: 0.684524; P: 0.689387; F1: 0.679382\n",
      "(train @ 18): L: 0.572853; A: 0.714286; R: 0.714286; P: 0.745945; F1: 0.700712\n",
      "(valid @ 18): L: 0.755065; A: 0.738095; R: 0.738095; P: 0.769524; F1: 0.729817\n",
      "(train @ 19): L: 0.437459; A: 0.777778; R: 0.777778; P: 0.800463; F1: 0.772859\n",
      "(valid @ 19): L: 0.729371; A: 0.702381; R: 0.702381; P: 0.712800; F1: 0.696288\n",
      "(train @ 20): L: 0.416081; A: 0.785714; R: 0.785714; P: 0.810505; F1: 0.779978\n",
      "(valid @ 20): L: 0.809658; A: 0.696429; R: 0.696429; P: 0.728520; F1: 0.686069\n",
      "(train @ 21): L: 0.386974; A: 0.817460; R: 0.817460; P: 0.818818; F1: 0.817210\n",
      "(valid @ 21): L: 0.679000; A: 0.755952; R: 0.755952; P: 0.791684; F1: 0.745636\n",
      "(train @ 22): L: 0.445247; A: 0.781746; R: 0.781746; P: 0.784210; F1: 0.781714\n",
      "(valid @ 22): L: 0.672900; A: 0.738095; R: 0.738095; P: 0.767728; F1: 0.726268\n",
      "(train @ 23): L: 0.503087; A: 0.714286; R: 0.714286; P: 0.742716; F1: 0.701917\n",
      "(valid @ 23): L: 0.693178; A: 0.696429; R: 0.696429; P: 0.718288; F1: 0.686992\n",
      "(train @ 24): L: 0.420381; A: 0.813492; R: 0.813492; P: 0.841898; F1: 0.808463\n",
      "(valid @ 24): L: 0.615563; A: 0.732143; R: 0.732143; P: 0.731156; F1: 0.728604\n",
      "(train @ 25): L: 0.311712; A: 0.900794; R: 0.900794; P: 0.913736; F1: 0.899847\n",
      "(valid @ 25): L: 0.657855; A: 0.750000; R: 0.750000; P: 0.776778; F1: 0.741575\n",
      "(train @ 26): L: 0.320730; A: 0.857143; R: 0.857143; P: 0.858719; F1: 0.856947\n",
      "(valid @ 26): L: 0.651550; A: 0.744048; R: 0.744048; P: 0.763664; F1: 0.737347\n",
      "(train @ 27): L: 0.267053; A: 0.912698; R: 0.912698; P: 0.913755; F1: 0.912631\n",
      "(valid @ 27): L: 0.624165; A: 0.738095; R: 0.738095; P: 0.740179; F1: 0.737284\n",
      "(train @ 28): L: 0.288481; A: 0.896825; R: 0.896825; P: 0.898621; F1: 0.896684\n",
      "(valid @ 28): L: 0.829048; A: 0.750000; R: 0.750000; P: 0.790594; F1: 0.738340\n",
      "(train @ 29): L: 0.274754; A: 0.873016; R: 0.873016; P: 0.883726; F1: 0.871918\n",
      "(valid @ 29): L: 0.626121; A: 0.738095; R: 0.738095; P: 0.739261; F1: 0.737640\n",
      "(train @ 30): L: 0.240769; A: 0.896825; R: 0.896825; P: 0.898621; F1: 0.896684\n",
      "(valid @ 30): L: 0.715800; A: 0.738095; R: 0.738095; P: 0.763088; F1: 0.729269\n",
      "Best val Metric 0.745636 @ 21\n",
      "\n",
      "models are saved @ ./predictions/211209093359/flvl_2_3_5_pretrained_model_c1.pt\n",
      "Saving predictions @ ./predictions/211209093359/flvl_train_2_3_5_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209093359/flvl_valid_4_6_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209093359/flvl_public_test_trained_on_2_3_5_r21d_rgb.csv\n",
      "{'train': [2, 4, 5], 'valid': [3, 6], 'public_test': [10, 11, 12], 'private_test': [13, 14, 15]}\n",
      "(train @ 1): L: 1.470985; A: 0.412698; R: 0.412698; P: 0.341588; F1: 0.351356\n",
      "(valid @ 1): L: 0.972265; A: 0.529762; R: 0.529762; P: 0.576612; F1: 0.452999\n",
      "(train @ 2): L: 1.104498; A: 0.531746; R: 0.531746; P: 0.625870; F1: 0.494037\n",
      "(valid @ 2): L: 0.935874; A: 0.523810; R: 0.523810; P: 0.527405; F1: 0.519283\n",
      "(train @ 3): L: 0.852842; A: 0.583333; R: 0.583333; P: 0.629827; F1: 0.531743\n",
      "(valid @ 3): L: 0.871528; A: 0.529762; R: 0.529762; P: 0.576612; F1: 0.452999\n",
      "(train @ 4): L: 0.829103; A: 0.579365; R: 0.579365; P: 0.707061; F1: 0.541915\n",
      "(valid @ 4): L: 0.878236; A: 0.589286; R: 0.589286; P: 0.650642; F1: 0.592017\n",
      "(train @ 5): L: 0.743385; A: 0.682540; R: 0.682540; P: 0.704127; F1: 0.681234\n",
      "(valid @ 5): L: 0.877989; A: 0.607143; R: 0.607143; P: 0.746004; F1: 0.575116\n",
      "(train @ 6): L: 0.684418; A: 0.646825; R: 0.646825; P: 0.692381; F1: 0.643730\n",
      "(valid @ 6): L: 1.009369; A: 0.535714; R: 0.535714; P: 0.661269; F1: 0.547373\n",
      "(train @ 7): L: 0.651301; A: 0.710317; R: 0.710317; P: 0.743546; F1: 0.704821\n",
      "(valid @ 7): L: 0.788617; A: 0.642857; R: 0.642857; P: 0.661394; F1: 0.645326\n",
      "(train @ 8): L: 0.684215; A: 0.634921; R: 0.634921; P: 0.638956; F1: 0.615757\n",
      "(valid @ 8): L: 1.261700; A: 0.482143; R: 0.482143; P: 0.660391; F1: 0.467508\n",
      "(train @ 9): L: 0.602768; A: 0.706349; R: 0.706349; P: 0.775167; F1: 0.690345\n",
      "(valid @ 9): L: 0.767830; A: 0.619048; R: 0.619048; P: 0.638310; F1: 0.619471\n",
      "(train @ 10): L: 0.622785; A: 0.670635; R: 0.670635; P: 0.684997; F1: 0.674360\n",
      "(valid @ 10): L: 0.993674; A: 0.583333; R: 0.583333; P: 0.676949; F1: 0.590847\n",
      "(train @ 11): L: 0.650218; A: 0.690476; R: 0.690476; P: 0.688075; F1: 0.688353\n",
      "(valid @ 11): L: 0.813309; A: 0.607143; R: 0.607143; P: 0.712483; F1: 0.591483\n",
      "(train @ 12): L: 0.558742; A: 0.781746; R: 0.781746; P: 0.792291; F1: 0.781245\n",
      "(valid @ 12): L: 0.751699; A: 0.642857; R: 0.642857; P: 0.721003; F1: 0.622408\n",
      "(train @ 13): L: 0.456233; A: 0.801587; R: 0.801587; P: 0.808795; F1: 0.801877\n",
      "(valid @ 13): L: 1.343072; A: 0.470238; R: 0.470238; P: 0.649250; F1: 0.452482\n",
      "(train @ 14): L: 0.532978; A: 0.722222; R: 0.722222; P: 0.722319; F1: 0.721075\n",
      "(valid @ 14): L: 0.638682; A: 0.714286; R: 0.714286; P: 0.730791; F1: 0.714286\n",
      "(train @ 15): L: 0.458908; A: 0.813492; R: 0.813492; P: 0.818822; F1: 0.814895\n",
      "(valid @ 15): L: 0.766899; A: 0.678571; R: 0.678571; P: 0.760671; F1: 0.647248\n",
      "(train @ 16): L: 0.421637; A: 0.805556; R: 0.805556; P: 0.813706; F1: 0.803481\n",
      "(valid @ 16): L: 0.716537; A: 0.684524; R: 0.684524; P: 0.697252; F1: 0.680752\n",
      "(train @ 17): L: 0.437854; A: 0.801587; R: 0.801587; P: 0.836347; F1: 0.793937\n",
      "(valid @ 17): L: 0.846564; A: 0.672619; R: 0.672619; P: 0.771145; F1: 0.628752\n",
      "(train @ 18): L: 0.568883; A: 0.769841; R: 0.769841; P: 0.850258; F1: 0.751957\n",
      "(valid @ 18): L: 0.587666; A: 0.714286; R: 0.714286; P: 0.711465; F1: 0.712512\n",
      "(train @ 19): L: 0.511565; A: 0.710317; R: 0.710317; P: 0.775382; F1: 0.685212\n",
      "(valid @ 19): L: 1.064181; A: 0.613095; R: 0.613095; P: 0.714050; F1: 0.573580\n",
      "(train @ 20): L: 0.461408; A: 0.746032; R: 0.746032; P: 0.783161; F1: 0.735091\n",
      "(valid @ 20): L: 0.644020; A: 0.714286; R: 0.714286; P: 0.763719; F1: 0.696176\n",
      "(train @ 21): L: 0.437801; A: 0.793651; R: 0.793651; P: 0.801860; F1: 0.792565\n",
      "(valid @ 21): L: 0.621789; A: 0.714286; R: 0.714286; P: 0.795556; F1: 0.687051\n",
      "(train @ 22): L: 0.417173; A: 0.769841; R: 0.769841; P: 0.772321; F1: 0.769129\n",
      "(valid @ 22): L: 0.594380; A: 0.708333; R: 0.708333; P: 0.775120; F1: 0.683005\n",
      "(train @ 23): L: 0.425909; A: 0.785714; R: 0.785714; P: 0.833204; F1: 0.775534\n",
      "(valid @ 23): L: 0.519736; A: 0.726190; R: 0.726190; P: 0.739135; F1: 0.718713\n",
      "(train @ 24): L: 0.386743; A: 0.845238; R: 0.845238; P: 0.869823; F1: 0.841983\n",
      "(valid @ 24): L: 0.516536; A: 0.744048; R: 0.744048; P: 0.774589; F1: 0.734143\n",
      "(train @ 25): L: 0.333813; A: 0.865079; R: 0.865079; P: 0.886239; F1: 0.862773\n",
      "(valid @ 25): L: 0.490361; A: 0.750000; R: 0.750000; P: 0.764931; F1: 0.745083\n",
      "(train @ 26): L: 0.296722; A: 0.876984; R: 0.876984; P: 0.878273; F1: 0.876855\n",
      "(valid @ 26): L: 0.537383; A: 0.750000; R: 0.750000; P: 0.787515; F1: 0.738657\n",
      "(train @ 27): L: 0.269351; A: 0.896825; R: 0.896825; P: 0.897272; F1: 0.896790\n",
      "(valid @ 27): L: 0.512230; A: 0.761905; R: 0.761905; P: 0.790468; F1: 0.753881\n",
      "(train @ 28): L: 0.258225; A: 0.912698; R: 0.912698; P: 0.912815; F1: 0.912691\n",
      "(valid @ 28): L: 0.542314; A: 0.761905; R: 0.761905; P: 0.798738; F1: 0.752075\n",
      "(train @ 29): L: 0.241616; A: 0.896825; R: 0.896825; P: 0.900893; F1: 0.896506\n",
      "(valid @ 29): L: 0.444777; A: 0.773810; R: 0.773810; P: 0.774436; F1: 0.773635\n",
      "(train @ 30): L: 0.277935; A: 0.853175; R: 0.853175; P: 0.860333; F1: 0.852259\n",
      "(valid @ 30): L: 0.612151; A: 0.744048; R: 0.744048; P: 0.815725; F1: 0.723787\n",
      "Best val Metric 0.773635 @ 29\n",
      "\n",
      "models are saved @ ./predictions/211209093359/flvl_2_4_5_pretrained_model_c1.pt\n",
      "Saving predictions @ ./predictions/211209093359/flvl_train_2_4_5_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209093359/flvl_valid_3_6_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209093359/flvl_public_test_trained_on_2_4_5_r21d_rgb.csv\n",
      "{'train': [3, 4, 6], 'valid': [2, 5], 'public_test': [10, 11, 12], 'private_test': [13, 14, 15]}\n",
      "(train @ 1): L: 1.527190; A: 0.408730; R: 0.408730; P: 0.340269; F1: 0.349231\n",
      "(valid @ 1): L: 1.015274; A: 0.553571; R: 0.553571; P: 0.619790; F1: 0.478944\n",
      "(train @ 2): L: 1.181152; A: 0.416667; R: 0.416667; P: 0.554645; F1: 0.351187\n",
      "(valid @ 2): L: 0.968461; A: 0.428571; R: 0.428571; P: 0.183673; F1: 0.257143\n",
      "(train @ 3): L: 0.996547; A: 0.436508; R: 0.436508; P: 0.471601; F1: 0.274748\n",
      "(valid @ 3): L: 0.917963; A: 0.595238; R: 0.595238; P: 0.593060; F1: 0.541443\n",
      "(train @ 4): L: 0.975566; A: 0.488095; R: 0.488095; P: 0.548672; F1: 0.388837\n",
      "(valid @ 4): L: 0.828517; A: 0.464286; R: 0.464286; P: 0.318641; F1: 0.340062\n",
      "(train @ 5): L: 0.874395; A: 0.559524; R: 0.559524; P: 0.586868; F1: 0.546276\n",
      "(valid @ 5): L: 0.792277; A: 0.553571; R: 0.553571; P: 0.463237; F1: 0.479309\n",
      "(train @ 6): L: 0.854195; A: 0.476190; R: 0.476190; P: 0.408621; F1: 0.439819\n",
      "(valid @ 6): L: 0.771481; A: 0.642857; R: 0.642857; P: 0.548377; F1: 0.591541\n",
      "(train @ 7): L: 0.813027; A: 0.646825; R: 0.646825; P: 0.742274; F1: 0.631005\n",
      "(valid @ 7): L: 0.684553; A: 0.750000; R: 0.750000; P: 0.768057; F1: 0.745605\n",
      "(train @ 8): L: 0.825531; A: 0.634921; R: 0.634921; P: 0.663217; F1: 0.599465\n",
      "(valid @ 8): L: 0.695476; A: 0.636905; R: 0.636905; P: 0.675101; F1: 0.613937\n",
      "(train @ 9): L: 0.735722; A: 0.694444; R: 0.694444; P: 0.762008; F1: 0.688917\n",
      "(valid @ 9): L: 0.674362; A: 0.642857; R: 0.642857; P: 0.778154; F1: 0.619254\n",
      "(train @ 10): L: 0.695300; A: 0.603175; R: 0.603175; P: 0.627083; F1: 0.593604\n",
      "(valid @ 10): L: 0.775769; A: 0.529762; R: 0.529762; P: 0.347209; F1: 0.395213\n",
      "(train @ 11): L: 0.718057; A: 0.626984; R: 0.626984; P: 0.688074; F1: 0.589358\n",
      "(valid @ 11): L: 0.594843; A: 0.726190; R: 0.726190; P: 0.820351; F1: 0.709073\n",
      "(train @ 12): L: 0.649557; A: 0.682540; R: 0.682540; P: 0.753530; F1: 0.656522\n",
      "(valid @ 12): L: 0.598335; A: 0.690476; R: 0.690476; P: 0.751145; F1: 0.662527\n",
      "(train @ 13): L: 0.607122; A: 0.662698; R: 0.662698; P: 0.681173; F1: 0.651754\n",
      "(valid @ 13): L: 0.502042; A: 0.803571; R: 0.803571; P: 0.811573; F1: 0.804132\n",
      "(train @ 14): L: 0.570726; A: 0.678571; R: 0.678571; P: 0.752468; F1: 0.642044\n",
      "(valid @ 14): L: 0.486208; A: 0.779762; R: 0.779762; P: 0.793506; F1: 0.776863\n",
      "(train @ 15): L: 0.575701; A: 0.662698; R: 0.662698; P: 0.679275; F1: 0.650290\n",
      "(valid @ 15): L: 0.536401; A: 0.690476; R: 0.690476; P: 0.775549; F1: 0.654776\n",
      "(train @ 16): L: 0.585501; A: 0.626984; R: 0.626984; P: 0.636883; F1: 0.612327\n",
      "(valid @ 16): L: 0.442597; A: 0.839286; R: 0.839286; P: 0.849160; F1: 0.838842\n",
      "(train @ 17): L: 0.585190; A: 0.654762; R: 0.654762; P: 0.661237; F1: 0.648425\n",
      "(valid @ 17): L: 0.559724; A: 0.625000; R: 0.625000; P: 0.800000; F1: 0.536232\n",
      "(train @ 18): L: 0.558109; A: 0.646825; R: 0.646825; P: 0.648327; F1: 0.643313\n",
      "(valid @ 18): L: 0.433016; A: 0.809524; R: 0.809524; P: 0.839286; F1: 0.804082\n",
      "(train @ 19): L: 0.480644; A: 0.769841; R: 0.769841; P: 0.770936; F1: 0.769525\n",
      "(valid @ 19): L: 0.456668; A: 0.785714; R: 0.785714; P: 0.838462; F1: 0.774583\n",
      "(train @ 20): L: 0.437866; A: 0.845238; R: 0.845238; P: 0.845450; F1: 0.845208\n",
      "(valid @ 20): L: 0.392523; A: 0.875000; R: 0.875000; P: 0.885230; F1: 0.873973\n",
      "(train @ 21): L: 0.434620; A: 0.809524; R: 0.809524; P: 0.810261; F1: 0.809377\n",
      "(valid @ 21): L: 0.369997; A: 0.827381; R: 0.827381; P: 0.828621; F1: 0.827173\n",
      "(train @ 22): L: 0.408813; A: 0.829365; R: 0.829365; P: 0.829564; F1: 0.829332\n",
      "(valid @ 22): L: 0.359769; A: 0.839286; R: 0.839286; P: 0.859335; F1: 0.836438\n",
      "(train @ 23): L: 0.384542; A: 0.801587; R: 0.801587; P: 0.823738; F1: 0.797135\n",
      "(valid @ 23): L: 0.455777; A: 0.773810; R: 0.773810; P: 0.841270; F1: 0.758730\n",
      "(train @ 24): L: 0.438997; A: 0.769841; R: 0.769841; P: 0.774294; F1: 0.768571\n",
      "(valid @ 24): L: 0.328731; A: 0.869048; R: 0.869048; P: 0.880742; F1: 0.867798\n",
      "(train @ 25): L: 0.347242; A: 0.849206; R: 0.849206; P: 0.852679; F1: 0.848739\n",
      "(valid @ 25): L: 0.315586; A: 0.863095; R: 0.863095; P: 0.865878; F1: 0.862771\n",
      "(train @ 26): L: 0.336759; A: 0.857143; R: 0.857143; P: 0.859614; F1: 0.856836\n",
      "(valid @ 26): L: 0.299593; A: 0.863095; R: 0.863095; P: 0.864509; F1: 0.862930\n",
      "(train @ 27): L: 0.310584; A: 0.884921; R: 0.884921; P: 0.885163; F1: 0.884898\n",
      "(valid @ 27): L: 0.299606; A: 0.869048; R: 0.869048; P: 0.872768; F1: 0.868642\n",
      "(train @ 28): L: 0.302121; A: 0.873016; R: 0.873016; P: 0.876786; F1: 0.872623\n",
      "(valid @ 28): L: 0.317149; A: 0.833333; R: 0.833333; P: 0.850794; F1: 0.830688\n",
      "(train @ 29): L: 0.309697; A: 0.869048; R: 0.869048; P: 0.869073; F1: 0.869045\n",
      "(valid @ 29): L: 0.281123; A: 0.880952; R: 0.880952; P: 0.887041; F1: 0.880375\n",
      "(train @ 30): L: 0.293443; A: 0.869048; R: 0.869048; P: 0.870303; F1: 0.868910\n",
      "(valid @ 30): L: 0.290485; A: 0.857143; R: 0.857143; P: 0.868370; F1: 0.855780\n",
      "Best val Metric 0.880375 @ 29\n",
      "\n",
      "models are saved @ ./predictions/211209093359/flvl_3_4_6_pretrained_model_c1.pt\n",
      "Saving predictions @ ./predictions/211209093359/flvl_train_3_4_6_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209093359/flvl_valid_2_5_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209093359/flvl_public_test_trained_on_3_4_6_r21d_rgb.csv\n",
      "Average of Best Metrics on Each Valid Set: 0.799882, 211209093359\n",
      "Saved test results @ ./predictions/211209093359/flvl_public_test_agg_r21d_rgb.csv\n"
     ]
    }
   ],
   "source": [
    "# View 0,1,2\n",
    "from main_3Views_NoOpaque import Config, run_kfold\n",
    "\n",
    "cfg = Config()\n",
    "cfg.assign_variable('task', 'flvl')\n",
    "cfg.assign_variable('output_dim', 3)\n",
    "cfg.assign_variable('model_type', 'GRU')\n",
    "cfg.assign_variable('bi_dir', False)\n",
    "cfg.assign_variable('device', 'cuda:0')\n",
    "cfg.assign_variable('data_root', './r21d_rgb_features')\n",
    "cfg.assign_variable('drop_p', 0.0) # results will be irreproducible\n",
    "cfg.assign_variable('batch_size', 64)\n",
    "cfg.assign_variable('input_dim', 512)\n",
    "cfg.assign_variable('hidden_dim', 512)\n",
    "cfg.assign_variable('n_layers', 3)\n",
    "cfg.assign_variable('num_epochs', 30)\n",
    "cfg.assign_variable('seed', 1337)\n",
    "\n",
    "run_kfold(cfg,[0,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': [2, 3, 5], 'valid': [4, 6], 'public_test': [10, 11, 12], 'private_test': [13, 14, 15]}\n",
      "(train @ 1): L: 1.488426; A: 0.420635; R: 0.420635; P: 0.350473; F1: 0.359216\n",
      "(valid @ 1): L: 0.950730; A: 0.559524; R: 0.559524; P: 0.556277; F1: 0.503268\n",
      "(train @ 2): L: 1.011485; A: 0.559524; R: 0.559524; P: 0.712811; F1: 0.509509\n",
      "(valid @ 2): L: 0.917145; A: 0.577381; R: 0.577381; P: 0.628937; F1: 0.575910\n",
      "(train @ 3): L: 0.865303; A: 0.535714; R: 0.535714; P: 0.597757; F1: 0.463406\n",
      "(valid @ 3): L: 0.921588; A: 0.547619; R: 0.547619; P: 0.575510; F1: 0.480647\n",
      "(train @ 4): L: 0.874544; A: 0.579365; R: 0.579365; P: 0.672891; F1: 0.547892\n",
      "(valid @ 4): L: 0.969894; A: 0.559524; R: 0.559524; P: 0.648587; F1: 0.567008\n",
      "(train @ 5): L: 0.735771; A: 0.646825; R: 0.646825; P: 0.675276; F1: 0.637043\n",
      "(valid @ 5): L: 0.860113; A: 0.607143; R: 0.607143; P: 0.692036; F1: 0.587860\n",
      "(train @ 6): L: 0.677389; A: 0.662698; R: 0.662698; P: 0.741200; F1: 0.629823\n",
      "(valid @ 6): L: 1.162856; A: 0.535714; R: 0.535714; P: 0.689252; F1: 0.541720\n",
      "(train @ 7): L: 0.701824; A: 0.686508; R: 0.686508; P: 0.741877; F1: 0.673482\n",
      "(valid @ 7): L: 0.833159; A: 0.583333; R: 0.583333; P: 0.619512; F1: 0.585953\n",
      "(train @ 8): L: 0.640286; A: 0.654762; R: 0.654762; P: 0.687958; F1: 0.630602\n",
      "(valid @ 8): L: 0.913958; A: 0.595238; R: 0.595238; P: 0.670591; F1: 0.585420\n",
      "(train @ 9): L: 0.583531; A: 0.726190; R: 0.726190; P: 0.797390; F1: 0.707408\n",
      "(valid @ 9): L: 0.910493; A: 0.595238; R: 0.595238; P: 0.640693; F1: 0.597065\n",
      "(train @ 10): L: 0.511430; A: 0.773810; R: 0.773810; P: 0.780220; F1: 0.775646\n",
      "(valid @ 10): L: 0.864120; A: 0.571429; R: 0.571429; P: 0.575420; F1: 0.565179\n",
      "(train @ 11): L: 0.582455; A: 0.718254; R: 0.718254; P: 0.721174; F1: 0.718551\n",
      "(valid @ 11): L: 1.086382; A: 0.553571; R: 0.553571; P: 0.656895; F1: 0.546969\n",
      "(train @ 12): L: 0.492422; A: 0.750000; R: 0.750000; P: 0.758454; F1: 0.752428\n",
      "(valid @ 12): L: 0.914534; A: 0.613095; R: 0.613095; P: 0.685565; F1: 0.604632\n",
      "(train @ 13): L: 0.448227; A: 0.773810; R: 0.773810; P: 0.776723; F1: 0.774660\n",
      "(valid @ 13): L: 1.371380; A: 0.500000; R: 0.500000; P: 0.660452; F1: 0.490182\n",
      "(train @ 14): L: 0.529990; A: 0.738095; R: 0.738095; P: 0.747341; F1: 0.734772\n",
      "(valid @ 14): L: 0.713734; A: 0.666667; R: 0.666667; P: 0.670064; F1: 0.662295\n",
      "(train @ 15): L: 0.415205; A: 0.876984; R: 0.876984; P: 0.880952; F1: 0.876975\n",
      "(valid @ 15): L: 0.702495; A: 0.708333; R: 0.708333; P: 0.717119; F1: 0.704379\n",
      "(train @ 16): L: 0.498116; A: 0.706349; R: 0.706349; P: 0.739560; F1: 0.691095\n",
      "(valid @ 16): L: 0.818159; A: 0.613095; R: 0.613095; P: 0.616169; F1: 0.603646\n",
      "(train @ 17): L: 0.435691; A: 0.821429; R: 0.821429; P: 0.868525; F1: 0.814060\n",
      "(valid @ 17): L: 0.885129; A: 0.708333; R: 0.708333; P: 0.743171; F1: 0.697154\n",
      "(train @ 18): L: 0.556909; A: 0.742063; R: 0.742063; P: 0.781929; F1: 0.729244\n",
      "(valid @ 18): L: 0.747251; A: 0.678571; R: 0.678571; P: 0.686807; F1: 0.680975\n",
      "(train @ 19): L: 0.456167; A: 0.777778; R: 0.777778; P: 0.826220; F1: 0.766898\n",
      "(valid @ 19): L: 0.916590; A: 0.672619; R: 0.672619; P: 0.723246; F1: 0.651476\n",
      "(train @ 20): L: 0.417611; A: 0.777778; R: 0.777778; P: 0.843704; F1: 0.763881\n",
      "(valid @ 20): L: 0.762365; A: 0.666667; R: 0.666667; P: 0.665986; F1: 0.660868\n",
      "(train @ 21): L: 0.498484; A: 0.726190; R: 0.726190; P: 0.740086; F1: 0.720432\n",
      "(valid @ 21): L: 0.806270; A: 0.678571; R: 0.678571; P: 0.736871; F1: 0.654743\n",
      "(train @ 22): L: 0.423915; A: 0.777778; R: 0.777778; P: 0.778916; F1: 0.777473\n",
      "(valid @ 22): L: 0.643742; A: 0.744048; R: 0.744048; P: 0.763664; F1: 0.737347\n",
      "(train @ 23): L: 0.423006; A: 0.738095; R: 0.738095; P: 0.754135; F1: 0.732218\n",
      "(valid @ 23): L: 0.632329; A: 0.720238; R: 0.720238; P: 0.726687; F1: 0.713431\n",
      "(train @ 24): L: 0.377079; A: 0.849206; R: 0.849206; P: 0.869223; F1: 0.846629\n",
      "(valid @ 24): L: 0.686136; A: 0.720238; R: 0.720238; P: 0.734244; F1: 0.712983\n",
      "(train @ 25): L: 0.304871; A: 0.876984; R: 0.876984; P: 0.886743; F1: 0.876025\n",
      "(valid @ 25): L: 0.640125; A: 0.714286; R: 0.714286; P: 0.718367; F1: 0.712288\n",
      "(train @ 26): L: 0.359970; A: 0.821429; R: 0.821429; P: 0.822483; F1: 0.821241\n",
      "(valid @ 26): L: 0.730896; A: 0.744048; R: 0.744048; P: 0.772294; F1: 0.734721\n",
      "(train @ 27): L: 0.274281; A: 0.892857; R: 0.892857; P: 0.896227; F1: 0.892579\n",
      "(valid @ 27): L: 0.650859; A: 0.726190; R: 0.726190; P: 0.728125; F1: 0.725343\n",
      "(train @ 28): L: 0.272406; A: 0.900794; R: 0.900794; P: 0.901048; F1: 0.900775\n",
      "(valid @ 28): L: 0.870843; A: 0.714286; R: 0.714286; P: 0.749653; F1: 0.701854\n",
      "(train @ 29): L: 0.249752; A: 0.884921; R: 0.884921; P: 0.894933; F1: 0.884023\n",
      "(valid @ 29): L: 0.688554; A: 0.714286; R: 0.714286; P: 0.715285; F1: 0.713789\n",
      "(train @ 30): L: 0.235367; A: 0.912698; R: 0.912698; P: 0.913167; F1: 0.912668\n",
      "(valid @ 30): L: 0.760620; A: 0.738095; R: 0.738095; P: 0.752535; F1: 0.730829\n",
      "Best val Metric 0.737347 @ 22\n",
      "\n",
      "models are saved @ ./predictions/211209095437/flvl_2_3_5_pretrained_model_c1.pt\n",
      "Saving predictions @ ./predictions/211209095437/flvl_train_2_3_5_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209095437/flvl_valid_4_6_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209095437/flvl_public_test_trained_on_2_3_5_r21d_rgb.csv\n",
      "{'train': [2, 4, 5], 'valid': [3, 6], 'public_test': [10, 11, 12], 'private_test': [13, 14, 15]}\n",
      "(train @ 1): L: 1.474001; A: 0.420635; R: 0.420635; P: 0.350473; F1: 0.359216\n",
      "(valid @ 1): L: 1.008532; A: 0.505952; R: 0.505952; P: 0.598880; F1: 0.408407\n",
      "(train @ 2): L: 1.125543; A: 0.523810; R: 0.523810; P: 0.623562; F1: 0.475132\n",
      "(valid @ 2): L: 0.952839; A: 0.500000; R: 0.500000; P: 0.551153; F1: 0.492447\n",
      "(train @ 3): L: 0.854784; A: 0.571429; R: 0.571429; P: 0.615665; F1: 0.524895\n",
      "(valid @ 3): L: 0.873533; A: 0.547619; R: 0.547619; P: 0.587370; F1: 0.477535\n",
      "(train @ 4): L: 0.832968; A: 0.567460; R: 0.567460; P: 0.698901; F1: 0.520812\n",
      "(valid @ 4): L: 0.882103; A: 0.583333; R: 0.583333; P: 0.650185; F1: 0.579124\n",
      "(train @ 5): L: 0.742534; A: 0.674603; R: 0.674603; P: 0.693015; F1: 0.674684\n",
      "(valid @ 5): L: 0.861609; A: 0.619048; R: 0.619048; P: 0.727891; F1: 0.593502\n",
      "(train @ 6): L: 0.674437; A: 0.682540; R: 0.682540; P: 0.722619; F1: 0.682127\n",
      "(valid @ 6): L: 1.015650; A: 0.553571; R: 0.553571; P: 0.680017; F1: 0.565026\n",
      "(train @ 7): L: 0.637313; A: 0.718254; R: 0.718254; P: 0.758944; F1: 0.711079\n",
      "(valid @ 7): L: 0.783932; A: 0.619048; R: 0.619048; P: 0.631253; F1: 0.621292\n",
      "(train @ 8): L: 0.680216; A: 0.642857; R: 0.642857; P: 0.648303; F1: 0.623439\n",
      "(valid @ 8): L: 1.264143; A: 0.482143; R: 0.482143; P: 0.662970; F1: 0.468416\n",
      "(train @ 9): L: 0.609159; A: 0.690476; R: 0.690476; P: 0.770709; F1: 0.674519\n",
      "(valid @ 9): L: 0.750851; A: 0.642857; R: 0.642857; P: 0.663071; F1: 0.642694\n",
      "(train @ 10): L: 0.645964; A: 0.654762; R: 0.654762; P: 0.668100; F1: 0.657921\n",
      "(valid @ 10): L: 1.009499; A: 0.583333; R: 0.583333; P: 0.673472; F1: 0.589126\n",
      "(train @ 11): L: 0.645531; A: 0.702381; R: 0.702381; P: 0.700624; F1: 0.700143\n",
      "(valid @ 11): L: 0.795964; A: 0.595238; R: 0.595238; P: 0.680901; F1: 0.579715\n",
      "(train @ 12): L: 0.557457; A: 0.781746; R: 0.781746; P: 0.791856; F1: 0.781898\n",
      "(valid @ 12): L: 0.722587; A: 0.654762; R: 0.654762; P: 0.722222; F1: 0.630128\n",
      "(train @ 13): L: 0.463525; A: 0.785714; R: 0.785714; P: 0.792406; F1: 0.785839\n",
      "(valid @ 13): L: 1.284744; A: 0.470238; R: 0.470238; P: 0.651065; F1: 0.456511\n",
      "(train @ 14): L: 0.540144; A: 0.734127; R: 0.734127; P: 0.733433; F1: 0.731465\n",
      "(valid @ 14): L: 0.653380; A: 0.696429; R: 0.696429; P: 0.705260; F1: 0.692889\n",
      "(train @ 15): L: 0.467013; A: 0.805556; R: 0.805556; P: 0.810296; F1: 0.807015\n",
      "(valid @ 15): L: 0.730007; A: 0.684524; R: 0.684524; P: 0.758455; F1: 0.653562\n",
      "(train @ 16): L: 0.444823; A: 0.793651; R: 0.793651; P: 0.815038; F1: 0.789020\n",
      "(valid @ 16): L: 0.737025; A: 0.654762; R: 0.654762; P: 0.663025; F1: 0.649258\n",
      "(train @ 17): L: 0.480281; A: 0.738095; R: 0.738095; P: 0.793651; F1: 0.720635\n",
      "(valid @ 17): L: 0.800046; A: 0.672619; R: 0.672619; P: 0.760773; F1: 0.632913\n",
      "(train @ 18): L: 0.583295; A: 0.734127; R: 0.734127; P: 0.835918; F1: 0.705823\n",
      "(valid @ 18): L: 0.601668; A: 0.702381; R: 0.702381; P: 0.701978; F1: 0.700943\n",
      "(train @ 19): L: 0.523508; A: 0.706349; R: 0.706349; P: 0.787755; F1: 0.676009\n",
      "(valid @ 19): L: 0.922404; A: 0.642857; R: 0.642857; P: 0.733420; F1: 0.612622\n",
      "(train @ 20): L: 0.454361; A: 0.769841; R: 0.769841; P: 0.814122; F1: 0.759358\n",
      "(valid @ 20): L: 0.732286; A: 0.708333; R: 0.708333; P: 0.781660; F1: 0.681959\n",
      "(train @ 21): L: 0.421572; A: 0.797619; R: 0.797619; P: 0.802787; F1: 0.797339\n",
      "(valid @ 21): L: 0.595727; A: 0.726190; R: 0.726190; P: 0.767637; F1: 0.711326\n",
      "(train @ 22): L: 0.425550; A: 0.793651; R: 0.793651; P: 0.807322; F1: 0.790617\n",
      "(valid @ 22): L: 0.652819; A: 0.720238; R: 0.720238; P: 0.795338; F1: 0.696976\n",
      "(train @ 23): L: 0.471894; A: 0.730159; R: 0.730159; P: 0.801020; F1: 0.707597\n",
      "(valid @ 23): L: 0.554443; A: 0.738095; R: 0.738095; P: 0.740331; F1: 0.734430\n",
      "(train @ 24): L: 0.418504; A: 0.809524; R: 0.809524; P: 0.856908; F1: 0.801278\n",
      "(valid @ 24): L: 0.536139; A: 0.755952; R: 0.755952; P: 0.773109; F1: 0.750649\n",
      "(train @ 25): L: 0.353176; A: 0.845238; R: 0.845238; P: 0.873435; F1: 0.841539\n",
      "(valid @ 25): L: 0.535151; A: 0.750000; R: 0.750000; P: 0.764931; F1: 0.745083\n",
      "(train @ 26): L: 0.305190; A: 0.873016; R: 0.873016; P: 0.878170; F1: 0.872480\n",
      "(valid @ 26): L: 0.554433; A: 0.755952; R: 0.755952; P: 0.776922; F1: 0.749563\n",
      "(train @ 27): L: 0.272327; A: 0.904762; R: 0.904762; P: 0.904876; F1: 0.904754\n",
      "(valid @ 27): L: 0.577125; A: 0.755952; R: 0.755952; P: 0.780820; F1: 0.748611\n",
      "(train @ 28): L: 0.267006; A: 0.920635; R: 0.920635; P: 0.920755; F1: 0.920628\n",
      "(valid @ 28): L: 0.584965; A: 0.750000; R: 0.750000; P: 0.776472; F1: 0.741864\n",
      "(train @ 29): L: 0.241239; A: 0.900794; R: 0.900794; P: 0.905636; F1: 0.900433\n",
      "(valid @ 29): L: 0.509388; A: 0.767857; R: 0.767857; P: 0.767895; F1: 0.767846\n",
      "(train @ 30): L: 0.286431; A: 0.865079; R: 0.865079; P: 0.868750; F1: 0.864662\n",
      "(valid @ 30): L: 0.631900; A: 0.738095; R: 0.738095; P: 0.795615; F1: 0.721680\n",
      "Best val Metric 0.767846 @ 29\n",
      "\n",
      "models are saved @ ./predictions/211209095437/flvl_2_4_5_pretrained_model_c1.pt\n",
      "Saving predictions @ ./predictions/211209095437/flvl_train_2_4_5_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209095437/flvl_valid_3_6_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209095437/flvl_public_test_trained_on_2_4_5_r21d_rgb.csv\n",
      "{'train': [3, 4, 6], 'valid': [2, 5], 'public_test': [10, 11, 12], 'private_test': [13, 14, 15]}\n",
      "(train @ 1): L: 1.528981; A: 0.400794; R: 0.400794; P: 0.330608; F1: 0.342911\n",
      "(valid @ 1): L: 1.064390; A: 0.511905; R: 0.511905; P: 0.628942; F1: 0.412607\n",
      "(train @ 2): L: 1.225900; A: 0.412698; R: 0.412698; P: 0.563978; F1: 0.336163\n",
      "(valid @ 2): L: 0.993406; A: 0.434524; R: 0.434524; P: 0.327630; F1: 0.269647\n",
      "(train @ 3): L: 1.023442; A: 0.428571; R: 0.428571; P: 0.183673; F1: 0.257143\n",
      "(valid @ 3): L: 0.920976; A: 0.619048; R: 0.619048; P: 0.585714; F1: 0.571429\n",
      "(train @ 4): L: 0.990469; A: 0.484127; R: 0.484127; P: 0.544438; F1: 0.382418\n",
      "(valid @ 4): L: 0.813443; A: 0.571429; R: 0.571429; P: 0.482001; F1: 0.503579\n",
      "(train @ 5): L: 0.879847; A: 0.571429; R: 0.571429; P: 0.586834; F1: 0.562549\n",
      "(valid @ 5): L: 0.828165; A: 0.446429; R: 0.446429; P: 0.286337; F1: 0.315635\n",
      "(train @ 6): L: 0.850694; A: 0.476190; R: 0.476190; P: 0.403332; F1: 0.435614\n",
      "(valid @ 6): L: 0.766940; A: 0.630952; R: 0.630952; P: 0.557143; F1: 0.586331\n",
      "(train @ 7): L: 0.829806; A: 0.611111; R: 0.611111; P: 0.727108; F1: 0.584568\n",
      "(valid @ 7): L: 0.700325; A: 0.678571; R: 0.678571; P: 0.706982; F1: 0.667005\n",
      "(train @ 8): L: 0.843204; A: 0.615079; R: 0.615079; P: 0.643415; F1: 0.587778\n",
      "(valid @ 8): L: 0.710463; A: 0.630952; R: 0.630952; P: 0.684590; F1: 0.602042\n",
      "(train @ 9): L: 0.752664; A: 0.686508; R: 0.686508; P: 0.750849; F1: 0.680474\n",
      "(valid @ 9): L: 0.687397; A: 0.648810; R: 0.648810; P: 0.792531; F1: 0.624429\n",
      "(train @ 10): L: 0.719196; A: 0.595238; R: 0.595238; P: 0.616553; F1: 0.582530\n",
      "(valid @ 10): L: 0.791514; A: 0.511905; R: 0.511905; P: 0.343228; F1: 0.378335\n",
      "(train @ 11): L: 0.737519; A: 0.611111; R: 0.611111; P: 0.702635; F1: 0.550567\n",
      "(valid @ 11): L: 0.621433; A: 0.696429; R: 0.696429; P: 0.807598; F1: 0.672996\n",
      "(train @ 12): L: 0.666867; A: 0.650794; R: 0.650794; P: 0.749375; F1: 0.603608\n",
      "(valid @ 12): L: 0.607549; A: 0.684524; R: 0.684524; P: 0.718351; F1: 0.668092\n",
      "(train @ 13): L: 0.627349; A: 0.623016; R: 0.623016; P: 0.634658; F1: 0.610941\n",
      "(valid @ 13): L: 0.535611; A: 0.779762; R: 0.779762; P: 0.789680; F1: 0.778402\n",
      "(train @ 14): L: 0.590103; A: 0.674603; R: 0.674603; P: 0.738000; F1: 0.643472\n",
      "(valid @ 14): L: 0.493201; A: 0.827381; R: 0.827381; P: 0.829779; F1: 0.828159\n",
      "(train @ 15): L: 0.605524; A: 0.638889; R: 0.638889; P: 0.640894; F1: 0.635091\n",
      "(valid @ 15): L: 0.608001; A: 0.589286; R: 0.589286; P: 0.650678; F1: 0.491321\n",
      "(train @ 16): L: 0.602921; A: 0.615079; R: 0.615079; P: 0.621980; F1: 0.607432\n",
      "(valid @ 16): L: 0.476286; A: 0.785714; R: 0.785714; P: 0.819789; F1: 0.781604\n",
      "(train @ 17): L: 0.595441; A: 0.615079; R: 0.615079; P: 0.615083; F1: 0.615071\n",
      "(valid @ 17): L: 0.598528; A: 0.595238; R: 0.595238; P: 0.791837; F1: 0.479075\n",
      "(train @ 18): L: 0.555398; A: 0.698413; R: 0.698413; P: 0.698806; F1: 0.698180\n",
      "(valid @ 18): L: 0.454988; A: 0.809524; R: 0.809524; P: 0.845228; F1: 0.803105\n",
      "(train @ 19): L: 0.494274; A: 0.742063; R: 0.742063; P: 0.767651; F1: 0.733371\n",
      "(valid @ 19): L: 0.470477; A: 0.761905; R: 0.761905; P: 0.798701; F1: 0.752410\n",
      "(train @ 20): L: 0.467286; A: 0.825397; R: 0.825397; P: 0.834416; F1: 0.823887\n",
      "(valid @ 20): L: 0.411564; A: 0.857143; R: 0.857143; P: 0.857143; F1: 0.857143\n",
      "(train @ 21): L: 0.454775; A: 0.801587; R: 0.801587; P: 0.806752; F1: 0.800493\n",
      "(valid @ 21): L: 0.386572; A: 0.863095; R: 0.863095; P: 0.863602; F1: 0.863036\n",
      "(train @ 22): L: 0.471190; A: 0.765873; R: 0.765873; P: 0.767911; F1: 0.765264\n",
      "(valid @ 22): L: 0.381592; A: 0.851190; R: 0.851190; P: 0.872131; F1: 0.848554\n",
      "(train @ 23): L: 0.444578; A: 0.742063; R: 0.742063; P: 0.796193; F1: 0.725539\n",
      "(valid @ 23): L: 0.473591; A: 0.726190; R: 0.726190; P: 0.807453; F1: 0.700551\n",
      "(train @ 24): L: 0.469475; A: 0.746032; R: 0.746032; P: 0.770702; F1: 0.737920\n",
      "(valid @ 24): L: 0.378447; A: 0.815476; R: 0.815476; P: 0.848933; F1: 0.809742\n",
      "(train @ 25): L: 0.400091; A: 0.793651; R: 0.793651; P: 0.800000; F1: 0.792208\n",
      "(valid @ 25): L: 0.404626; A: 0.821429; R: 0.821429; P: 0.865974; F1: 0.814412\n",
      "(train @ 26): L: 0.383377; A: 0.845238; R: 0.845238; P: 0.855997; F1: 0.843761\n",
      "(valid @ 26): L: 0.361614; A: 0.815476; R: 0.815476; P: 0.848933; F1: 0.809742\n",
      "(train @ 27): L: 0.358983; A: 0.841270; R: 0.841270; P: 0.841641; F1: 0.841215\n",
      "(valid @ 27): L: 0.350292; A: 0.851190; R: 0.851190; P: 0.882983; F1: 0.847295\n",
      "(train @ 28): L: 0.347562; A: 0.837302; R: 0.837302; P: 0.837507; F1: 0.837270\n",
      "(valid @ 28): L: 0.323685; A: 0.851190; R: 0.851190; P: 0.867708; F1: 0.849087\n",
      "(train @ 29): L: 0.351664; A: 0.849206; R: 0.849206; P: 0.851608; F1: 0.848882\n",
      "(valid @ 29): L: 0.319515; A: 0.833333; R: 0.833333; P: 0.855232; F1: 0.830055\n",
      "(train @ 30): L: 0.327077; A: 0.861111; R: 0.861111; P: 0.861335; F1: 0.861084\n",
      "(valid @ 30): L: 0.279199; A: 0.910714; R: 0.910714; P: 0.913952; F1: 0.910503\n",
      "Best val Metric 0.910503 @ 30\n",
      "\n",
      "models are saved @ ./predictions/211209095437/flvl_3_4_6_pretrained_model_c1.pt\n",
      "Saving predictions @ ./predictions/211209095437/flvl_train_3_4_6_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209095437/flvl_valid_2_5_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209095437/flvl_public_test_trained_on_3_4_6_r21d_rgb.csv\n",
      "Average of Best Metrics on Each Valid Set: 0.805232, 211209095437\n",
      "Saved test results @ ./predictions/211209095437/flvl_public_test_agg_r21d_rgb.csv\n"
     ]
    }
   ],
   "source": [
    "# View 0,1,3\n",
    "from main_3Views_NoOpaque import Config, run_kfold\n",
    "\n",
    "cfg = Config()\n",
    "cfg.assign_variable('task', 'flvl')\n",
    "cfg.assign_variable('output_dim', 3)\n",
    "cfg.assign_variable('model_type', 'GRU')\n",
    "cfg.assign_variable('bi_dir', False)\n",
    "cfg.assign_variable('device', 'cuda:0')\n",
    "cfg.assign_variable('data_root', './r21d_rgb_features')\n",
    "cfg.assign_variable('drop_p', 0.0) # results will be irreproducible\n",
    "cfg.assign_variable('batch_size', 64)\n",
    "cfg.assign_variable('input_dim', 512)\n",
    "cfg.assign_variable('hidden_dim', 512)\n",
    "cfg.assign_variable('n_layers', 3)\n",
    "cfg.assign_variable('num_epochs', 30)\n",
    "cfg.assign_variable('seed', 1337)\n",
    "\n",
    "run_kfold(cfg,[0,1,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': [2, 3, 5], 'valid': [4, 6], 'public_test': [10, 11, 12], 'private_test': [13, 14, 15]}\n",
      "(train @ 1): L: 1.494764; A: 0.416667; R: 0.416667; P: 0.345365; F1: 0.354157\n",
      "(valid @ 1): L: 0.923278; A: 0.577381; R: 0.577381; P: 0.539702; F1: 0.531361\n",
      "(train @ 2): L: 1.003550; A: 0.575397; R: 0.575397; P: 0.726969; F1: 0.531966\n",
      "(valid @ 2): L: 0.895670; A: 0.589286; R: 0.589286; P: 0.676019; F1: 0.563145\n",
      "(train @ 3): L: 0.863972; A: 0.539683; R: 0.539683; P: 0.452922; F1: 0.472793\n",
      "(valid @ 3): L: 0.920087; A: 0.553571; R: 0.553571; P: 0.579119; F1: 0.488400\n",
      "(train @ 4): L: 0.870139; A: 0.587302; R: 0.587302; P: 0.680605; F1: 0.559071\n",
      "(valid @ 4): L: 0.969503; A: 0.559524; R: 0.559524; P: 0.649211; F1: 0.569803\n",
      "(train @ 5): L: 0.731843; A: 0.650794; R: 0.650794; P: 0.680130; F1: 0.639943\n",
      "(valid @ 5): L: 0.873885; A: 0.583333; R: 0.583333; P: 0.686801; F1: 0.548537\n",
      "(train @ 6): L: 0.687819; A: 0.646825; R: 0.646825; P: 0.720845; F1: 0.615052\n",
      "(valid @ 6): L: 1.143813; A: 0.529762; R: 0.529762; P: 0.674747; F1: 0.535097\n",
      "(train @ 7): L: 0.718008; A: 0.674603; R: 0.674603; P: 0.727729; F1: 0.661165\n",
      "(valid @ 7): L: 0.832701; A: 0.589286; R: 0.589286; P: 0.626611; F1: 0.591919\n",
      "(train @ 8): L: 0.646048; A: 0.619048; R: 0.619048; P: 0.654094; F1: 0.584601\n",
      "(valid @ 8): L: 0.888054; A: 0.595238; R: 0.595238; P: 0.668075; F1: 0.584490\n",
      "(train @ 9): L: 0.589049; A: 0.718254; R: 0.718254; P: 0.797095; F1: 0.699640\n",
      "(valid @ 9): L: 0.943738; A: 0.583333; R: 0.583333; P: 0.638959; F1: 0.586692\n",
      "(train @ 10): L: 0.496918; A: 0.805556; R: 0.805556; P: 0.810747; F1: 0.807051\n",
      "(valid @ 10): L: 0.864740; A: 0.559524; R: 0.559524; P: 0.560740; F1: 0.553487\n",
      "(train @ 11): L: 0.593185; A: 0.698413; R: 0.698413; P: 0.702326; F1: 0.699452\n",
      "(valid @ 11): L: 1.128006; A: 0.553571; R: 0.553571; P: 0.654865; F1: 0.543983\n",
      "(train @ 12): L: 0.484465; A: 0.765873; R: 0.765873; P: 0.772880; F1: 0.767976\n",
      "(valid @ 12): L: 0.922632; A: 0.636905; R: 0.636905; P: 0.707046; F1: 0.634938\n",
      "(train @ 13): L: 0.443593; A: 0.785714; R: 0.785714; P: 0.789768; F1: 0.786884\n",
      "(valid @ 13): L: 1.344016; A: 0.511905; R: 0.511905; P: 0.664204; F1: 0.499207\n",
      "(train @ 14): L: 0.516575; A: 0.750000; R: 0.750000; P: 0.762283; F1: 0.746458\n",
      "(valid @ 14): L: 0.747756; A: 0.654762; R: 0.654762; P: 0.661323; F1: 0.650920\n",
      "(train @ 15): L: 0.400289; A: 0.896825; R: 0.896825; P: 0.899375; F1: 0.896918\n",
      "(valid @ 15): L: 0.714901; A: 0.720238; R: 0.720238; P: 0.731502; F1: 0.714381\n",
      "(train @ 16): L: 0.466971; A: 0.761905; R: 0.761905; P: 0.799812; F1: 0.751597\n",
      "(valid @ 16): L: 0.860315; A: 0.577381; R: 0.577381; P: 0.595815; F1: 0.561662\n",
      "(train @ 17): L: 0.415223; A: 0.817460; R: 0.817460; P: 0.861299; F1: 0.810288\n",
      "(valid @ 17): L: 0.884431; A: 0.708333; R: 0.708333; P: 0.743032; F1: 0.697266\n",
      "(train @ 18): L: 0.530401; A: 0.765873; R: 0.765873; P: 0.817464; F1: 0.755953\n",
      "(valid @ 18): L: 0.768165; A: 0.654762; R: 0.654762; P: 0.661667; F1: 0.657507\n",
      "(train @ 19): L: 0.439338; A: 0.781746; R: 0.781746; P: 0.826840; F1: 0.771812\n",
      "(valid @ 19): L: 1.118011; A: 0.613095; R: 0.613095; P: 0.695238; F1: 0.594149\n",
      "(train @ 20): L: 0.390206; A: 0.797619; R: 0.797619; P: 0.853512; F1: 0.787385\n",
      "(valid @ 20): L: 0.736588; A: 0.684524; R: 0.684524; P: 0.679870; F1: 0.680504\n",
      "(train @ 21): L: 0.545735; A: 0.714286; R: 0.714286; P: 0.728493; F1: 0.709052\n",
      "(valid @ 21): L: 0.895855; A: 0.666667; R: 0.666667; P: 0.740685; F1: 0.630291\n",
      "(train @ 22): L: 0.441222; A: 0.757937; R: 0.757937; P: 0.760679; F1: 0.757057\n",
      "(valid @ 22): L: 0.629417; A: 0.714286; R: 0.714286; P: 0.712844; F1: 0.709687\n",
      "(train @ 23): L: 0.402947; A: 0.769841; R: 0.769841; P: 0.773232; F1: 0.768870\n",
      "(valid @ 23): L: 0.654730; A: 0.702381; R: 0.702381; P: 0.719007; F1: 0.692416\n",
      "(train @ 24): L: 0.345250; A: 0.861111; R: 0.861111; P: 0.872494; F1: 0.859786\n",
      "(valid @ 24): L: 0.651204; A: 0.714286; R: 0.714286; P: 0.717759; F1: 0.707847\n",
      "(train @ 25): L: 0.277502; A: 0.896825; R: 0.896825; P: 0.904127; F1: 0.896256\n",
      "(valid @ 25): L: 0.629467; A: 0.720238; R: 0.720238; P: 0.723115; F1: 0.717780\n",
      "(train @ 26): L: 0.341588; A: 0.829365; R: 0.829365; P: 0.831169; F1: 0.829068\n",
      "(valid @ 26): L: 0.701956; A: 0.744048; R: 0.744048; P: 0.774589; F1: 0.734143\n",
      "(train @ 27): L: 0.255080; A: 0.900794; R: 0.900794; P: 0.905636; F1: 0.900433\n",
      "(valid @ 27): L: 0.626411; A: 0.744048; R: 0.744048; P: 0.745695; F1: 0.743441\n",
      "(train @ 28): L: 0.253296; A: 0.912698; R: 0.912698; P: 0.912815; F1: 0.912691\n",
      "(valid @ 28): L: 0.808621; A: 0.732143; R: 0.732143; P: 0.765306; F1: 0.720192\n",
      "(train @ 29): L: 0.222713; A: 0.904762; R: 0.904762; P: 0.910459; F1: 0.904360\n",
      "(valid @ 29): L: 0.638702; A: 0.738095; R: 0.738095; P: 0.739261; F1: 0.737640\n",
      "(train @ 30): L: 0.201422; A: 0.936508; R: 0.936508; P: 0.936508; F1: 0.936508\n",
      "(valid @ 30): L: 0.692912; A: 0.750000; R: 0.750000; P: 0.764931; F1: 0.745083\n",
      "Best val Metric 0.745083 @ 30\n",
      "\n",
      "models are saved @ ./predictions/211209101450/flvl_2_3_5_pretrained_model_c1.pt\n",
      "Saving predictions @ ./predictions/211209101450/flvl_train_2_3_5_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209101450/flvl_valid_4_6_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209101450/flvl_public_test_trained_on_2_3_5_r21d_rgb.csv\n",
      "{'train': [2, 4, 5], 'valid': [3, 6], 'public_test': [10, 11, 12], 'private_test': [13, 14, 15]}\n",
      "(train @ 1): L: 1.462505; A: 0.412698; R: 0.412698; P: 0.341588; F1: 0.351356\n",
      "(valid @ 1): L: 0.976578; A: 0.500000; R: 0.500000; P: 0.572368; F1: 0.404221\n",
      "(train @ 2): L: 1.118557; A: 0.547619; R: 0.547619; P: 0.691298; F1: 0.499050\n",
      "(valid @ 2): L: 0.928353; A: 0.547619; R: 0.547619; P: 0.547619; F1: 0.546239\n",
      "(train @ 3): L: 0.854724; A: 0.515873; R: 0.515873; P: 0.426304; F1: 0.436850\n",
      "(valid @ 3): L: 0.873634; A: 0.541667; R: 0.541667; P: 0.583936; F1: 0.469487\n",
      "(train @ 4): L: 0.834324; A: 0.587302; R: 0.587302; P: 0.718704; F1: 0.547651\n",
      "(valid @ 4): L: 0.892318; A: 0.559524; R: 0.559524; P: 0.646840; F1: 0.566012\n",
      "(train @ 5): L: 0.737506; A: 0.666667; R: 0.666667; P: 0.693012; F1: 0.662701\n",
      "(valid @ 5): L: 0.863276; A: 0.619048; R: 0.619048; P: 0.751282; F1: 0.590375\n",
      "(train @ 6): L: 0.672775; A: 0.662698; R: 0.662698; P: 0.706198; F1: 0.660258\n",
      "(valid @ 6): L: 1.050213; A: 0.541667; R: 0.541667; P: 0.688448; F1: 0.557243\n",
      "(train @ 7): L: 0.637522; A: 0.714286; R: 0.714286; P: 0.753874; F1: 0.708523\n",
      "(valid @ 7): L: 0.785013; A: 0.619048; R: 0.619048; P: 0.653934; F1: 0.621769\n",
      "(train @ 8): L: 0.679031; A: 0.642857; R: 0.642857; P: 0.648922; F1: 0.624629\n",
      "(valid @ 8): L: 1.302699; A: 0.470238; R: 0.470238; P: 0.656791; F1: 0.455125\n",
      "(train @ 9): L: 0.607080; A: 0.698413; R: 0.698413; P: 0.766614; F1: 0.685060\n",
      "(valid @ 9): L: 0.749898; A: 0.642857; R: 0.642857; P: 0.663071; F1: 0.642694\n",
      "(train @ 10): L: 0.639329; A: 0.662698; R: 0.662698; P: 0.677269; F1: 0.666549\n",
      "(valid @ 10): L: 1.039098; A: 0.571429; R: 0.571429; P: 0.685878; F1: 0.582184\n",
      "(train @ 11): L: 0.641663; A: 0.702381; R: 0.702381; P: 0.700256; F1: 0.700175\n",
      "(valid @ 11): L: 0.792888; A: 0.607143; R: 0.607143; P: 0.688684; F1: 0.589050\n",
      "(train @ 12): L: 0.564975; A: 0.757937; R: 0.757937; P: 0.770352; F1: 0.757400\n",
      "(valid @ 12): L: 0.707494; A: 0.666667; R: 0.666667; P: 0.732143; F1: 0.646154\n",
      "(train @ 13): L: 0.452076; A: 0.801587; R: 0.801587; P: 0.808059; F1: 0.802363\n",
      "(valid @ 13): L: 1.383681; A: 0.446429; R: 0.446429; P: 0.633458; F1: 0.424497\n",
      "(train @ 14): L: 0.535263; A: 0.734127; R: 0.734127; P: 0.737035; F1: 0.730897\n",
      "(valid @ 14): L: 0.640979; A: 0.726190; R: 0.726190; P: 0.740109; F1: 0.724081\n",
      "(train @ 15): L: 0.458050; A: 0.809524; R: 0.809524; P: 0.815321; F1: 0.810582\n",
      "(valid @ 15): L: 0.727041; A: 0.696429; R: 0.696429; P: 0.783180; F1: 0.665151\n",
      "(train @ 16): L: 0.421008; A: 0.805556; R: 0.805556; P: 0.821933; F1: 0.802201\n",
      "(valid @ 16): L: 0.768294; A: 0.654762; R: 0.654762; P: 0.679400; F1: 0.652327\n",
      "(train @ 17): L: 0.450647; A: 0.773810; R: 0.773810; P: 0.819081; F1: 0.761892\n",
      "(valid @ 17): L: 0.849270; A: 0.666667; R: 0.666667; P: 0.768904; F1: 0.624136\n",
      "(train @ 18): L: 0.580050; A: 0.753968; R: 0.753968; P: 0.843697; F1: 0.733744\n",
      "(valid @ 18): L: 0.585752; A: 0.690476; R: 0.690476; P: 0.690407; F1: 0.690335\n",
      "(train @ 19): L: 0.515766; A: 0.730159; R: 0.730159; P: 0.801986; F1: 0.707513\n",
      "(valid @ 19): L: 1.077286; A: 0.595238; R: 0.595238; P: 0.704145; F1: 0.559400\n",
      "(train @ 20): L: 0.461038; A: 0.753968; R: 0.753968; P: 0.795668; F1: 0.743273\n",
      "(valid @ 20): L: 0.678825; A: 0.714286; R: 0.714286; P: 0.786804; F1: 0.690485\n",
      "(train @ 21): L: 0.414766; A: 0.813492; R: 0.813492; P: 0.819982; F1: 0.812934\n",
      "(valid @ 21): L: 0.589762; A: 0.750000; R: 0.750000; P: 0.818946; F1: 0.731288\n",
      "(train @ 22): L: 0.396337; A: 0.801587; R: 0.801587; P: 0.805521; F1: 0.800750\n",
      "(valid @ 22): L: 0.608051; A: 0.714286; R: 0.714286; P: 0.792072; F1: 0.689207\n",
      "(train @ 23): L: 0.415019; A: 0.793651; R: 0.793651; P: 0.842899; F1: 0.783848\n",
      "(valid @ 23): L: 0.532906; A: 0.738095; R: 0.738095; P: 0.742433; F1: 0.733904\n",
      "(train @ 24): L: 0.372086; A: 0.845238; R: 0.845238; P: 0.873435; F1: 0.841539\n",
      "(valid @ 24): L: 0.536856; A: 0.738095; R: 0.738095; P: 0.773501; F1: 0.726756\n",
      "(train @ 25): L: 0.317018; A: 0.869048; R: 0.869048; P: 0.892174; F1: 0.866644\n",
      "(valid @ 25): L: 0.485491; A: 0.755952; R: 0.755952; P: 0.762171; F1: 0.753947\n",
      "(train @ 26): L: 0.282457; A: 0.888889; R: 0.888889; P: 0.892857; F1: 0.888545\n",
      "(valid @ 26): L: 0.543728; A: 0.767857; R: 0.767857; P: 0.805895; F1: 0.758044\n",
      "(train @ 27): L: 0.240662; A: 0.908730; R: 0.908730; P: 0.908759; F1: 0.908728\n",
      "(valid @ 27): L: 0.491909; A: 0.767857; R: 0.767857; P: 0.780089; F1: 0.764017\n",
      "(train @ 28): L: 0.219598; A: 0.944444; R: 0.944444; P: 0.944444; F1: 0.944444\n",
      "(valid @ 28): L: 0.554469; A: 0.767857; R: 0.767857; P: 0.806287; F1: 0.758653\n",
      "(train @ 29): L: 0.204636; A: 0.912698; R: 0.912698; P: 0.915650; F1: 0.912511\n",
      "(valid @ 29): L: 0.449144; A: 0.779762; R: 0.779762; P: 0.780124; F1: 0.779666\n",
      "(train @ 30): L: 0.244584; A: 0.873016; R: 0.873016; P: 0.874680; F1: 0.872841\n",
      "(valid @ 30): L: 0.571218; A: 0.767857; R: 0.767857; P: 0.816512; F1: 0.756166\n",
      "Best val Metric 0.779666 @ 29\n",
      "\n",
      "models are saved @ ./predictions/211209101450/flvl_2_4_5_pretrained_model_c1.pt\n",
      "Saving predictions @ ./predictions/211209101450/flvl_train_2_4_5_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209101450/flvl_valid_3_6_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209101450/flvl_public_test_trained_on_2_4_5_r21d_rgb.csv\n",
      "{'train': [3, 4, 6], 'valid': [2, 5], 'public_test': [10, 11, 12], 'private_test': [13, 14, 15]}\n",
      "(train @ 1): L: 1.527662; A: 0.404762; R: 0.404762; P: 0.334213; F1: 0.345732\n",
      "(valid @ 1): L: 1.037788; A: 0.535714; R: 0.535714; P: 0.634286; F1: 0.449421\n",
      "(train @ 2): L: 1.212486; A: 0.436508; R: 0.436508; P: 0.580139; F1: 0.354276\n",
      "(valid @ 2): L: 0.986993; A: 0.440476; R: 0.440476; P: 0.328744; F1: 0.281282\n",
      "(train @ 3): L: 1.014202; A: 0.428571; R: 0.428571; P: 0.183673; F1: 0.257143\n",
      "(valid @ 3): L: 0.922165; A: 0.625000; R: 0.625000; P: 0.594966; F1: 0.576790\n",
      "(train @ 4): L: 0.986158; A: 0.488095; R: 0.488095; P: 0.548672; F1: 0.388837\n",
      "(valid @ 4): L: 0.804534; A: 0.601190; R: 0.601190; P: 0.510407; F1: 0.539749\n",
      "(train @ 5): L: 0.867239; A: 0.599206; R: 0.599206; P: 0.624730; F1: 0.593128\n",
      "(valid @ 5): L: 0.839496; A: 0.428571; R: 0.428571; P: 0.196542; F1: 0.269495\n",
      "(train @ 6): L: 0.850134; A: 0.480159; R: 0.480159; P: 0.407125; F1: 0.439492\n",
      "(valid @ 6): L: 0.762117; A: 0.636905; R: 0.636905; P: 0.573552; F1: 0.591867\n",
      "(train @ 7): L: 0.821950; A: 0.619048; R: 0.619048; P: 0.745398; F1: 0.590278\n",
      "(valid @ 7): L: 0.707403; A: 0.672619; R: 0.672619; P: 0.719691; F1: 0.655498\n",
      "(train @ 8): L: 0.828358; A: 0.630952; R: 0.630952; P: 0.666446; F1: 0.612239\n",
      "(valid @ 8): L: 0.697243; A: 0.702381; R: 0.702381; P: 0.728507; F1: 0.691783\n",
      "(train @ 9): L: 0.744424; A: 0.626984; R: 0.626984; P: 0.730415; F1: 0.606564\n",
      "(valid @ 9): L: 0.665836; A: 0.684524; R: 0.684524; P: 0.777656; F1: 0.673696\n",
      "(train @ 10): L: 0.695628; A: 0.611111; R: 0.611111; P: 0.630401; F1: 0.602216\n",
      "(valid @ 10): L: 0.785065; A: 0.511905; R: 0.511905; P: 0.343228; F1: 0.378335\n",
      "(train @ 11): L: 0.715732; A: 0.634921; R: 0.634921; P: 0.658994; F1: 0.624435\n",
      "(valid @ 11): L: 0.608367; A: 0.702381; R: 0.702381; P: 0.810222; F1: 0.680834\n",
      "(train @ 12): L: 0.648496; A: 0.666667; R: 0.666667; P: 0.727609; F1: 0.637056\n",
      "(valid @ 12): L: 0.611443; A: 0.630952; R: 0.630952; P: 0.707111; F1: 0.577997\n",
      "(train @ 13): L: 0.607670; A: 0.698413; R: 0.698413; P: 0.737867; F1: 0.682821\n",
      "(valid @ 13): L: 0.513666; A: 0.809524; R: 0.809524; P: 0.835691; F1: 0.807486\n",
      "(train @ 14): L: 0.567487; A: 0.662698; R: 0.662698; P: 0.732167; F1: 0.621840\n",
      "(valid @ 14): L: 0.501394; A: 0.738095; R: 0.738095; P: 0.774619; F1: 0.726241\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2025/1914999690.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'seed'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1337\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mrun_kfold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/ICV_fill/CORSMAL/filling_level/r21d_rgb/main_3Views_NoOpaque.py\u001b[0m in \u001b[0;36mrun_kfold\u001b[0;34m(cfg, c_list, use_pretrained, predict_on_private)\u001b[0m\n\u001b[1;32m    364\u001b[0m     ]\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m     \u001b[0mbest_fold_metrics_and_test_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_pretrained\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_on_private\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfold\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfolds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m     \u001b[0mbest_fold_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmetric\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbest_fold_metrics_and_test_predictions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m     \u001b[0mtest_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpreds\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbest_fold_metrics_and_test_predictions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ICV_fill/CORSMAL/filling_level/r21d_rgb/main_3Views_NoOpaque.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    364\u001b[0m     ]\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m     \u001b[0mbest_fold_metrics_and_test_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_pretrained\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_on_private\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfold\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfolds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m     \u001b[0mbest_fold_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmetric\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbest_fold_metrics_and_test_predictions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m     \u001b[0mtest_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpreds\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbest_fold_metrics_and_test_predictions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ICV_fill/CORSMAL/filling_level/r21d_rgb/main_3Views_NoOpaque.py\u001b[0m in \u001b[0;36mexperiment\u001b[0;34m(cfg, fold, use_pretrained, predict_on_private, c_list)\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0mbest_metric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         best_metric = train(\n\u001b[0m\u001b[1;32m    330\u001b[0m             \u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path_c1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path_c2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path_c3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         )\n",
      "\u001b[0;32m~/ICV_fill/CORSMAL/filling_level/r21d_rgb/main_3Views_NoOpaque.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(cfg, datasets, dataloaders, device, save_model_path_c1, save_model_path_c2, save_model_path_c3, c_list)\u001b[0m\n\u001b[1;32m    158\u001b[0m                     \u001b[0;31m# (B, T, D)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m                     \u001b[0moutputs_c1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhiddens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_c1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m                     \u001b[0moutputs_c2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhiddens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_c2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m                     \u001b[0moutputs_c3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhiddens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_c3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m                     \u001b[0;31m# outputs_c4, hiddens = model_c4(inputs[:, 3, :, :])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ICV_fill/CORSMAL/filling_level/r21d_rgb/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mhiddens_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhiddens\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhiddens_0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# (T, out_dim), (3, 3, 512)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    847\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 849\u001b[0;31m             result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0m\u001b[1;32m    850\u001b[0m                              self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[1;32m    851\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# View 0,2,3\n",
    "from main_3Views_NoOpaque import Config, run_kfold\n",
    "\n",
    "cfg = Config()\n",
    "cfg.assign_variable('task', 'flvl')\n",
    "cfg.assign_variable('output_dim', 3)\n",
    "cfg.assign_variable('model_type', 'GRU')\n",
    "cfg.assign_variable('bi_dir', False)\n",
    "cfg.assign_variable('device', 'cuda:0')\n",
    "cfg.assign_variable('data_root', './r21d_rgb_features')\n",
    "cfg.assign_variable('drop_p', 0.0) # results will be irreproducible\n",
    "cfg.assign_variable('batch_size', 64)\n",
    "cfg.assign_variable('input_dim', 512)\n",
    "cfg.assign_variable('hidden_dim', 512)\n",
    "cfg.assign_variable('n_layers', 3)\n",
    "cfg.assign_variable('num_epochs', 30)\n",
    "cfg.assign_variable('seed', 1337)\n",
    "\n",
    "run_kfold(cfg,[0,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# View 1,2,3\n",
    "from main_3Views_NoOpaque import Config, run_kfold\n",
    "\n",
    "cfg = Config()\n",
    "cfg.assign_variable('task', 'flvl')\n",
    "cfg.assign_variable('output_dim', 3)\n",
    "cfg.assign_variable('model_type', 'GRU')\n",
    "cfg.assign_variable('bi_dir', False)\n",
    "cfg.assign_variable('device', 'cuda:0')\n",
    "cfg.assign_variable('data_root', './r21d_rgb_features')\n",
    "cfg.assign_variable('drop_p', 0.0) # results will be irreproducible\n",
    "cfg.assign_variable('batch_size', 64)\n",
    "cfg.assign_variable('input_dim', 512)\n",
    "cfg.assign_variable('hidden_dim', 512)\n",
    "cfg.assign_variable('n_layers', 3)\n",
    "cfg.assign_variable('num_epochs', 30)\n",
    "cfg.assign_variable('seed', 1337)\n",
    "\n",
    "run_kfold(cfg,[1,2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### C. EXPERIMENT WITH 2 VIEWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': [2, 3, 5], 'valid': [4, 6], 'public_test': [10, 11, 12], 'private_test': [13, 14, 15]}\n",
      "(train @ 1): L: 1.189569; A: 0.472222; R: 0.472222; P: 0.419674; F1: 0.443324\n",
      "(valid @ 1): L: 1.045105; A: 0.505952; R: 0.505952; P: 0.576827; F1: 0.413521\n",
      "(train @ 2): L: 0.915744; A: 0.555556; R: 0.555556; P: 0.511557; F1: 0.511815\n",
      "(valid @ 2): L: 0.946251; A: 0.517857; R: 0.517857; P: 0.513032; F1: 0.513177\n",
      "(train @ 3): L: 0.830592; A: 0.559524; R: 0.559524; P: 0.607619; F1: 0.509189\n",
      "(valid @ 3): L: 1.002908; A: 0.529762; R: 0.529762; P: 0.563796; F1: 0.456663\n",
      "(train @ 4): L: 0.828893; A: 0.539683; R: 0.539683; P: 0.630932; F1: 0.502990\n",
      "(valid @ 4): L: 0.878804; A: 0.553571; R: 0.553571; P: 0.578513; F1: 0.552931\n",
      "(train @ 5): L: 0.766399; A: 0.611111; R: 0.611111; P: 0.644788; F1: 0.611948\n",
      "(valid @ 5): L: 0.972626; A: 0.517857; R: 0.517857; P: 0.625795; F1: 0.505574\n",
      "(train @ 6): L: 0.674026; A: 0.698413; R: 0.698413; P: 0.741167; F1: 0.694654\n",
      "(valid @ 6): L: 0.854755; A: 0.505952; R: 0.505952; P: 0.504885; F1: 0.503197\n",
      "(train @ 7): L: 0.656747; A: 0.702381; R: 0.702381; P: 0.715550; F1: 0.705465\n",
      "(valid @ 7): L: 0.997829; A: 0.565476; R: 0.565476; P: 0.668642; F1: 0.567444\n",
      "(train @ 8): L: 0.708313; A: 0.686508; R: 0.686508; P: 0.696047; F1: 0.688817\n",
      "(valid @ 8): L: 1.011781; A: 0.565476; R: 0.565476; P: 0.666756; F1: 0.564688\n",
      "(train @ 9): L: 0.554586; A: 0.757937; R: 0.757937; P: 0.795690; F1: 0.751617\n",
      "(valid @ 9): L: 0.894791; A: 0.577381; R: 0.577381; P: 0.608737; F1: 0.577687\n",
      "(train @ 10): L: 0.570971; A: 0.738095; R: 0.738095; P: 0.739848; F1: 0.738843\n",
      "(valid @ 10): L: 0.869550; A: 0.601190; R: 0.601190; P: 0.620918; F1: 0.599555\n",
      "(train @ 11): L: 0.504853; A: 0.773810; R: 0.773810; P: 0.779782; F1: 0.775538\n",
      "(valid @ 11): L: 0.982519; A: 0.619048; R: 0.619048; P: 0.677295; F1: 0.620825\n",
      "(train @ 12): L: 0.462033; A: 0.793651; R: 0.793651; P: 0.796197; F1: 0.794454\n",
      "(valid @ 12): L: 0.963509; A: 0.601190; R: 0.601190; P: 0.628678; F1: 0.598347\n",
      "(train @ 13): L: 0.449041; A: 0.769841; R: 0.769841; P: 0.771978; F1: 0.769911\n",
      "(valid @ 13): L: 1.030162; A: 0.636905; R: 0.636905; P: 0.702965; F1: 0.623994\n",
      "(train @ 14): L: 0.476781; A: 0.726190; R: 0.726190; P: 0.732756; F1: 0.726145\n",
      "(valid @ 14): L: 0.895827; A: 0.613095; R: 0.613095; P: 0.612080; F1: 0.606346\n",
      "(train @ 15): L: 0.459687; A: 0.809524; R: 0.809524; P: 0.820250; F1: 0.807241\n",
      "(valid @ 15): L: 0.885867; A: 0.642857; R: 0.642857; P: 0.645126; F1: 0.636040\n",
      "(train @ 16): L: 0.524881; A: 0.726190; R: 0.726190; P: 0.762771; F1: 0.713543\n",
      "(valid @ 16): L: 0.870663; A: 0.678571; R: 0.678571; P: 0.722004; F1: 0.657478\n",
      "(train @ 17): L: 0.445042; A: 0.746032; R: 0.746032; P: 0.823980; F1: 0.724797\n",
      "(valid @ 17): L: 0.795264; A: 0.636905; R: 0.636905; P: 0.636944; F1: 0.629991\n",
      "(train @ 18): L: 0.395681; A: 0.849206; R: 0.849206; P: 0.879765; F1: 0.845375\n",
      "(valid @ 18): L: 0.946009; A: 0.625000; R: 0.625000; P: 0.646541; F1: 0.620645\n",
      "(train @ 19): L: 0.375193; A: 0.837302; R: 0.837302; P: 0.845793; F1: 0.836033\n",
      "(valid @ 19): L: 0.851882; A: 0.666667; R: 0.666667; P: 0.672498; F1: 0.659165\n",
      "(train @ 20): L: 0.437113; A: 0.809524; R: 0.809524; P: 0.822434; F1: 0.807727\n",
      "(valid @ 20): L: 0.835499; A: 0.720238; R: 0.720238; P: 0.732427; F1: 0.712484\n",
      "(train @ 21): L: 0.459609; A: 0.746032; R: 0.746032; P: 0.760631; F1: 0.741036\n",
      "(valid @ 21): L: 0.750132; A: 0.684524; R: 0.684524; P: 0.682002; F1: 0.680525\n",
      "(train @ 22): L: 0.475335; A: 0.785714; R: 0.785714; P: 0.817816; F1: 0.778849\n",
      "(valid @ 22): L: 0.907863; A: 0.660714; R: 0.660714; P: 0.675376; F1: 0.653553\n",
      "(train @ 23): L: 0.406980; A: 0.781746; R: 0.781746; P: 0.788804; F1: 0.779352\n",
      "(valid @ 23): L: 0.716233; A: 0.696429; R: 0.696429; P: 0.698274; F1: 0.691230\n",
      "(train @ 24): L: 0.332539; A: 0.920635; R: 0.920635; P: 0.920755; F1: 0.920628\n",
      "(valid @ 24): L: 0.657997; A: 0.708333; R: 0.708333; P: 0.708231; F1: 0.706191\n",
      "(train @ 25): L: 0.296830; A: 0.904762; R: 0.904762; P: 0.905220; F1: 0.904729\n",
      "(valid @ 25): L: 0.846652; A: 0.690476; R: 0.690476; P: 0.701960; F1: 0.682375\n",
      "(train @ 26): L: 0.289995; A: 0.888889; R: 0.888889; P: 0.894315; F1: 0.888420\n",
      "(valid @ 26): L: 0.738280; A: 0.684524; R: 0.684524; P: 0.681188; F1: 0.682582\n",
      "(train @ 27): L: 0.501514; A: 0.757937; R: 0.757937; P: 0.770370; F1: 0.754094\n",
      "(valid @ 27): L: 1.232290; A: 0.636905; R: 0.636905; P: 0.716649; F1: 0.610780\n",
      "(train @ 28): L: 0.543882; A: 0.781746; R: 0.781746; P: 0.806426; F1: 0.775861\n",
      "(valid @ 28): L: 0.726294; A: 0.666667; R: 0.666667; P: 0.699007; F1: 0.646801\n",
      "(train @ 29): L: 0.533320; A: 0.690476; R: 0.690476; P: 0.743860; F1: 0.665631\n",
      "(valid @ 29): L: 0.795557; A: 0.708333; R: 0.708333; P: 0.786203; F1: 0.680361\n",
      "(train @ 30): L: 0.422619; A: 0.765873; R: 0.765873; P: 0.831198; F1: 0.750605\n",
      "(valid @ 30): L: 0.712884; A: 0.696429; R: 0.696429; P: 0.722944; F1: 0.687659\n",
      "Best val Metric 0.712484 @ 20\n",
      "\n",
      "models are saved @ ./predictions/211209103155/flvl_2_3_5_pretrained_model_c1.pt\n",
      "Saving predictions @ ./predictions/211209103155/flvl_train_2_3_5_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209103155/flvl_valid_4_6_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209103155/flvl_public_test_trained_on_2_3_5_r21d_rgb.csv\n",
      "{'train': [2, 4, 5], 'valid': [3, 6], 'public_test': [10, 11, 12], 'private_test': [13, 14, 15]}\n",
      "(train @ 1): L: 1.135366; A: 0.464286; R: 0.464286; P: 0.427979; F1: 0.437016\n",
      "(valid @ 1): L: 0.973467; A: 0.523810; R: 0.523810; P: 0.572674; F1: 0.444550\n",
      "(train @ 2): L: 0.904609; A: 0.500000; R: 0.500000; P: 0.577659; F1: 0.468598\n",
      "(valid @ 2): L: 0.919044; A: 0.583333; R: 0.583333; P: 0.676266; F1: 0.546138\n",
      "(train @ 3): L: 0.834367; A: 0.587302; R: 0.587302; P: 0.500411; F1: 0.539978\n",
      "(valid @ 3): L: 1.038315; A: 0.511905; R: 0.511905; P: 0.580952; F1: 0.422651\n",
      "(train @ 4): L: 0.821041; A: 0.551587; R: 0.551587; P: 0.662473; F1: 0.513276\n",
      "(valid @ 4): L: 0.870048; A: 0.565476; R: 0.565476; P: 0.588847; F1: 0.570445\n",
      "(train @ 5): L: 0.708170; A: 0.690476; R: 0.690476; P: 0.716440; F1: 0.686997\n",
      "(valid @ 5): L: 0.879916; A: 0.577381; R: 0.577381; P: 0.666523; F1: 0.574204\n",
      "(train @ 6): L: 0.706511; A: 0.702381; R: 0.702381; P: 0.738216; F1: 0.701716\n",
      "(valid @ 6): L: 0.943497; A: 0.565476; R: 0.565476; P: 0.649910; F1: 0.576754\n",
      "(train @ 7): L: 0.634267; A: 0.718254; R: 0.718254; P: 0.735503; F1: 0.717834\n",
      "(valid @ 7): L: 0.934363; A: 0.571429; R: 0.571429; P: 0.667340; F1: 0.567580\n",
      "(train @ 8): L: 0.804216; A: 0.595238; R: 0.595238; P: 0.597597; F1: 0.589799\n",
      "(valid @ 8): L: 1.422934; A: 0.446429; R: 0.446429; P: 0.638095; F1: 0.422536\n",
      "(train @ 9): L: 0.813382; A: 0.587302; R: 0.587302; P: 0.608181; F1: 0.587386\n",
      "(valid @ 9): L: 0.848111; A: 0.541667; R: 0.541667; P: 0.591429; F1: 0.532609\n",
      "(train @ 10): L: 0.796499; A: 0.611111; R: 0.611111; P: 0.654141; F1: 0.598590\n",
      "(valid @ 10): L: 0.891434; A: 0.553571; R: 0.553571; P: 0.677657; F1: 0.514265\n",
      "(train @ 11): L: 0.631477; A: 0.686508; R: 0.686508; P: 0.729720; F1: 0.681587\n",
      "(valid @ 11): L: 0.876365; A: 0.583333; R: 0.583333; P: 0.642857; F1: 0.582931\n",
      "(train @ 12): L: 0.538454; A: 0.785714; R: 0.785714; P: 0.815743; F1: 0.778233\n",
      "(valid @ 12): L: 0.733025; A: 0.702381; R: 0.702381; P: 0.757937; F1: 0.695131\n",
      "(train @ 13): L: 0.524743; A: 0.730159; R: 0.730159; P: 0.771340; F1: 0.729644\n",
      "(valid @ 13): L: 0.764200; A: 0.702381; R: 0.702381; P: 0.742537; F1: 0.691473\n",
      "(train @ 14): L: 0.479702; A: 0.781746; R: 0.781746; P: 0.783183; F1: 0.781782\n",
      "(valid @ 14): L: 0.846643; A: 0.613095; R: 0.613095; P: 0.632062; F1: 0.607202\n",
      "(train @ 15): L: 0.437581; A: 0.793651; R: 0.793651; P: 0.793459; F1: 0.792731\n",
      "(valid @ 15): L: 0.797617; A: 0.720238; R: 0.720238; P: 0.746958; F1: 0.707928\n",
      "(train @ 16): L: 0.487764; A: 0.765873; R: 0.765873; P: 0.772010; F1: 0.766211\n",
      "(valid @ 16): L: 0.900492; A: 0.684524; R: 0.684524; P: 0.775331; F1: 0.644850\n",
      "(train @ 17): L: 0.606599; A: 0.678571; R: 0.678571; P: 0.694745; F1: 0.672603\n",
      "(valid @ 17): L: 0.785511; A: 0.684524; R: 0.684524; P: 0.751382; F1: 0.660543\n",
      "(train @ 18): L: 0.435291; A: 0.785714; R: 0.785714; P: 0.809288; F1: 0.780270\n",
      "(valid @ 18): L: 0.619940; A: 0.690476; R: 0.690476; P: 0.685542; F1: 0.686609\n",
      "(train @ 19): L: 0.445881; A: 0.773810; R: 0.773810; P: 0.776785; F1: 0.772987\n",
      "(valid @ 19): L: 0.753622; A: 0.690476; R: 0.690476; P: 0.773383; F1: 0.658743\n",
      "(train @ 20): L: 0.435132; A: 0.765873; R: 0.765873; P: 0.768732; F1: 0.765022\n",
      "(valid @ 20): L: 0.618047; A: 0.726190; R: 0.726190; P: 0.732002; F1: 0.722809\n",
      "(train @ 21): L: 0.408747; A: 0.809524; R: 0.809524; P: 0.811583; F1: 0.809115\n",
      "(valid @ 21): L: 0.702895; A: 0.720238; R: 0.720238; P: 0.752385; F1: 0.706309\n",
      "(train @ 22): L: 0.479942; A: 0.777778; R: 0.777778; P: 0.788497; F1: 0.775000\n",
      "(valid @ 22): L: 0.662302; A: 0.720238; R: 0.720238; P: 0.735782; F1: 0.711239\n",
      "(train @ 23): L: 0.383492; A: 0.829365; R: 0.829365; P: 0.834439; F1: 0.828538\n",
      "(valid @ 23): L: 0.703929; A: 0.708333; R: 0.708333; P: 0.751814; F1: 0.688240\n",
      "(train @ 24): L: 0.355789; A: 0.845238; R: 0.845238; P: 0.846393; F1: 0.845075\n",
      "(valid @ 24): L: 0.568817; A: 0.732143; R: 0.732143; P: 0.732422; F1: 0.732027\n",
      "(train @ 25): L: 0.337182; A: 0.865079; R: 0.865079; P: 0.873469; F1: 0.864136\n",
      "(valid @ 25): L: 0.754511; A: 0.720238; R: 0.720238; P: 0.775892; F1: 0.698778\n",
      "(train @ 26): L: 0.371827; A: 0.817460; R: 0.817460; P: 0.835189; F1: 0.814341\n",
      "(valid @ 26): L: 0.584954; A: 0.708333; R: 0.708333; P: 0.711605; F1: 0.706621\n",
      "(train @ 27): L: 0.451696; A: 0.753968; R: 0.753968; P: 0.767122; F1: 0.749763\n",
      "(valid @ 27): L: 0.915989; A: 0.678571; R: 0.678571; P: 0.774876; F1: 0.637451\n",
      "(train @ 28): L: 0.459743; A: 0.773810; R: 0.773810; P: 0.781762; F1: 0.771651\n",
      "(valid @ 28): L: 0.523718; A: 0.732143; R: 0.732143; P: 0.732922; F1: 0.731820\n",
      "(train @ 29): L: 0.371662; A: 0.809524; R: 0.809524; P: 0.847377; F1: 0.802760\n",
      "(valid @ 29): L: 0.697814; A: 0.702381; R: 0.702381; P: 0.788894; F1: 0.670982\n",
      "(train @ 30): L: 0.429740; A: 0.785714; R: 0.785714; P: 0.830770; F1: 0.776200\n",
      "(valid @ 30): L: 0.552523; A: 0.726190; R: 0.726190; P: 0.767637; F1: 0.711326\n",
      "Best val Metric 0.732027 @ 24\n",
      "\n",
      "models are saved @ ./predictions/211209103155/flvl_2_4_5_pretrained_model_c1.pt\n",
      "Saving predictions @ ./predictions/211209103155/flvl_train_2_4_5_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209103155/flvl_valid_3_6_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209103155/flvl_public_test_trained_on_2_4_5_r21d_rgb.csv\n",
      "{'train': [3, 4, 6], 'valid': [2, 5], 'public_test': [10, 11, 12], 'private_test': [13, 14, 15]}\n",
      "(train @ 1): L: 1.236201; A: 0.412698; R: 0.412698; P: 0.385545; F1: 0.387622\n",
      "(valid @ 1): L: 0.919284; A: 0.553571; R: 0.553571; P: 0.604076; F1: 0.482310\n",
      "(train @ 2): L: 0.997633; A: 0.511905; R: 0.511905; P: 0.542330; F1: 0.434206\n",
      "(valid @ 2): L: 0.897945; A: 0.440476; R: 0.440476; P: 0.328744; F1: 0.281282\n",
      "(train @ 3): L: 0.983300; A: 0.452381; R: 0.452381; P: 0.259598; F1: 0.301762\n",
      "(valid @ 3): L: 0.840638; A: 0.607143; R: 0.607143; P: 0.585044; F1: 0.557656\n",
      "(train @ 4): L: 0.889681; A: 0.492063; R: 0.492063; P: 0.509753; F1: 0.411203\n",
      "(valid @ 4): L: 0.777287; A: 0.589286; R: 0.589286; P: 0.735248; F1: 0.549825\n",
      "(train @ 5): L: 0.862748; A: 0.626984; R: 0.626984; P: 0.709647; F1: 0.610209\n",
      "(valid @ 5): L: 0.926850; A: 0.428571; R: 0.428571; P: 0.185886; F1: 0.259304\n",
      "(train @ 6): L: 0.937616; A: 0.456349; R: 0.456349; P: 0.533407; F1: 0.410239\n",
      "(valid @ 6): L: 0.775000; A: 0.583333; R: 0.583333; P: 0.595635; F1: 0.525528\n",
      "(train @ 7): L: 0.833033; A: 0.559524; R: 0.559524; P: 0.735077; F1: 0.504183\n",
      "(valid @ 7): L: 0.752764; A: 0.511905; R: 0.511905; P: 0.448008; F1: 0.398203\n",
      "(train @ 8): L: 0.892966; A: 0.484127; R: 0.484127; P: 0.542113; F1: 0.450140\n",
      "(valid @ 8): L: 0.753662; A: 0.607143; R: 0.607143; P: 0.667077; F1: 0.561121\n",
      "(train @ 9): L: 0.798776; A: 0.623016; R: 0.623016; P: 0.707770; F1: 0.608065\n",
      "(valid @ 9): L: 0.673608; A: 0.654762; R: 0.654762; P: 0.790518; F1: 0.621249\n",
      "(train @ 10): L: 0.735514; A: 0.623016; R: 0.623016; P: 0.644350; F1: 0.607892\n",
      "(valid @ 10): L: 0.720401; A: 0.517857; R: 0.517857; P: 0.344538; F1: 0.384176\n",
      "(train @ 11): L: 0.683650; A: 0.603175; R: 0.603175; P: 0.629507; F1: 0.569032\n",
      "(valid @ 11): L: 0.603254; A: 0.738095; R: 0.738095; P: 0.751742; F1: 0.740816\n",
      "(train @ 12): L: 0.657124; A: 0.646825; R: 0.646825; P: 0.688245; F1: 0.624025\n",
      "(valid @ 12): L: 0.553887; A: 0.750000; R: 0.750000; P: 0.754545; F1: 0.750572\n",
      "(train @ 13): L: 0.616361; A: 0.710317; R: 0.710317; P: 0.722847; F1: 0.705142\n",
      "(valid @ 13): L: 0.557229; A: 0.732143; R: 0.732143; P: 0.756604; F1: 0.724870\n",
      "(train @ 14): L: 0.578917; A: 0.702381; R: 0.702381; P: 0.706445; F1: 0.703887\n",
      "(valid @ 14): L: 0.552631; A: 0.702381; R: 0.702381; P: 0.744637; F1: 0.684384\n",
      "(train @ 15): L: 0.557531; A: 0.718254; R: 0.718254; P: 0.724836; F1: 0.715518\n",
      "(valid @ 15): L: 0.487195; A: 0.755952; R: 0.755952; P: 0.776922; F1: 0.749563\n",
      "(train @ 16): L: 0.526350; A: 0.746032; R: 0.746032; P: 0.746032; F1: 0.746032\n",
      "(valid @ 16): L: 0.491289; A: 0.738095; R: 0.738095; P: 0.774619; F1: 0.726241\n",
      "(train @ 17): L: 0.548405; A: 0.706349; R: 0.706349; P: 0.707756; F1: 0.706995\n",
      "(valid @ 17): L: 0.451824; A: 0.791667; R: 0.791667; P: 0.801659; F1: 0.789381\n",
      "(train @ 18): L: 0.512007; A: 0.726190; R: 0.726190; P: 0.749398; F1: 0.716963\n",
      "(valid @ 18): L: 0.473539; A: 0.750000; R: 0.750000; P: 0.818946; F1: 0.731288\n",
      "(train @ 19): L: 0.499970; A: 0.769841; R: 0.769841; P: 0.772321; F1: 0.769129\n",
      "(valid @ 19): L: 0.466892; A: 0.755952; R: 0.755952; P: 0.832573; F1: 0.736634\n",
      "(train @ 20): L: 0.478880; A: 0.757937; R: 0.757937; P: 0.763893; F1: 0.756049\n",
      "(valid @ 20): L: 0.412312; A: 0.815476; R: 0.815476; P: 0.823700; F1: 0.813960\n",
      "(train @ 21): L: 0.486291; A: 0.742063; R: 0.742063; P: 0.757384; F1: 0.736639\n",
      "(valid @ 21): L: 0.448444; A: 0.761905; R: 0.761905; P: 0.835447; F1: 0.744083\n",
      "(train @ 22): L: 0.471202; A: 0.781746; R: 0.781746; P: 0.803404; F1: 0.776530\n",
      "(valid @ 22): L: 0.392289; A: 0.857143; R: 0.857143; P: 0.858027; F1: 0.857033\n",
      "(train @ 23): L: 0.466984; A: 0.769841; R: 0.769841; P: 0.771557; F1: 0.769347\n",
      "(valid @ 23): L: 0.401486; A: 0.827381; R: 0.827381; P: 0.862470; F1: 0.822016\n",
      "(train @ 24): L: 0.427168; A: 0.809524; R: 0.809524; P: 0.811583; F1: 0.809115\n",
      "(valid @ 24): L: 0.353801; A: 0.857143; R: 0.857143; P: 0.858027; F1: 0.857033\n",
      "(train @ 25): L: 0.405724; A: 0.805556; R: 0.805556; P: 0.808998; F1: 0.804849\n",
      "(valid @ 25): L: 0.344791; A: 0.869048; R: 0.869048; P: 0.874902; F1: 0.868413\n",
      "(train @ 26): L: 0.405743; A: 0.821429; R: 0.821429; P: 0.821622; F1: 0.821394\n",
      "(valid @ 26): L: 0.361666; A: 0.851190; R: 0.851190; P: 0.877202; F1: 0.847957\n",
      "(train @ 27): L: 0.405208; A: 0.817460; R: 0.817460; P: 0.817545; F1: 0.817445\n",
      "(valid @ 27): L: 0.316847; A: 0.875000; R: 0.875000; P: 0.875059; F1: 0.874994\n",
      "(train @ 28): L: 0.392108; A: 0.817460; R: 0.817460; P: 0.817460; F1: 0.817460\n",
      "(valid @ 28): L: 0.335724; A: 0.869048; R: 0.869048; P: 0.888889; F1: 0.866969\n",
      "(train @ 29): L: 0.409166; A: 0.813492; R: 0.813492; P: 0.815185; F1: 0.813168\n",
      "(valid @ 29): L: 0.334559; A: 0.875000; R: 0.875000; P: 0.892923; F1: 0.873233\n",
      "(train @ 30): L: 0.368529; A: 0.849206; R: 0.849206; P: 0.853954; F1: 0.848570\n",
      "(valid @ 30): L: 0.333107; A: 0.851190; R: 0.851190; P: 0.867708; F1: 0.849087\n",
      "Best val Metric 0.874994 @ 27\n",
      "\n",
      "models are saved @ ./predictions/211209103155/flvl_3_4_6_pretrained_model_c1.pt\n",
      "Saving predictions @ ./predictions/211209103155/flvl_train_3_4_6_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209103155/flvl_valid_2_5_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209103155/flvl_public_test_trained_on_3_4_6_r21d_rgb.csv\n",
      "Average of Best Metrics on Each Valid Set: 0.773168, 211209103155\n",
      "Saved test results @ ./predictions/211209103155/flvl_public_test_agg_r21d_rgb.csv\n"
     ]
    }
   ],
   "source": [
    "# View 0,1\n",
    "from main_2Views_NoOpaque import Config, run_kfold\n",
    "\n",
    "cfg = Config()\n",
    "cfg.assign_variable('task', 'flvl')\n",
    "cfg.assign_variable('output_dim', 3)\n",
    "cfg.assign_variable('model_type', 'GRU')\n",
    "cfg.assign_variable('bi_dir', False)\n",
    "cfg.assign_variable('device', 'cuda:0')\n",
    "cfg.assign_variable('data_root', './r21d_rgb_features')\n",
    "cfg.assign_variable('drop_p', 0.0) # results will be irreproducible\n",
    "cfg.assign_variable('batch_size', 64)\n",
    "cfg.assign_variable('input_dim', 512)\n",
    "cfg.assign_variable('hidden_dim', 512)\n",
    "cfg.assign_variable('n_layers', 3)\n",
    "cfg.assign_variable('num_epochs', 30)\n",
    "cfg.assign_variable('seed', 1337)\n",
    "\n",
    "run_kfold(cfg,[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# View 0,2\n",
    "from main_2Views_NoOpaque import Config, run_kfold\n",
    "\n",
    "cfg = Config()\n",
    "cfg.assign_variable('task', 'flvl')\n",
    "cfg.assign_variable('output_dim', 3)\n",
    "cfg.assign_variable('model_type', 'GRU')\n",
    "cfg.assign_variable('bi_dir', False)\n",
    "cfg.assign_variable('device', 'cuda:0')\n",
    "cfg.assign_variable('data_root', './r21d_rgb_features')\n",
    "cfg.assign_variable('drop_p', 0.0) # results will be irreproducible\n",
    "cfg.assign_variable('batch_size', 64)\n",
    "cfg.assign_variable('input_dim', 512)\n",
    "cfg.assign_variable('hidden_dim', 512)\n",
    "cfg.assign_variable('n_layers', 3)\n",
    "cfg.assign_variable('num_epochs', 30)\n",
    "cfg.assign_variable('seed', 1337)\n",
    "\n",
    "run_kfold(cfg,[0,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# View 0,3\n",
    "from main_2Views_NoOpaque import Config, run_kfold\n",
    "\n",
    "cfg = Config()\n",
    "cfg.assign_variable('task', 'flvl')\n",
    "cfg.assign_variable('output_dim', 3)\n",
    "cfg.assign_variable('model_type', 'GRU')\n",
    "cfg.assign_variable('bi_dir', False)\n",
    "cfg.assign_variable('device', 'cuda:0')\n",
    "cfg.assign_variable('data_root', './r21d_rgb_features')\n",
    "cfg.assign_variable('drop_p', 0.0) # results will be irreproducible\n",
    "cfg.assign_variable('batch_size', 64)\n",
    "cfg.assign_variable('input_dim', 512)\n",
    "cfg.assign_variable('hidden_dim', 512)\n",
    "cfg.assign_variable('n_layers', 3)\n",
    "cfg.assign_variable('num_epochs', 30)\n",
    "cfg.assign_variable('seed', 1337)\n",
    "\n",
    "run_kfold(cfg,[0,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# View 1,2\n",
    "from main_2Views_NoOpaque import Config, run_kfold\n",
    "\n",
    "cfg = Config()\n",
    "cfg.assign_variable('task', 'flvl')\n",
    "cfg.assign_variable('output_dim', 3)\n",
    "cfg.assign_variable('model_type', 'GRU')\n",
    "cfg.assign_variable('bi_dir', False)\n",
    "cfg.assign_variable('device', 'cuda:0')\n",
    "cfg.assign_variable('data_root', './r21d_rgb_features')\n",
    "cfg.assign_variable('drop_p', 0.0) # results will be irreproducible\n",
    "cfg.assign_variable('batch_size', 64)\n",
    "cfg.assign_variable('input_dim', 512)\n",
    "cfg.assign_variable('hidden_dim', 512)\n",
    "cfg.assign_variable('n_layers', 3)\n",
    "cfg.assign_variable('num_epochs', 30)\n",
    "cfg.assign_variable('seed', 1337)\n",
    "\n",
    "run_kfold(cfg,[1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# View 1,3\n",
    "from main_2Views_NoOpaque import Config, run_kfold\n",
    "\n",
    "cfg = Config()\n",
    "cfg.assign_variable('task', 'flvl')\n",
    "cfg.assign_variable('output_dim', 3)\n",
    "cfg.assign_variable('model_type', 'GRU')\n",
    "cfg.assign_variable('bi_dir', False)\n",
    "cfg.assign_variable('device', 'cuda:0')\n",
    "cfg.assign_variable('data_root', './r21d_rgb_features')\n",
    "cfg.assign_variable('drop_p', 0.0) # results will be irreproducible\n",
    "cfg.assign_variable('batch_size', 64)\n",
    "cfg.assign_variable('input_dim', 512)\n",
    "cfg.assign_variable('hidden_dim', 512)\n",
    "cfg.assign_variable('n_layers', 3)\n",
    "cfg.assign_variable('num_epochs', 30)\n",
    "cfg.assign_variable('seed', 1337)\n",
    "\n",
    "run_kfold(cfg,[1,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# View 2,3\n",
    "from main_2Views_NoOpaque import Config, run_kfold\n",
    "\n",
    "cfg = Config()\n",
    "cfg.assign_variable('task', 'flvl')\n",
    "cfg.assign_variable('output_dim', 3)\n",
    "cfg.assign_variable('model_type', 'GRU')\n",
    "cfg.assign_variable('bi_dir', False)\n",
    "cfg.assign_variable('device', 'cuda:0')\n",
    "cfg.assign_variable('data_root', './r21d_rgb_features')\n",
    "cfg.assign_variable('drop_p', 0.0) # results will be irreproducible\n",
    "cfg.assign_variable('batch_size', 64)\n",
    "cfg.assign_variable('input_dim', 512)\n",
    "cfg.assign_variable('hidden_dim', 512)\n",
    "cfg.assign_variable('n_layers', 3)\n",
    "cfg.assign_variable('num_epochs', 30)\n",
    "cfg.assign_variable('seed', 1337)\n",
    "\n",
    "run_kfold(cfg,[2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### D. Stacked-feature test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': [2, 3, 5], 'valid': [4, 6], 'public_test': [10, 11, 12], 'private_test': [13, 14, 15]}\n",
      "(train @ 1): L: 1.131648; A: 0.317460; R: 0.317460; P: 0.315020; F1: 0.305254\n",
      "(valid @ 1): L: 0.964650; A: 0.470238; R: 0.470238; P: 0.541732; F1: 0.354952\n",
      "(train @ 2): L: 0.934706; A: 0.511905; R: 0.511905; P: 0.466954; F1: 0.470159\n",
      "(valid @ 2): L: 0.920974; A: 0.505952; R: 0.505952; P: 0.576827; F1: 0.413521\n",
      "(train @ 3): L: 0.891857; A: 0.531746; R: 0.531746; P: 0.470220; F1: 0.490261\n",
      "(valid @ 3): L: 0.872071; A: 0.565476; R: 0.565476; P: 0.519700; F1: 0.521840\n",
      "(train @ 4): L: 0.811468; A: 0.579365; R: 0.579365; P: 0.512820; F1: 0.538187\n",
      "(valid @ 4): L: 0.929984; A: 0.583333; R: 0.583333; P: 0.687600; F1: 0.541291\n",
      "(train @ 5): L: 0.783027; A: 0.619048; R: 0.619048; P: 0.710772; F1: 0.600437\n",
      "(valid @ 5): L: 0.824530; A: 0.482143; R: 0.482143; P: 0.543471; F1: 0.452434\n",
      "(train @ 6): L: 0.751725; A: 0.607143; R: 0.607143; P: 0.628629; F1: 0.595233\n",
      "(valid @ 6): L: 0.862973; A: 0.636905; R: 0.636905; P: 0.709536; F1: 0.595803\n",
      "(train @ 7): L: 0.669110; A: 0.682540; R: 0.682540; P: 0.763455; F1: 0.673469\n",
      "(valid @ 7): L: 0.719111; A: 0.666667; R: 0.666667; P: 0.706792; F1: 0.666355\n",
      "(train @ 8): L: 0.587451; A: 0.801587; R: 0.801587; P: 0.804918; F1: 0.802691\n",
      "(valid @ 8): L: 0.743183; A: 0.672619; R: 0.672619; P: 0.675250; F1: 0.667497\n",
      "(train @ 9): L: 0.522857; A: 0.821429; R: 0.821429; P: 0.822266; F1: 0.821812\n",
      "(valid @ 9): L: 0.811913; A: 0.672619; R: 0.672619; P: 0.742751; F1: 0.641099\n",
      "(train @ 10): L: 0.477808; A: 0.821429; R: 0.821429; P: 0.830789; F1: 0.821246\n",
      "(valid @ 10): L: 0.667667; A: 0.642857; R: 0.642857; P: 0.650072; F1: 0.644477\n",
      "(train @ 11): L: 0.630755; A: 0.662698; R: 0.662698; P: 0.663565; F1: 0.661599\n",
      "(valid @ 11): L: 0.709324; A: 0.684524; R: 0.684524; P: 0.748440; F1: 0.650593\n",
      "(train @ 12): L: 0.455926; A: 0.841270; R: 0.841270; P: 0.851399; F1: 0.840505\n",
      "(valid @ 12): L: 0.567833; A: 0.696429; R: 0.696429; P: 0.705166; F1: 0.696677\n",
      "(train @ 13): L: 0.440726; A: 0.773810; R: 0.773810; P: 0.778951; F1: 0.772400\n",
      "(valid @ 13): L: 0.558764; A: 0.726190; R: 0.726190; P: 0.732002; F1: 0.722809\n",
      "(train @ 14): L: 0.459302; A: 0.797619; R: 0.797619; P: 0.820912; F1: 0.792782\n",
      "(valid @ 14): L: 0.701151; A: 0.702381; R: 0.702381; P: 0.759354; F1: 0.676923\n",
      "(train @ 15): L: 0.408335; A: 0.817460; R: 0.817460; P: 0.818222; F1: 0.817319\n",
      "(valid @ 15): L: 0.530058; A: 0.726190; R: 0.726190; P: 0.731981; F1: 0.727444\n",
      "(train @ 16): L: 0.326706; A: 0.900794; R: 0.900794; P: 0.904246; F1: 0.900536\n",
      "(valid @ 16): L: 0.697834; A: 0.714286; R: 0.714286; P: 0.778061; F1: 0.690397\n",
      "(train @ 17): L: 0.339977; A: 0.873016; R: 0.873016; P: 0.874680; F1: 0.872841\n",
      "(valid @ 17): L: 0.593048; A: 0.744048; R: 0.744048; P: 0.760098; F1: 0.738486\n",
      "(train @ 18): L: 0.456622; A: 0.813492; R: 0.813492; P: 0.814134; F1: 0.812724\n",
      "(valid @ 18): L: 0.529550; A: 0.755952; R: 0.755952; P: 0.756273; F1: 0.755846\n",
      "(train @ 19): L: 0.279049; A: 0.892857; R: 0.892857; P: 0.893105; F1: 0.892836\n",
      "(valid @ 19): L: 0.761348; A: 0.714286; R: 0.714286; P: 0.787970; F1: 0.687720\n",
      "(train @ 20): L: 0.490610; A: 0.734127; R: 0.734127; P: 0.742972; F1: 0.731352\n",
      "(valid @ 20): L: 0.523174; A: 0.767857; R: 0.767857; P: 0.800000; F1: 0.759398\n",
      "(train @ 21): L: 0.421035; A: 0.837302; R: 0.837302; P: 0.861174; F1: 0.833880\n",
      "(valid @ 21): L: 0.472590; A: 0.767857; R: 0.767857; P: 0.776770; F1: 0.765311\n",
      "(train @ 22): L: 0.337860; A: 0.861111; R: 0.861111; P: 0.887121; F1: 0.858190\n",
      "(valid @ 22): L: 0.516786; A: 0.761905; R: 0.761905; P: 0.801921; F1: 0.751102\n",
      "(train @ 23): L: 0.427824; A: 0.773810; R: 0.773810; P: 0.821092; F1: 0.762568\n",
      "(valid @ 23): L: 0.450348; A: 0.791667; R: 0.791667; P: 0.792734; F1: 0.791415\n",
      "(train @ 24): L: 0.318621; A: 0.892857; R: 0.892857; P: 0.914286; F1: 0.891156\n",
      "(valid @ 24): L: 0.462220; A: 0.773810; R: 0.773810; P: 0.784323; F1: 0.770982\n",
      "(train @ 25): L: 0.265439; A: 0.920635; R: 0.920635; P: 0.926603; F1: 0.920300\n",
      "(valid @ 25): L: 0.448293; A: 0.785714; R: 0.785714; P: 0.789929; F1: 0.784676\n",
      "(train @ 26): L: 0.229174; A: 0.928571; R: 0.928571; P: 0.929677; F1: 0.928516\n",
      "(valid @ 26): L: 0.479675; A: 0.797619; R: 0.797619; P: 0.812698; F1: 0.794407\n",
      "(train @ 27): L: 0.202527; A: 0.928571; R: 0.928571; P: 0.929677; F1: 0.928516\n",
      "(valid @ 27): L: 0.480132; A: 0.779762; R: 0.779762; P: 0.786783; F1: 0.777952\n",
      "(train @ 28): L: 0.237691; A: 0.900794; R: 0.900794; P: 0.904246; F1: 0.900536\n",
      "(valid @ 28): L: 0.610623; A: 0.755952; R: 0.755952; P: 0.797943; F1: 0.744092\n",
      "(train @ 29): L: 0.234340; A: 0.900794; R: 0.900794; P: 0.902183; F1: 0.900689\n",
      "(valid @ 29): L: 0.485194; A: 0.797619; R: 0.797619; P: 0.798319; F1: 0.797463\n",
      "(train @ 30): L: 0.148241; A: 0.944444; R: 0.944444; P: 0.944957; F1: 0.944425\n",
      "(valid @ 30): L: 0.491290; A: 0.791667; R: 0.791667; P: 0.795163; F1: 0.790850\n",
      "Best val Metric 0.797463 @ 29\n",
      "\n",
      "models are saved @ ./predictions/211209104933/flvl_2_3_5_pretrained_model_c1.pt\n",
      "Saving predictions @ ./predictions/211209104933/flvl_train_2_3_5_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209104933/flvl_valid_4_6_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209104933/flvl_public_test_trained_on_2_3_5_r21d_rgb.csv\n",
      "{'train': [2, 4, 5], 'valid': [3, 6], 'public_test': [10, 11, 12], 'private_test': [13, 14, 15]}\n",
      "(train @ 1): L: 1.124620; A: 0.333333; R: 0.333333; P: 0.323858; F1: 0.313058\n",
      "(valid @ 1): L: 0.959570; A: 0.488095; R: 0.488095; P: 0.587912; F1: 0.379162\n",
      "(train @ 2): L: 0.899964; A: 0.559524; R: 0.559524; P: 0.575891; F1: 0.497784\n",
      "(valid @ 2): L: 0.898733; A: 0.553571; R: 0.553571; P: 0.604076; F1: 0.482310\n",
      "(train @ 3): L: 0.887335; A: 0.567460; R: 0.567460; P: 0.487858; F1: 0.524526\n",
      "(valid @ 3): L: 0.908280; A: 0.505952; R: 0.505952; P: 0.598880; F1: 0.408407\n",
      "(train @ 4): L: 0.860511; A: 0.500000; R: 0.500000; P: 0.460714; F1: 0.455357\n",
      "(valid @ 4): L: 0.827831; A: 0.577381; R: 0.577381; P: 0.634957; F1: 0.565047\n",
      "(train @ 5): L: 0.773419; A: 0.650794; R: 0.650794; P: 0.683298; F1: 0.653356\n",
      "(valid @ 5): L: 0.786595; A: 0.577381; R: 0.577381; P: 0.627408; F1: 0.571947\n",
      "(train @ 6): L: 0.756974; A: 0.571429; R: 0.571429; P: 0.608651; F1: 0.565351\n",
      "(valid @ 6): L: 0.811244; A: 0.672619; R: 0.672619; P: 0.735615; F1: 0.648048\n",
      "(train @ 7): L: 0.649458; A: 0.722222; R: 0.722222; P: 0.777867; F1: 0.711419\n",
      "(valid @ 7): L: 0.702056; A: 0.666667; R: 0.666667; P: 0.698980; F1: 0.667857\n",
      "(train @ 8): L: 0.633725; A: 0.662698; R: 0.662698; P: 0.673442; F1: 0.665795\n",
      "(valid @ 8): L: 0.717325; A: 0.696429; R: 0.696429; P: 0.735571; F1: 0.679790\n",
      "(train @ 9): L: 0.524229; A: 0.825397; R: 0.825397; P: 0.826798; F1: 0.825157\n",
      "(valid @ 9): L: 0.685491; A: 0.708333; R: 0.708333; P: 0.756047; F1: 0.689653\n",
      "(train @ 10): L: 0.688471; A: 0.674603; R: 0.674603; P: 0.683233; F1: 0.668413\n",
      "(valid @ 10): L: 0.633738; A: 0.702381; R: 0.702381; P: 0.748977; F1: 0.698995\n",
      "(train @ 11): L: 0.684055; A: 0.670635; R: 0.670635; P: 0.708449; F1: 0.647502\n",
      "(valid @ 11): L: 0.600562; A: 0.702381; R: 0.702381; P: 0.788894; F1: 0.670982\n",
      "(train @ 12): L: 0.574796; A: 0.690476; R: 0.690476; P: 0.751349; F1: 0.665396\n",
      "(valid @ 12): L: 0.605298; A: 0.684524; R: 0.684524; P: 0.775238; F1: 0.648773\n",
      "(train @ 13): L: 0.497476; A: 0.857143; R: 0.857143; P: 0.859614; F1: 0.856836\n",
      "(valid @ 13): L: 0.556172; A: 0.684524; R: 0.684524; P: 0.690271; F1: 0.684521\n",
      "(train @ 14): L: 0.497937; A: 0.769841; R: 0.769841; P: 0.774371; F1: 0.769722\n",
      "(valid @ 14): L: 0.604680; A: 0.672619; R: 0.672619; P: 0.774515; F1: 0.625665\n",
      "(train @ 15): L: 0.450896; A: 0.781746; R: 0.781746; P: 0.791738; F1: 0.779243\n",
      "(valid @ 15): L: 0.527786; A: 0.755952; R: 0.755952; P: 0.786147; F1: 0.747060\n",
      "(train @ 16): L: 0.393076; A: 0.861111; R: 0.861111; P: 0.862333; F1: 0.860965\n",
      "(valid @ 16): L: 0.571888; A: 0.726190; R: 0.726190; P: 0.795281; F1: 0.703297\n",
      "(train @ 17): L: 0.358286; A: 0.880952; R: 0.880952; P: 0.889796; F1: 0.880120\n",
      "(valid @ 17): L: 0.493255; A: 0.773810; R: 0.773810; P: 0.777790; F1: 0.772713\n",
      "(train @ 18): L: 0.381662; A: 0.857143; R: 0.857143; P: 0.860714; F1: 0.856701\n",
      "(valid @ 18): L: 0.687365; A: 0.708333; R: 0.708333; P: 0.792245; F1: 0.679071\n",
      "(train @ 19): L: 0.388886; A: 0.813492; R: 0.813492; P: 0.813513; F1: 0.813488\n",
      "(valid @ 19): L: 0.529801; A: 0.732143; R: 0.732143; P: 0.768716; F1: 0.719126\n",
      "(train @ 20): L: 0.352100; A: 0.845238; R: 0.845238; P: 0.845826; F1: 0.845155\n",
      "(valid @ 20): L: 0.570615; A: 0.750000; R: 0.750000; P: 0.809524; F1: 0.733333\n",
      "(train @ 21): L: 0.410385; A: 0.769841; R: 0.769841; P: 0.770114; F1: 0.769762\n",
      "(valid @ 21): L: 0.465655; A: 0.773810; R: 0.773810; P: 0.804158; F1: 0.766187\n",
      "(train @ 22): L: 0.275569; A: 0.880952; R: 0.880952; P: 0.881911; F1: 0.880860\n",
      "(valid @ 22): L: 0.564583; A: 0.720238; R: 0.720238; P: 0.791648; F1: 0.695558\n",
      "(train @ 23): L: 0.301735; A: 0.849206; R: 0.849206; P: 0.850739; F1: 0.848999\n",
      "(valid @ 23): L: 0.451128; A: 0.803571; R: 0.803571; P: 0.825156; F1: 0.799303\n",
      "(train @ 24): L: 0.329163; A: 0.841270; R: 0.841270; P: 0.841362; F1: 0.841256\n",
      "(valid @ 24): L: 0.397310; A: 0.791667; R: 0.791667; P: 0.792734; F1: 0.791415\n",
      "(train @ 25): L: 0.238478; A: 0.924603; R: 0.924603; P: 0.929796; F1: 0.924329\n",
      "(valid @ 25): L: 0.419953; A: 0.821429; R: 0.821429; P: 0.831252; F1: 0.819725\n",
      "(train @ 26): L: 0.210638; A: 0.908730; R: 0.908730; P: 0.913689; F1: 0.908398\n",
      "(valid @ 26): L: 0.427681; A: 0.827381; R: 0.827381; P: 0.842492; F1: 0.824941\n",
      "(train @ 27): L: 0.340883; A: 0.865079; R: 0.865079; P: 0.864785; F1: 0.864803\n",
      "(valid @ 27): L: 0.545507; A: 0.767857; R: 0.767857; P: 0.812557; F1: 0.756576\n",
      "(train @ 28): L: 0.255588; A: 0.920635; R: 0.920635; P: 0.920755; F1: 0.920628\n",
      "(valid @ 28): L: 0.391375; A: 0.797619; R: 0.797619; P: 0.798319; F1: 0.797463\n",
      "(train @ 29): L: 0.182710; A: 0.936508; R: 0.936508; P: 0.937638; F1: 0.936459\n",
      "(valid @ 29): L: 0.395127; A: 0.833333; R: 0.833333; P: 0.833333; F1: 0.833333\n",
      "(train @ 30): L: 0.182603; A: 0.936508; R: 0.936508; P: 0.938522; F1: 0.936421\n",
      "(valid @ 30): L: 0.458010; A: 0.809524; R: 0.809524; P: 0.821892; F1: 0.807143\n",
      "Best val Metric 0.833333 @ 29\n",
      "\n",
      "models are saved @ ./predictions/211209104933/flvl_2_4_5_pretrained_model_c1.pt\n",
      "Saving predictions @ ./predictions/211209104933/flvl_train_2_4_5_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209104933/flvl_valid_3_6_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209104933/flvl_public_test_trained_on_2_4_5_r21d_rgb.csv\n",
      "{'train': [3, 4, 6], 'valid': [2, 5], 'public_test': [10, 11, 12], 'private_test': [13, 14, 15]}\n",
      "(train @ 1): L: 1.129746; A: 0.376984; R: 0.376984; P: 0.331095; F1: 0.341731\n",
      "(valid @ 1): L: 0.960194; A: 0.482143; R: 0.482143; P: 0.622642; F1: 0.362400\n",
      "(train @ 2): L: 0.971570; A: 0.456349; R: 0.456349; P: 0.398582; F1: 0.407773\n",
      "(valid @ 2): L: 0.905632; A: 0.607143; R: 0.607143; P: 0.538579; F1: 0.564146\n",
      "(train @ 3): L: 0.923461; A: 0.515873; R: 0.515873; P: 0.445910; F1: 0.477743\n",
      "(valid @ 3): L: 0.869553; A: 0.523810; R: 0.523810; P: 0.607619; F1: 0.436036\n",
      "(train @ 4): L: 0.945857; A: 0.452381; R: 0.452381; P: 0.390598; F1: 0.417531\n",
      "(valid @ 4): L: 0.878862; A: 0.428571; R: 0.428571; P: 0.187013; F1: 0.260398\n",
      "(train @ 5): L: 0.866111; A: 0.523810; R: 0.523810; P: 0.594264; F1: 0.489320\n",
      "(valid @ 5): L: 0.788505; A: 0.654762; R: 0.654762; P: 0.558604; F1: 0.602538\n",
      "(train @ 6): L: 0.827573; A: 0.543651; R: 0.543651; P: 0.585379; F1: 0.540075\n",
      "(valid @ 6): L: 0.726069; A: 0.732143; R: 0.732143; P: 0.753903; F1: 0.722753\n",
      "(train @ 7): L: 0.723962; A: 0.682540; R: 0.682540; P: 0.756367; F1: 0.660385\n",
      "(valid @ 7): L: 0.652769; A: 0.767857; R: 0.767857; P: 0.779013; F1: 0.767451\n",
      "(train @ 8): L: 0.735714; A: 0.714286; R: 0.714286; P: 0.718913; F1: 0.714861\n",
      "(valid @ 8): L: 0.606385; A: 0.726190; R: 0.726190; P: 0.753271; F1: 0.717534\n",
      "(train @ 9): L: 0.635026; A: 0.686508; R: 0.686508; P: 0.697851; F1: 0.690000\n",
      "(valid @ 9): L: 0.572807; A: 0.779762; R: 0.779762; P: 0.789562; F1: 0.780134\n",
      "(train @ 10): L: 0.624111; A: 0.702381; R: 0.702381; P: 0.726031; F1: 0.692724\n",
      "(valid @ 10): L: 0.548446; A: 0.708333; R: 0.708333; P: 0.809988; F1: 0.673558\n",
      "(train @ 11): L: 0.612404; A: 0.678571; R: 0.678571; P: 0.701080; F1: 0.663988\n",
      "(valid @ 11): L: 0.498267; A: 0.815476; R: 0.815476; P: 0.833743; F1: 0.812207\n",
      "(train @ 12): L: 0.584220; A: 0.694444; R: 0.694444; P: 0.733468; F1: 0.674869\n",
      "(valid @ 12): L: 0.471632; A: 0.863095; R: 0.863095; P: 0.868469; F1: 0.863207\n",
      "(train @ 13): L: 0.522902; A: 0.761905; R: 0.761905; P: 0.795844; F1: 0.752549\n",
      "(valid @ 13): L: 0.479110; A: 0.738095; R: 0.738095; P: 0.824060; F1: 0.713744\n",
      "(train @ 14): L: 0.490462; A: 0.777778; R: 0.777778; P: 0.783673; F1: 0.776224\n",
      "(valid @ 14): L: 0.417372; A: 0.880952; R: 0.880952; P: 0.883117; F1: 0.880745\n",
      "(train @ 15): L: 0.532671; A: 0.714286; R: 0.714286; P: 0.740819; F1: 0.703163\n",
      "(valid @ 15): L: 0.402258; A: 0.869048; R: 0.869048; P: 0.877551; F1: 0.868132\n",
      "(train @ 16): L: 0.503294; A: 0.718254; R: 0.718254; P: 0.774943; F1: 0.697165\n",
      "(valid @ 16): L: 0.446384; A: 0.785714; R: 0.785714; P: 0.847212; F1: 0.773063\n",
      "(train @ 17): L: 0.512549; A: 0.714286; R: 0.714286; P: 0.784509; F1: 0.688632\n",
      "(valid @ 17): L: 0.391111; A: 0.857143; R: 0.857143; P: 0.876190; F1: 0.854875\n",
      "(train @ 18): L: 0.462422; A: 0.773810; R: 0.773810; P: 0.782912; F1: 0.770676\n",
      "(valid @ 18): L: 0.438620; A: 0.809524; R: 0.809524; P: 0.853221; F1: 0.801938\n",
      "(train @ 19): L: 0.436157; A: 0.821429; R: 0.821429; P: 0.838095; F1: 0.818594\n",
      "(valid @ 19): L: 0.375050; A: 0.839286; R: 0.839286; P: 0.864191; F1: 0.835793\n",
      "(train @ 20): L: 0.409808; A: 0.793651; R: 0.793651; P: 0.803271; F1: 0.791488\n",
      "(valid @ 20): L: 0.320332; A: 0.892857; R: 0.892857; P: 0.892857; F1: 0.892857\n",
      "(train @ 21): L: 0.369358; A: 0.845238; R: 0.845238; P: 0.847153; F1: 0.844969\n",
      "(valid @ 21): L: 0.309762; A: 0.910714; R: 0.910714; P: 0.911304; F1: 0.910676\n",
      "(train @ 22): L: 0.358729; A: 0.837302; R: 0.837302; P: 0.837059; F1: 0.836844\n",
      "(valid @ 22): L: 0.298611; A: 0.904762; R: 0.904762; P: 0.908929; F1: 0.904467\n",
      "(train @ 23): L: 0.346981; A: 0.841270; R: 0.841270; P: 0.844643; F1: 0.840778\n",
      "(valid @ 23): L: 0.368991; A: 0.815476; R: 0.815476; P: 0.855411; F1: 0.808753\n",
      "(train @ 24): L: 0.379107; A: 0.817460; R: 0.817460; P: 0.818222; F1: 0.817319\n",
      "(valid @ 24): L: 0.278468; A: 0.910714; R: 0.910714; P: 0.911304; F1: 0.910676\n",
      "(train @ 25): L: 0.299197; A: 0.873016; R: 0.873016; P: 0.883726; F1: 0.871918\n",
      "(valid @ 25): L: 0.349272; A: 0.815476; R: 0.815476; P: 0.843210; F1: 0.810646\n",
      "(train @ 26): L: 0.279556; A: 0.900794; R: 0.900794; P: 0.901501; F1: 0.900740\n",
      "(valid @ 26): L: 0.254538; A: 0.928571; R: 0.928571; P: 0.928571; F1: 0.928571\n",
      "(train @ 27): L: 0.243942; A: 0.912698; R: 0.912698; P: 0.912815; F1: 0.912691\n",
      "(valid @ 27): L: 0.262364; A: 0.892857; R: 0.892857; P: 0.895105; F1: 0.892671\n",
      "(train @ 28): L: 0.272368; A: 0.876984; R: 0.876984; P: 0.877220; F1: 0.876960\n",
      "(valid @ 28): L: 0.244571; A: 0.910714; R: 0.910714; P: 0.911304; F1: 0.910676\n",
      "(train @ 29): L: 0.223952; A: 0.928571; R: 0.928571; P: 0.931660; F1: 0.928418\n",
      "(valid @ 29): L: 0.310244; A: 0.863095; R: 0.863095; P: 0.876329; F1: 0.861593\n",
      "(train @ 30): L: 0.233921; A: 0.904762; R: 0.904762; P: 0.908929; F1: 0.904467\n",
      "(valid @ 30): L: 0.454559; A: 0.797619; R: 0.797619; P: 0.862534; F1: 0.785671\n",
      "Best val Metric 0.928571 @ 26\n",
      "\n",
      "models are saved @ ./predictions/211209104933/flvl_3_4_6_pretrained_model_c1.pt\n",
      "Saving predictions @ ./predictions/211209104933/flvl_train_3_4_6_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209104933/flvl_valid_2_5_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209104933/flvl_public_test_trained_on_3_4_6_r21d_rgb.csv\n",
      "Average of Best Metrics on Each Valid Set: 0.853123, 211209104933\n",
      "Saved test results @ ./predictions/211209104933/flvl_public_test_agg_r21d_rgb.csv\n"
     ]
    }
   ],
   "source": [
    "# Combined\n",
    "from main_stacked_feature_noOpaque import Config, run_kfold\n",
    "\n",
    "cfg = Config()\n",
    "cfg.assign_variable('task', 'flvl')\n",
    "cfg.assign_variable('output_dim', 3)\n",
    "cfg.assign_variable('model_type', 'GRU')\n",
    "cfg.assign_variable('bi_dir', False)\n",
    "cfg.assign_variable('device', 'cuda:0')\n",
    "cfg.assign_variable('data_root', './r21d_rgb_features')\n",
    "cfg.assign_variable('drop_p', 0.0) # results will be irreproducible\n",
    "cfg.assign_variable('batch_size', 64)\n",
    "cfg.assign_variable('input_dim', 512*4)\n",
    "cfg.assign_variable('hidden_dim', 512)\n",
    "cfg.assign_variable('n_layers', 3)\n",
    "cfg.assign_variable('num_epochs', 30)\n",
    "cfg.assign_variable('seed', 1337)\n",
    "\n",
    "run_kfold(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### E. Experiments with Single View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': [2, 3, 5], 'valid': [4, 6], 'public_test': [10, 11, 12], 'private_test': [13, 14, 15]}\n",
      "(train @ 1): L: 1.116469; A: 0.384921; R: 0.384921; P: 0.501884; F1: 0.363069\n",
      "(valid @ 1): L: 1.021641; A: 0.446429; R: 0.446429; P: 0.347866; F1: 0.315321\n",
      "(train @ 2): L: 0.962767; A: 0.503968; R: 0.503968; P: 0.430108; F1: 0.453963\n",
      "(valid @ 2): L: 0.971604; A: 0.505952; R: 0.505952; P: 0.545190; F1: 0.422827\n",
      "(train @ 3): L: 0.901876; A: 0.567460; R: 0.567460; P: 0.530114; F1: 0.521526\n",
      "(valid @ 3): L: 0.911219; A: 0.458333; R: 0.458333; P: 0.386912; F1: 0.418447\n",
      "(train @ 4): L: 0.835693; A: 0.523810; R: 0.523810; P: 0.444219; F1: 0.480010\n",
      "(valid @ 4): L: 0.943125; A: 0.541667; R: 0.541667; P: 0.571766; F1: 0.472776\n",
      "(train @ 5): L: 0.781116; A: 0.607143; R: 0.607143; P: 0.556162; F1: 0.562610\n",
      "(valid @ 5): L: 0.907124; A: 0.589286; R: 0.589286; P: 0.604591; F1: 0.589223\n",
      "(train @ 6): L: 0.733482; A: 0.710317; R: 0.710317; P: 0.724572; F1: 0.709981\n",
      "(valid @ 6): L: 0.940392; A: 0.553571; R: 0.553571; P: 0.601463; F1: 0.563012\n",
      "(train @ 7): L: 0.642856; A: 0.722222; R: 0.722222; P: 0.782306; F1: 0.717066\n",
      "(valid @ 7): L: 0.891221; A: 0.601190; R: 0.601190; P: 0.639577; F1: 0.601146\n",
      "(train @ 8): L: 0.608982; A: 0.730159; R: 0.730159; P: 0.745029; F1: 0.731294\n",
      "(valid @ 8): L: 1.193544; A: 0.547619; R: 0.547619; P: 0.694553; F1: 0.558945\n",
      "(train @ 9): L: 0.573887; A: 0.753968; R: 0.753968; P: 0.755939; F1: 0.753786\n",
      "(valid @ 9): L: 0.915529; A: 0.535714; R: 0.535714; P: 0.540728; F1: 0.535825\n",
      "(train @ 10): L: 0.587880; A: 0.750000; R: 0.750000; P: 0.756885; F1: 0.751436\n",
      "(valid @ 10): L: 1.250475; A: 0.529762; R: 0.529762; P: 0.671085; F1: 0.533956\n",
      "(train @ 11): L: 0.581146; A: 0.714286; R: 0.714286; P: 0.730233; F1: 0.710627\n",
      "(valid @ 11): L: 0.856789; A: 0.642857; R: 0.642857; P: 0.673194; F1: 0.638677\n",
      "(train @ 12): L: 0.576734; A: 0.730159; R: 0.730159; P: 0.740828; F1: 0.731354\n",
      "(valid @ 12): L: 0.995027; A: 0.607143; R: 0.607143; P: 0.678387; F1: 0.610792\n",
      "(train @ 13): L: 0.462493; A: 0.801587; R: 0.801587; P: 0.805535; F1: 0.800114\n",
      "(valid @ 13): L: 1.004565; A: 0.619048; R: 0.619048; P: 0.695593; F1: 0.623214\n",
      "(train @ 14): L: 0.563078; A: 0.694444; R: 0.694444; P: 0.715019; F1: 0.687100\n",
      "(valid @ 14): L: 0.838774; A: 0.690476; R: 0.690476; P: 0.724195; F1: 0.680735\n",
      "(train @ 15): L: 0.584722; A: 0.706349; R: 0.706349; P: 0.759051; F1: 0.684681\n",
      "(valid @ 15): L: 0.730086; A: 0.678571; R: 0.678571; P: 0.693894; F1: 0.671705\n",
      "(train @ 16): L: 0.546735; A: 0.702381; R: 0.702381; P: 0.733450; F1: 0.689626\n",
      "(valid @ 16): L: 0.663672; A: 0.654762; R: 0.654762; P: 0.648515; F1: 0.648697\n",
      "(train @ 17): L: 0.449105; A: 0.785714; R: 0.785714; P: 0.789192; F1: 0.784089\n",
      "(valid @ 17): L: 0.865765; A: 0.642857; R: 0.642857; P: 0.703999; F1: 0.630542\n",
      "(train @ 18): L: 0.423564; A: 0.825397; R: 0.825397; P: 0.830049; F1: 0.824151\n",
      "(valid @ 18): L: 0.640750; A: 0.702381; R: 0.702381; P: 0.707527; F1: 0.699541\n",
      "(train @ 19): L: 0.401220; A: 0.841270; R: 0.841270; P: 0.841641; F1: 0.841215\n",
      "(valid @ 19): L: 0.694034; A: 0.714286; R: 0.714286; P: 0.721707; F1: 0.710714\n",
      "(train @ 20): L: 0.523142; A: 0.718254; R: 0.718254; P: 0.729361; F1: 0.716439\n",
      "(valid @ 20): L: 0.747956; A: 0.720238; R: 0.720238; P: 0.734687; F1: 0.712038\n",
      "(train @ 21): L: 0.391057; A: 0.801587; R: 0.801587; P: 0.818172; F1: 0.798196\n",
      "(valid @ 21): L: 0.700849; A: 0.678571; R: 0.678571; P: 0.674000; F1: 0.674593\n",
      "(train @ 22): L: 0.351456; A: 0.849206; R: 0.849206; P: 0.850558; F1: 0.848734\n",
      "(valid @ 22): L: 0.769344; A: 0.684524; R: 0.684524; P: 0.691198; F1: 0.678124\n",
      "(train @ 23): L: 0.351091; A: 0.829365; R: 0.829365; P: 0.833157; F1: 0.828745\n",
      "(valid @ 23): L: 0.741441; A: 0.702381; R: 0.702381; P: 0.706406; F1: 0.696648\n",
      "(train @ 24): L: 0.338708; A: 0.869048; R: 0.869048; P: 0.874902; F1: 0.868413\n",
      "(valid @ 24): L: 0.681957; A: 0.696429; R: 0.696429; P: 0.696646; F1: 0.696297\n",
      "(train @ 25): L: 0.320660; A: 0.873016; R: 0.873016; P: 0.874680; F1: 0.872841\n",
      "(valid @ 25): L: 0.831214; A: 0.708333; R: 0.708333; P: 0.718685; F1: 0.701157\n",
      "(train @ 26): L: 0.626849; A: 0.797619; R: 0.797619; P: 0.802995; F1: 0.796881\n",
      "(valid @ 26): L: 0.761650; A: 0.702381; R: 0.702381; P: 0.707956; F1: 0.696302\n",
      "(train @ 27): L: 0.324145; A: 0.853175; R: 0.853175; P: 0.864910; F1: 0.851607\n",
      "(valid @ 27): L: 0.586360; A: 0.690476; R: 0.690476; P: 0.696660; F1: 0.686607\n",
      "(train @ 28): L: 0.371476; A: 0.793651; R: 0.793651; P: 0.805195; F1: 0.791071\n",
      "(valid @ 28): L: 0.772171; A: 0.696429; R: 0.696429; P: 0.715998; F1: 0.685274\n",
      "(train @ 29): L: 0.350389; A: 0.821429; R: 0.821429; P: 0.821966; F1: 0.821333\n",
      "(valid @ 29): L: 0.578838; A: 0.684524; R: 0.684524; P: 0.692989; F1: 0.678934\n",
      "(train @ 30): L: 0.360410; A: 0.821429; R: 0.821429; P: 0.847173; F1: 0.817161\n",
      "(valid @ 30): L: 1.113648; A: 0.636905; R: 0.636905; P: 0.716649; F1: 0.610780\n",
      "Best val Metric 0.712038 @ 20\n",
      "\n",
      "models are saved @ ./predictions/211209110334/flvl_2_3_5_pretrained_model_c1.pt\n",
      "Saving predictions @ ./predictions/211209110334/flvl_train_2_3_5_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209110334/flvl_valid_4_6_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209110334/flvl_public_test_trained_on_2_3_5_r21d_rgb.csv\n",
      "{'train': [2, 4, 5], 'valid': [3, 6], 'public_test': [10, 11, 12], 'private_test': [13, 14, 15]}\n",
      "(train @ 1): L: 1.109668; A: 0.384921; R: 0.384921; P: 0.527496; F1: 0.352727\n",
      "(valid @ 1): L: 1.036503; A: 0.428571; R: 0.428571; P: 0.184773; F1: 0.258219\n",
      "(train @ 2): L: 0.977820; A: 0.503968; R: 0.503968; P: 0.429685; F1: 0.448687\n",
      "(valid @ 2): L: 0.966271; A: 0.511905; R: 0.511905; P: 0.580952; F1: 0.422651\n",
      "(train @ 3): L: 0.896014; A: 0.591270; R: 0.591270; P: 0.573696; F1: 0.540594\n",
      "(valid @ 3): L: 0.899338; A: 0.511905; R: 0.511905; P: 0.443223; F1: 0.474497\n",
      "(train @ 4): L: 0.863432; A: 0.496032; R: 0.496032; P: 0.410054; F1: 0.441109\n",
      "(valid @ 4): L: 0.953501; A: 0.517857; R: 0.517857; P: 0.584801; F1: 0.431619\n",
      "(train @ 5): L: 0.804987; A: 0.587302; R: 0.587302; P: 0.567059; F1: 0.537128\n",
      "(valid @ 5): L: 0.873258; A: 0.607143; R: 0.607143; P: 0.676325; F1: 0.596516\n",
      "(train @ 6): L: 0.742649; A: 0.714286; R: 0.714286; P: 0.735230; F1: 0.707741\n",
      "(valid @ 6): L: 0.968821; A: 0.565476; R: 0.565476; P: 0.661795; F1: 0.578756\n",
      "(train @ 7): L: 0.701157; A: 0.698413; R: 0.698413; P: 0.720430; F1: 0.699380\n",
      "(valid @ 7): L: 0.851176; A: 0.577381; R: 0.577381; P: 0.629728; F1: 0.574310\n",
      "(train @ 8): L: 0.686710; A: 0.654762; R: 0.654762; P: 0.697876; F1: 0.627128\n",
      "(valid @ 8): L: 1.039124; A: 0.541667; R: 0.541667; P: 0.661377; F1: 0.538213\n",
      "(train @ 9): L: 0.600950; A: 0.761905; R: 0.761905; P: 0.763957; F1: 0.761586\n",
      "(valid @ 9): L: 1.068320; A: 0.547619; R: 0.547619; P: 0.662132; F1: 0.557644\n",
      "(train @ 10): L: 0.726538; A: 0.575397; R: 0.575397; P: 0.566215; F1: 0.564529\n",
      "(valid @ 10): L: 0.813295; A: 0.625000; R: 0.625000; P: 0.646463; F1: 0.624138\n",
      "(train @ 11): L: 0.609276; A: 0.670635; R: 0.670635; P: 0.696176; F1: 0.672505\n",
      "(valid @ 11): L: 0.968694; A: 0.535714; R: 0.535714; P: 0.672649; F1: 0.505942\n",
      "(train @ 12): L: 0.563860; A: 0.753968; R: 0.753968; P: 0.764804; F1: 0.753370\n",
      "(valid @ 12): L: 0.848668; A: 0.595238; R: 0.595238; P: 0.618934; F1: 0.587980\n",
      "(train @ 13): L: 0.503668; A: 0.785714; R: 0.785714; P: 0.802606; F1: 0.782684\n",
      "(valid @ 13): L: 1.031458; A: 0.577381; R: 0.577381; P: 0.679505; F1: 0.576945\n",
      "(train @ 14): L: 0.499348; A: 0.777778; R: 0.777778; P: 0.782362; F1: 0.778311\n",
      "(valid @ 14): L: 0.869381; A: 0.666667; R: 0.666667; P: 0.719438; F1: 0.653165\n",
      "(train @ 15): L: 0.540013; A: 0.757937; R: 0.757937; P: 0.763128; F1: 0.758512\n",
      "(valid @ 15): L: 0.800178; A: 0.696429; R: 0.696429; P: 0.728320; F1: 0.684757\n",
      "(train @ 16): L: 0.504632; A: 0.765873; R: 0.765873; P: 0.771350; F1: 0.766511\n",
      "(valid @ 16): L: 0.796188; A: 0.696429; R: 0.696429; P: 0.718281; F1: 0.688432\n",
      "(train @ 17): L: 0.465785; A: 0.777778; R: 0.777778; P: 0.783368; F1: 0.775330\n",
      "(valid @ 17): L: 0.648166; A: 0.684524; R: 0.684524; P: 0.679872; F1: 0.681188\n",
      "(train @ 18): L: 0.461531; A: 0.757937; R: 0.757937; P: 0.770370; F1: 0.754094\n",
      "(valid @ 18): L: 0.684192; A: 0.714286; R: 0.714286; P: 0.752413; F1: 0.698833\n",
      "(train @ 19): L: 0.452635; A: 0.785714; R: 0.785714; P: 0.785714; F1: 0.785714\n",
      "(valid @ 19): L: 0.841960; A: 0.642857; R: 0.642857; P: 0.736272; F1: 0.603253\n",
      "(train @ 20): L: 0.484866; A: 0.781746; R: 0.781746; P: 0.802759; F1: 0.776586\n",
      "(valid @ 20): L: 0.563570; A: 0.702381; R: 0.702381; P: 0.702381; F1: 0.702381\n",
      "(train @ 21): L: 0.456020; A: 0.746032; R: 0.746032; P: 0.810239; F1: 0.727731\n",
      "(valid @ 21): L: 0.630295; A: 0.744048; R: 0.744048; P: 0.786761; F1: 0.731064\n",
      "(train @ 22): L: 0.397655; A: 0.809524; R: 0.809524; P: 0.835717; F1: 0.804684\n",
      "(valid @ 22): L: 0.620153; A: 0.732143; R: 0.732143; P: 0.755796; F1: 0.723162\n",
      "(train @ 23): L: 0.479351; A: 0.781746; R: 0.781746; P: 0.803404; F1: 0.776530\n",
      "(valid @ 23): L: 0.734045; A: 0.714286; R: 0.714286; P: 0.784365; F1: 0.692674\n",
      "(train @ 24): L: 0.483455; A: 0.761905; R: 0.761905; P: 0.801903; F1: 0.751272\n",
      "(valid @ 24): L: 0.524866; A: 0.744048; R: 0.744048; P: 0.748173; F1: 0.742545\n",
      "(train @ 25): L: 0.410993; A: 0.805556; R: 0.805556; P: 0.836689; F1: 0.799678\n",
      "(valid @ 25): L: 0.505686; A: 0.750000; R: 0.750000; P: 0.752232; F1: 0.749226\n",
      "(train @ 26): L: 0.461648; A: 0.777778; R: 0.777778; P: 0.778916; F1: 0.777473\n",
      "(valid @ 26): L: 0.770965; A: 0.654762; R: 0.654762; P: 0.756458; F1: 0.605287\n",
      "(train @ 27): L: 0.429537; A: 0.805556; R: 0.805556; P: 0.814755; F1: 0.803700\n",
      "(valid @ 27): L: 0.497338; A: 0.738095; R: 0.738095; P: 0.742857; F1: 0.736264\n",
      "(train @ 28): L: 0.368216; A: 0.845238; R: 0.845238; P: 0.853983; F1: 0.844031\n",
      "(valid @ 28): L: 0.557073; A: 0.738095; R: 0.738095; P: 0.773501; F1: 0.726756\n",
      "(train @ 29): L: 0.355540; A: 0.873016; R: 0.873016; P: 0.875624; F1: 0.872743\n",
      "(valid @ 29): L: 0.490664; A: 0.750000; R: 0.750000; P: 0.751249; F1: 0.749565\n",
      "(train @ 30): L: 0.360831; A: 0.845238; R: 0.845238; P: 0.858246; F1: 0.843463\n",
      "(valid @ 30): L: 0.642877; A: 0.714286; R: 0.714286; P: 0.781300; F1: 0.691695\n",
      "Best val Metric 0.749565 @ 29\n",
      "\n",
      "models are saved @ ./predictions/211209110334/flvl_2_4_5_pretrained_model_c1.pt\n",
      "Saving predictions @ ./predictions/211209110334/flvl_train_2_4_5_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209110334/flvl_valid_3_6_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209110334/flvl_public_test_trained_on_2_4_5_r21d_rgb.csv\n",
      "{'train': [3, 4, 6], 'valid': [2, 5], 'public_test': [10, 11, 12], 'private_test': [13, 14, 15]}\n",
      "(train @ 1): L: 1.106358; A: 0.392857; R: 0.392857; P: 0.492063; F1: 0.386376\n",
      "(valid @ 1): L: 1.027732; A: 0.428571; R: 0.428571; P: 0.183673; F1: 0.257143\n",
      "(train @ 2): L: 0.977227; A: 0.464286; R: 0.464286; P: 0.395918; F1: 0.424429\n",
      "(valid @ 2): L: 0.887277; A: 0.613095; R: 0.613095; P: 0.526253; F1: 0.566345\n",
      "(train @ 3): L: 0.969610; A: 0.563492; R: 0.563492; P: 0.514901; F1: 0.520647\n",
      "(valid @ 3): L: 0.902083; A: 0.428571; R: 0.428571; P: 0.196542; F1: 0.269495\n",
      "(train @ 4): L: 0.931936; A: 0.452381; R: 0.452381; P: 0.370551; F1: 0.395951\n",
      "(valid @ 4): L: 0.840735; A: 0.613095; R: 0.613095; P: 0.595056; F1: 0.562941\n",
      "(train @ 5): L: 0.909065; A: 0.500000; R: 0.500000; P: 0.461460; F1: 0.452710\n",
      "(valid @ 5): L: 0.849422; A: 0.434524; R: 0.434524; P: 0.229923; F1: 0.289836\n",
      "(train @ 6): L: 0.897702; A: 0.519841; R: 0.519841; P: 0.562244; F1: 0.503796\n",
      "(valid @ 6): L: 0.773020; A: 0.619048; R: 0.619048; P: 0.662940; F1: 0.588245\n",
      "(train @ 7): L: 0.848655; A: 0.567460; R: 0.567460; P: 0.623103; F1: 0.554404\n",
      "(valid @ 7): L: 0.758764; A: 0.648810; R: 0.648810; P: 0.613788; F1: 0.600790\n",
      "(train @ 8): L: 0.830354; A: 0.531746; R: 0.531746; P: 0.626768; F1: 0.499790\n",
      "(valid @ 8): L: 0.782968; A: 0.494048; R: 0.494048; P: 0.510949; F1: 0.390746\n",
      "(train @ 9): L: 0.752369; A: 0.619048; R: 0.619048; P: 0.643045; F1: 0.608471\n",
      "(valid @ 9): L: 0.668992; A: 0.720238; R: 0.720238; P: 0.762863; F1: 0.704580\n",
      "(train @ 10): L: 0.795209; A: 0.567460; R: 0.567460; P: 0.565852; F1: 0.562112\n",
      "(valid @ 10): L: 0.704196; A: 0.636905; R: 0.636905; P: 0.669728; F1: 0.611694\n",
      "(train @ 11): L: 0.697797; A: 0.626984; R: 0.626984; P: 0.674789; F1: 0.626007\n",
      "(valid @ 11): L: 0.622312; A: 0.714286; R: 0.714286; P: 0.731633; F1: 0.713141\n",
      "(train @ 12): L: 0.630999; A: 0.722222; R: 0.722222; P: 0.727763; F1: 0.724285\n",
      "(valid @ 12): L: 0.573941; A: 0.720238; R: 0.720238; P: 0.784967; F1: 0.697705\n",
      "(train @ 13): L: 0.618613; A: 0.674603; R: 0.674603; P: 0.672356; F1: 0.670724\n",
      "(valid @ 13): L: 0.555281; A: 0.708333; R: 0.708333; P: 0.776999; F1: 0.682190\n",
      "(train @ 14): L: 0.573924; A: 0.710317; R: 0.710317; P: 0.711772; F1: 0.710939\n",
      "(valid @ 14): L: 0.501621; A: 0.803571; R: 0.803571; P: 0.806600; F1: 0.804672\n",
      "(train @ 15): L: 0.592171; A: 0.682540; R: 0.682540; P: 0.712852; F1: 0.664566\n",
      "(valid @ 15): L: 0.547513; A: 0.702381; R: 0.702381; P: 0.772906; F1: 0.674282\n",
      "(train @ 16): L: 0.541118; A: 0.686508; R: 0.686508; P: 0.706074; F1: 0.676230\n",
      "(valid @ 16): L: 0.471023; A: 0.785714; R: 0.785714; P: 0.800000; F1: 0.782313\n",
      "(train @ 17): L: 0.544591; A: 0.698413; R: 0.698413; P: 0.704714; F1: 0.694042\n",
      "(valid @ 17): L: 0.465550; A: 0.803571; R: 0.803571; P: 0.809119; F1: 0.802418\n",
      "(train @ 18): L: 0.514736; A: 0.746032; R: 0.746032; P: 0.748183; F1: 0.746352\n",
      "(valid @ 18): L: 0.513971; A: 0.708333; R: 0.708333; P: 0.787167; F1: 0.679292\n",
      "(train @ 19): L: 0.519976; A: 0.746032; R: 0.746032; P: 0.763878; F1: 0.740486\n",
      "(valid @ 19): L: 0.418081; A: 0.833333; R: 0.833333; P: 0.833536; F1: 0.833301\n",
      "(train @ 20): L: 0.545684; A: 0.718254; R: 0.718254; P: 0.744156; F1: 0.707230\n",
      "(valid @ 20): L: 0.422519; A: 0.815476; R: 0.815476; P: 0.821309; F1: 0.814393\n",
      "(train @ 21): L: 0.500646; A: 0.702381; R: 0.702381; P: 0.717022; F1: 0.695487\n",
      "(valid @ 21): L: 0.462995; A: 0.779762; R: 0.779762; P: 0.835165; F1: 0.767554\n",
      "(train @ 22): L: 0.466678; A: 0.769841; R: 0.769841; P: 0.770114; F1: 0.769762\n",
      "(valid @ 22): L: 0.405365; A: 0.851190; R: 0.851190; P: 0.857876; F1: 0.850317\n",
      "(train @ 23): L: 0.482350; A: 0.742063; R: 0.742063; P: 0.750151; F1: 0.738629\n",
      "(valid @ 23): L: 0.390340; A: 0.809524; R: 0.809524; P: 0.812500; F1: 0.808934\n",
      "(train @ 24): L: 0.441337; A: 0.809524; R: 0.809524; P: 0.812500; F1: 0.808934\n",
      "(valid @ 24): L: 0.380555; A: 0.833333; R: 0.833333; P: 0.843625; F1: 0.831743\n",
      "(train @ 25): L: 0.443428; A: 0.793651; R: 0.793651; P: 0.793764; F1: 0.793077\n",
      "(valid @ 25): L: 0.374408; A: 0.851190; R: 0.851190; P: 0.872131; F1: 0.848554\n",
      "(train @ 26): L: 0.509562; A: 0.738095; R: 0.738095; P: 0.771264; F1: 0.726757\n",
      "(valid @ 26): L: 0.380352; A: 0.845238; R: 0.845238; P: 0.863492; F1: 0.842782\n",
      "(train @ 27): L: 0.458064; A: 0.761905; R: 0.761905; P: 0.777831; F1: 0.757222\n",
      "(valid @ 27): L: 0.400937; A: 0.815476; R: 0.815476; P: 0.855411; F1: 0.808753\n",
      "(train @ 28): L: 0.437302; A: 0.813492; R: 0.813492; P: 0.832302; F1: 0.810068\n",
      "(valid @ 28): L: 0.343973; A: 0.869048; R: 0.869048; P: 0.874902; F1: 0.868413\n",
      "(train @ 29): L: 0.412000; A: 0.821429; R: 0.821429; P: 0.821622; F1: 0.821394\n",
      "(valid @ 29): L: 0.344852; A: 0.869048; R: 0.869048; P: 0.880742; F1: 0.867798\n",
      "(train @ 30): L: 0.409670; A: 0.825397; R: 0.825397; P: 0.826183; F1: 0.825262\n",
      "(valid @ 30): L: 0.315439; A: 0.886905; R: 0.886905; P: 0.888434; F1: 0.886768\n",
      "Best val Metric 0.886768 @ 30\n",
      "\n",
      "models are saved @ ./predictions/211209110334/flvl_3_4_6_pretrained_model_c1.pt\n",
      "Saving predictions @ ./predictions/211209110334/flvl_train_3_4_6_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209110334/flvl_valid_2_5_r21d_rgb.csv\n",
      "Saving predictions @ ./predictions/211209110334/flvl_public_test_trained_on_3_4_6_r21d_rgb.csv\n",
      "Average of Best Metrics on Each Valid Set: 0.782790, 211209110334\n",
      "Saved test results @ ./predictions/211209110334/flvl_public_test_agg_r21d_rgb.csv\n"
     ]
    }
   ],
   "source": [
    "# View 1\n",
    "from main_1View_NoOpaque import Config, run_kfold\n",
    "\n",
    "cfg = Config()\n",
    "cfg.assign_variable('task', 'flvl')\n",
    "cfg.assign_variable('output_dim', 3)\n",
    "cfg.assign_variable('model_type', 'GRU')\n",
    "cfg.assign_variable('bi_dir', False)\n",
    "cfg.assign_variable('device', 'cuda:0')\n",
    "cfg.assign_variable('data_root', './r21d_rgb_features')\n",
    "cfg.assign_variable('drop_p', 0.0) # results will be irreproducible\n",
    "cfg.assign_variable('batch_size', 64)\n",
    "cfg.assign_variable('input_dim', 512)\n",
    "cfg.assign_variable('hidden_dim', 512)\n",
    "cfg.assign_variable('n_layers', 3)\n",
    "cfg.assign_variable('num_epochs', 30)\n",
    "cfg.assign_variable('seed', 1337)\n",
    "\n",
    "run_kfold(cfg,[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# View 2\n",
    "from main_1View_NoOpaque import Config, run_kfold\n",
    "\n",
    "cfg = Config()\n",
    "cfg.assign_variable('task', 'flvl')\n",
    "cfg.assign_variable('output_dim', 3)\n",
    "cfg.assign_variable('model_type', 'GRU')\n",
    "cfg.assign_variable('bi_dir', False)\n",
    "cfg.assign_variable('device', 'cuda:0')\n",
    "cfg.assign_variable('data_root', './r21d_rgb_features')\n",
    "cfg.assign_variable('drop_p', 0.0) # results will be irreproducible\n",
    "cfg.assign_variable('batch_size', 64)\n",
    "cfg.assign_variable('input_dim', 512)\n",
    "cfg.assign_variable('hidden_dim', 512)\n",
    "cfg.assign_variable('n_layers', 3)\n",
    "cfg.assign_variable('num_epochs', 30)\n",
    "cfg.assign_variable('seed', 1337)\n",
    "\n",
    "run_kfold(cfg,[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# View 3\n",
    "from main_1View_NoOpaque import Config, run_kfold\n",
    "\n",
    "cfg = Config()\n",
    "cfg.assign_variable('task', 'flvl')\n",
    "cfg.assign_variable('output_dim', 3)\n",
    "cfg.assign_variable('model_type', 'GRU')\n",
    "cfg.assign_variable('bi_dir', False)\n",
    "cfg.assign_variable('device', 'cuda:0')\n",
    "cfg.assign_variable('data_root', './r21d_rgb_features')\n",
    "cfg.assign_variable('drop_p', 0.0) # results will be irreproducible\n",
    "cfg.assign_variable('batch_size', 64)\n",
    "cfg.assign_variable('input_dim', 512)\n",
    "cfg.assign_variable('hidden_dim', 512)\n",
    "cfg.assign_variable('n_layers', 3)\n",
    "cfg.assign_variable('num_epochs', 30)\n",
    "cfg.assign_variable('seed', 1337)\n",
    "\n",
    "run_kfold(cfg,[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# View 4\n",
    "from main_1View_NoOpaque import Config, run_kfold\n",
    "\n",
    "cfg = Config()\n",
    "cfg.assign_variable('task', 'flvl')\n",
    "cfg.assign_variable('output_dim', 3)\n",
    "cfg.assign_variable('model_type', 'GRU')\n",
    "cfg.assign_variable('bi_dir', False)\n",
    "cfg.assign_variable('device', 'cuda:0')\n",
    "cfg.assign_variable('data_root', './r21d_rgb_features')\n",
    "cfg.assign_variable('drop_p', 0.0) # results will be irreproducible\n",
    "cfg.assign_variable('batch_size', 64)\n",
    "cfg.assign_variable('input_dim', 512)\n",
    "cfg.assign_variable('hidden_dim', 512)\n",
    "cfg.assign_variable('n_layers', 3)\n",
    "cfg.assign_variable('num_epochs', 30)\n",
    "cfg.assign_variable('seed', 1337)\n",
    "\n",
    "run_kfold(cfg,[3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
